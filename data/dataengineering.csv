title,selftext,created_utc,num_comments,num_crossposts,score,subreddit
Data Lake in S3 Glacier,"Hi! My company is in the middle of a large scale data migration from on-prem servers into AWS. Personally, I have been tasked with moving datasets and tables that are older and no longer updated as one-time history loads into our S3 data lake. After taking some time to learn Glue and its features, I developed a relatively nice and parameterized pipeline that would be able to crawl the source data, migrate it to S3 in parquet format, and then crawl the target data so that it can be queried by Athena or Redshift Spectrum. However, I'm now being told to evaluate the option of migrating this data as archives into the Glacier storage class. From what I've read, data that is stored in Glacier must be in .csv format and can only be queried by Glacier Select as opposed to Athena or Redshift Spectrum. It seems unavoidable that the pipeline I've developed will need to be reconfigured, but for some of our larger datasets we had looked into partitioning the data as it gets migrated.

Does anyone know if Glacier Select will read partitions like Athena or how it interacts with them? And in a broader sense, has anyone specifically tried to build a data lake entirely within Glacier and can lay out some of the positives and negatives? I'm at a bit of a loss right now as there's kind of a lack of material online, so I'd be really appreciative of any guidance. Thanks!",1587506098,12,0,1,dataengineering
Conda vs pip and advise on setting up venv/git/directory,"Hello,

I am currently working as a data analyst. The organization I work for has a rather small IT team, hence the need to perform ETL.

Last year I developed my SQL and Python skills and now I am following courses to build a data platform. I am currently using VSCode but have no experience with virtual environments. I simply import the packages that I need with ""pip install package"". 

Now that believe this is not a scalable approach I would like to hear your thoughts on whether to use conda or pip, and how I can keep track on the packages in a manageble way. Also I have dived into Git to store scripts in a repository but have no clue if my approach is right. 

Do you have 1 repository for managing a dataplatform, or multiple?",1587503398,16,0,3,dataengineering
Data Modeling ERD Programs for Data Vault,"Looking for recommendations on tools for creating and managing Entity Relationship Diagrams to support Data Vault 2.0 Data Warehouse.  

Would like ability to assign color for entity types (hub, link, sat).  I have been using [draw.io](https://draw.io) but I'm sure there is something more technical and can support exporting out sql DDL statements.

thanks for your help!",1587491813,3,0,1,dataengineering
Get into DE with side projects,"hi@all

I would like to start a side project related to DE soon to get some experience and enhance my Coding / Software Engineering skills (in Scala). Do you have some advice on how to get an idea for an appropiate project? My main issue is that I do not have an idea, which I can use as a baseline for a side project. All I can think of are things, I already solved (at least partially) in past proejcts. 

Tbh: I never started any side projcets. The projects I did for my studies or side jobs were already very time consuming. Now I have some spare time after my job and on weekends, which I'd like to use to improve my skill set.   

I am especially interested in stuff like ETL processes using REST endpoints and Hadoop / Spark. 

I apprecicate your help 

A bit about myself:

\- CS graduate, working as a research associate in the area of ""retrieval systems / semantic web / linked data"" since January 2020.

\-  During my studies, my main focus was mainly related to data modelling and management with insights in the fields of: supervised machine learning models, Information retrieval, knowledge engineering, search engines and NLP

\- Very familiar with Python &amp; and RDBMS, also had some experience in C, Kotlon and Web Frontend Technologies. Recently started learning Scala.

\- Thanks to my parttime jobs, I gathered some experience with many of the concepts and topics of the cookbook (linux, docker, shell scripting, JSON web tokens, cronjobs). I would not call myself an expert,

\- Using this cookbook(https://github.com/andkret/Cookbook) as some sort of guide to get into DE",1587488798,0,0,1,dataengineering
Singer for ETL,Is anyone using Singer for their ETL? What are scheduling it with?,1587480323,6,0,1,dataengineering
Webinar on Enterprise Data Science &amp; It's Project Lifecycle and Management,,1587476134,0,0,1,dataengineering
"How to orchestrate a batch, monthly Machine Learning pipeline","Hi, we have a machine learning pipeline made of several steps:

1. data collection + preprocessing to build the training dataset
2. model building based on data of step 1
3. data collection + preprocessing to build the dataset to be scored (predicted)
4. scoring of the new data

This pipeline would run on a monthly basis and is linear. Currently, each step has its corresponding docker file (something we decided to do because we want to separability of concerns).

We need to run this on Azure and use some tool to orchestrate the different docker containers without too much overhead, infra management, costs, etc. Basically something simple but robust,  able to scale if needed, and inactive when not running the pipeline. 

Some context:

* first pipeline in production
* data storage: azure data lake storage gen 2
* ideally, the orchestration tool would use some of the outputs of step n for step n + 1
* monthly run
* other pipelines will be added in the future (nothing real-time though, everything is batch)
* very small team

We are currently looking into KubeFlow, which we like a lot but we are wondering whether it's too complex for what we want to achieve here.

Do you have any recommendations? Similar flows that you set up?

Thank you!",1587451750,1,0,1,dataengineering
Self-Service/Data Discovery for Internal Users,"Hi All,

I'm wondering how you handle exposing data to internal users?

Basically I would like a system where they can traverse the database tables in a frontend with search (and permissions) and then do data extracts for themselves rather than have the data team action them. These users are operations people so it can't be too technical.

What are people using as a self-serve data option? We could use PowerBI but it's a little clunky for discovery and extraction. Amundsen would seem to be ideal from a 10,000ft view (haven't tried it) but if it doesn't have direct download to csv it's a non-starter.

Could always do a custom internal website but really trying to not have to do a full build for this.

Thanks for your thoughts",1587430701,6,0,1,dataengineering
Apache Airflow: The ExternalTaskSensor demystified,,1587428462,0,0,1,dataengineering
What opensource cassandra modelling tool do you use,"I am looking for an open source modelling tool for cassandra. Could you please share which one you use and its pros/cons.

Thanks,",1587422048,0,0,1,dataengineering
"First DE project, plz criticize/give feedback.","Been following this sub for awhile whilst learning DE.  Any feedback from this community would be awesome. I've include a data lake architecture diagram and the database schema I will be using for the structured data and the data warehouse.

&amp;#x200B;

https://preview.redd.it/iwcdawcuv0u41.png?width=1100&amp;format=png&amp;auto=webp&amp;s=9d784d036f8e4cb767b23559f912b60d8cdcfef5

https://preview.redd.it/6871yctvv0u41.png?width=760&amp;format=png&amp;auto=webp&amp;s=96b63cf0714b0aceeb27c4e81aeb3b9697a81c35

This project is a capstone project (for the udacity de-nano course) that I have come up with for the company I'm currently working for.  It's basically combining some data sources to ultimately make a data warehouse that can be analyzed.  I will be using Airflow (on my local machine) to orchestrate ingesting and storing the data, processing the data, data validation, and staging the data on redshift.  I will use pySpark on an AWS EMR cluster to transform some of the unstructured/semi-structered data in to structured data (in tabular format).

Some questions I have are:

* Is it fair to call this a data lake? Albeit a simple one.
* Do you think having this as a primary portfolio piece will be enough to get a job as an entry DE or at least an interview?
* What could be done better? I'm trying specifically to gain experience in AWS S3, Redshift, pySpark / EMR, and Airflow.",1587410196,12,0,1,dataengineering
Cleaning spreadsheets with merged cells and missing labels: what's the best approach?,"I have a project that involves analyzing AP Exam test scores. To start, I need to download a few dozen .xls files containing historical data from CollegeBoard's website, but I've hit a pretty early roadblock. 

Each file contains performance data on every AP exam taken within a year, disaggregated by race. Their data files were clearly designed for a human reader, not a computer. There are a bunch of merged cells, a lot of columns contain numbers, letters, and symbols, only the first row of each group contains the race/ethnicity label, and the headers/row labels aren't consistent across each file. (I'm including a partial screenshot of one of the data files. These are also [available for download on CollegeBoard's website](https://research.collegeboard.org/programs/ap/data/archived/ap-2018), in case you want to download an .xls file for a US state and join my misery.)

My goal is to get all of these files into a tidy format like this:

&amp;#x200B;

|Year|Race/Ethnicity|Subject|Score|Students Earning Score|
|:-|:-|:-|:-|:-|
|2018|American Indian / Alaska Native|Biology|4|2|
|2018|American Indian / Alaska Native|Biology|3|5|

&amp;#x200B;

I'm struggling with the race/ethnicity labels. In the raw data, only the first row is labeled for each race, but I need each row to include the race/ethnicity. Considering that I have to repeat this process for \~24 files (and the race/ethnicity labels can differ from one file to the next), what's a good approach to getting the race/ethnicity label in every row?

I'm comfortable with R, SQL, and Power Query, but I'm open to any solution that doesn't involve sobbing while copy-pasting labels in Excel. 

Happy to provide more information/context if needed. Thanks, everyone!

&amp;#x200B;

https://preview.redd.it/cru8no8s70u41.png?width=1860&amp;format=png&amp;auto=webp&amp;s=ce974e3be78979f82694d68d667c8f0c2d3b1f25",1587402721,2,0,1,dataengineering
Data Engineering and Freelancing,"To me, it seems as if Data Engineering would be the perfect freelance gig. Yet, when I look for projects, I don't find as many as I would expect. 

What is your experience with this? Have you worked as a /with DE freelancers? And what kind of companies should I look for if I am looking for freelance opportunities?",1587396957,7,0,1,dataengineering
🛎️ Webinar | How are top data teams making the move to remote?,[removed],1587395812,0,0,1,dataengineering
What is your QA strategy? How do you validate data state changes on a daily basis?,"I've been having a hard time wrapping my head around QA'ing big data pipelines. At my organisation, we build pipelines in Scala using Spark SQL. It is the dev's responsibility to ensure unit testing is adequate, although we don't use coverage tool like SonarQube (we probably should). Unit testing isn't really an issue, but I struggle with testing a full end-to-end data pipeline. What tests do you perform that gives you confidence that your pipeline returns the proper data state?

For example, suppose you implement an SCD2 type transformation on a dimension. As fresh updates come in raw, you stage it using SCD2 (ie: row's endDate are closed, new records with fresh start/end dates are created with new surrogate keys, etc etc). The way I would test it (other than unit tests) would be to deploy a snapshot jar to a live QA environment which uses production data and runs the batch job on the same daily schedule as production. For a few days, reconcile the newly staged dimension with the raw data's delta changes. 

This is somewhat manageable with a dimension table, but once you get to a fact table spanning millions of records, joined to many dimensions in time, how do you ensure its state is the truth? Do you test it on a prod like environment? If so, yesterday's tests may no longer be valid as your batch jobs frequently update your data. How can you confidently tell yourself that your pipeline is good to be deployed in production?

I haven't used it, but Amazon's [deequ](https://github.com/awslabs/deequ) seems helpful in adding validation checks on dataframes. However, it doesn't help in answering the question ""is this record's state correct? Am I warehousing the truth?""

Thanks for reading, I look forward to hearing your input.",1587391878,2,0,1,dataengineering
Free Webinar on Enterprise Data Science &amp; It's Project Lifecycle and Management,,1587386826,0,0,1,dataengineering
Using heroku with AWS RDS. Am I doing it correct ?,"Inspired by the top answer [here](https://www.reddit.com/r/dataengineering/comments/g1dody/data_engineering_syllabus/), I am creating a system to learn DE fundamentals.

This is what I aim to achieve - A web app that shows cryptocurrency prices scraped from [coinmarketcap.com](https://coinmarketcap.com). Data in my web app would be updated on a periodic basis, say every 10 minutes. There will also be a button to fetch latest data.

&amp;#x200B;

How I plan to achieve it - 

Python Scraper scheduled on AWS lambda.

Inserts data into AWS RDS Postgres server.

Django Web app hosted on Heroku. Reads from AWS RDS and calls AWS lambda as needed.

&amp;#x200B;

My doubts -

Before starting, I'd like your guidance to know if I am approaching the problem correctly.

I am avoiding the whole Docker / Kubernetes setup because I want to keep the solution as simple as possible.",1587385845,2,0,1,dataengineering
25 Best Data Science Courses 2020,,1587385741,1,0,1,dataengineering
An interview with Eventador CEO Kenny Gorman about the challenges of building a managed service for streaming data to simplify building real time applications,,1587385091,0,0,1,dataengineering
Flink &amp; Hive: Flink as Unified Engine for Modern Data Warehousing: Production-Ready Hive Integration,,1587368994,0,0,1,dataengineering
"How do you deal with ""small data""?","We store our data in HDFS and have a robust pipeline to stream ""big"" (tens to hundreds GBs/day) data into it based on Kafka and custom consumer. But often this ""big"" data (e.g. clickstream) needs to be joined with ""small"" data (like promotions, or brands or whatever) that has like 1000 records a day which is kinda small to stream to HDFS (files are too small). How do you guys deal with it?",1587360176,6,0,1,dataengineering
Any recommendations on what should I be learning during the quarantine period?,,1587351542,2,0,1,dataengineering
Black Hawk Down,,1587328651,5,0,1,dataengineering
When should you use a cloud data warehouse?,I'm wondering what your opinion is on when it makes sense to migrate to a cloud data warehouse and when using a postgres instance is enough.,1587316049,11,0,1,dataengineering
Open source MlOps tool,"Hi,

I am looking for some open source mlops tool like dataiku, sas,datarobot. I would like to know what tools you use in that spacd, I am mainly looking at open source. Maimly looking at deployment of models, versioning, experimentation.",1587307008,2,0,1,dataengineering
Python Data Engineering Tools: The Next Generation,,1587302876,8,0,1,dataengineering
Practice project for spark with scala,Can you suggest me some practice projects for spark with scala. I am a beginner and want to practice more.,1587267694,2,0,1,dataengineering
Python UDFs in Redshift,Has anyone here written/use any Python UDFs in Redshift? I’m finding limited documentation and examples on this subject and figured I would ask the community here their experience.,1587247083,3,0,1,dataengineering
ETL Options," Hi all! 

I'm curious to expand my knowledge into ETL + Python. I have seen that Apache Airflow is quite popular, do I need to use another tool on top of Apache Airflow or that is just enough for robust ETL? 

With robust I mean, I can handle errors and deltas without a problem.

I'm not worried about scalability at the moment. Also, I've seen some people use Singer to standardize the consumption and output, do you recommend this tool or there are better ones?",1587242982,19,0,7,dataengineering
"Can't start zookeeper, please help",,1587230038,0,0,1,dataengineering
Has anyone successfully dockerized airflow?,"Hi,
I am trying to dockerize airflow, mainly to have not to install it on local machine while development.
My python version is `3.5.2` and airflow version is `1.9.0`.

PS: I have tried the medium articles but for some reason, they are not working for me. If any of you has done it that would be a great help thanks!",1587225246,29,0,1,dataengineering
Data Engineer opportunity with Global Media company in NYC,"A global [**hashtag#media**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23media&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D) company is hiring multiple Mid-Senior Level Data Engineers to work across their digital platforms reaching millions of users daily!  

I'm Looking to speak with level Data Engineers who specialize in [**hashtag#ETL**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23ETL&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D) and have at least 3 years experience building end to end pipelines using technologies such as [**#AWS**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23AWS&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D)**,**[ **#Reshift**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Reshift&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D), [**#BigQuery**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23BigQuery&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D), [**#Spark**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Spark&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D), [**#Snowflake**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Snowflake&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D) and [**#Airflow**](https://www.linkedin.com/feed/hashtag/?highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6657286680388730880&amp;keywords=%23Airflow&amp;originTrackingId=JsoEZbUQTSab2aZWYklIhw%3D%3D).    

They are currently onboarding fully remotely - 

 Contact: [**philip@alldus.com**](mailto:philip@alldus.com) for more details!",1587221364,0,0,1,dataengineering
Experienced DB Dev here looking to break into Big Data,"Hello DEs,

would like some tips on someone who was originally from DB Dev and made it into Big data projects/jobs. Cheers",1587219957,1,0,1,dataengineering
9 Common Mistakes with Cloud Data Fusion,,1587186166,0,0,1,dataengineering
What skills are expected from a Data Engineer?,"I recently gave an interview for the same position, cleared rounds of sql, hadoop, spark comfortably but was rejected at the end because couldn't implement heap sort for a given problem, only gave partial solution.

So I'm confused, a data engineer should be as proficient as a SE in data structures? I'm average in DS, need to know for DE how good should be my DS?

Looks like a DE is more superior than SE if these are the new standards",1587184962,12,0,2,dataengineering
$60 incentive offered for feedback from data engineers," Hey everybody, I've been using a research site called Respondent for the past few weeks to make a few extra bucks during these tough times. Some of studies on Respondent do not apply to me so I have been paying it forward and sharing the opportunities with people who might actually use them. This research group is offering a $60 incentive to hear from data engineers. If this is you, and you could use a few extra bucks, follow [this](https://app.respondent.io/respondents/projects/view/5e9509c7d850fe00389a31f6/seeking-data-engineers-for-a-quick-30-minute-remote-activity?referralCode=joshuaallessio-ff8154cce629) link and sign up!",1587175175,0,0,2,dataengineering
DE is not for me - rant,"First, sorry about this rant, but I just wanted to know if there are other people who share the same sentiment.

Before I became a data engineer, I thought data engineering is super cool. I liked the idea of using systems to build data pipelines and solving ""big data"" challenges. But now that I am actually a DE, I think DE is rather boring. And it all boils down to this: ""software engineers build the tools, while data engineers just use them"".

Let's take Spark for example. Software engineers built Spark, while data engineers just need to learn how to use Spark. I feel like it is much more intellectually challenging to build the tools rather than use them. However, most data engineers do not play a ""builder"" role. I admit, not everyone can get into the companies that actually build these kinds of tools. Maybe I feel this way because I don't fully understand the tools I'm using. After talking to people working at Amazon and Google, it seems like they get to work on cool shit like Borg and tools that help millions of developers. Of course it doesn't have to be Google or Amazon, or even FANNG. Companies like Databricks and Hashicorp make cool shit for other developers too.

In the end,  I think I enjoy building lower-level things for other developers to use rather than building pipelines or automations. But yea, I don't think DE is for me, because I would much rather build the tools than use them.",1587169546,15,0,2,dataengineering
"What do you enjoy about data engineering and what made you choose it instead of a ""sexier"" field such as ML Engineer or Data Scientist?",,1587161881,32,0,6,dataengineering
"When should I use each of these formats? Do you prefer one over the other in certain situations? The 'long' format seems much easier to work with, but you rarely see data in this format.",,1587159813,2,1,1,dataengineering
A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node. (- from appendix of Ares Le Mandat 4th ed)," This is data going back to 1896 that shows how the Dow Jones performed  during times when Mars was within 30 degrees of the lunar node. The data  contains the daily percentage changes of the Dow Jones since 1896. This  information was extrapolated from sources believed to be reliable  regarding stock market data. [https://zenodo.org/record/3711110](https://zenodo.org/record/3711110)

#",1587145710,9,0,2,dataengineering
Data Engineering MSc Dundee,Hi.  Can anyone who has been on this course please provide feedback of their experience and also how useful it was for entry to Data Engineering roles? Thanks.,1587133147,3,0,1,dataengineering
Free Webinar on Chatbots: ML Model deployment in Production,,1587129345,0,0,5,dataengineering
Scalable processing,"This is something I have written for those who would like to kickstart scalable processing with minimal cost and without entering hadoop spark world
https://medium.com/@ibnipun10/processing-at-scale-171a985c81c1",1587127899,2,0,0,dataengineering
Question for data teams working remotely,"Since I've been stuck at home for 5 weeks, I got into researching remote work for different teams, including data teams. A friend of mine sent me this article that I think summarizes it well:  [https://www.keboola.com/blog/how-to-empower-your-remote-data-team](https://www.keboola.com/blog/how-to-empower-your-remote-data-team)   
Just out of curiosity - is there anything super important that's missing on that list?",1587106077,3,0,6,dataengineering
A quick and dirty way to monitor data arriving on Kafka,,1587071653,0,0,12,dataengineering
Do you store logs on any cloud storage?,"Hey guys, I was wondering if you could take 1min to answer a simple survey.

We are checking how people analyze cloud storage objects, any insight can prove useful. 

[1-min survey](https://www.surveymonkey.com/r/9XTQZ8L)

Thanks in advance.",1587069072,0,0,1,dataengineering
Free Webinar on Chatbots: ML Model deployment in Production,,1587044933,0,0,7,dataengineering
How weird and ugly is writing queries like this?,"    SELECT product3       AS product
        , seller18        AS seller
        , date(timestamp) AS date

I'm referring both to the commas and the spaces to align the names. I have landed into a company that uses this. I really dislike it, but I don't have any objective argument to fight against it.

Do you have any, beyond that this is ugly? Is there something like PEP8 for python for SQL where it is clear that this is not the way? Am I crazy and this is not that bad?

Thanks",1587040436,12,0,0,dataengineering
Apache Flink Serialization Tuning: Choosing your Serializer,,1587039872,0,0,1,dataengineering
Do you have a DevOps team to assist in your Data engineer workflow ?,"So out of curiosity, what has DevOps engineer done for you ? Or is it expected for  Data engineer to knows the infrastructure side of things as well ? So things like building terraform module, setting kubernetes cluster and load balancing, setting up ci/cd aspect of it as well , making sure there is good security implementation in your infra.",1586988180,5,0,6,dataengineering
"Newbie Here! can someone explain the difference (or put them into context of one another) the following words? *Hive, Hadoop, Spark, MapReduce*","Much Appreciated! I am hoping to get more involved, would like to use Airflow and EMR (which I think is an AWS instance of some sort?)",1586985880,12,0,25,dataengineering
An interview with Rookout's CTO about the importance of including non-technical roles in the data collection process,,1586969705,0,0,5,dataengineering
Ten cheatsheets for data science and machine learning,,1586963247,0,0,3,dataengineering
Programming style,"Hello,

I have learned Python for data cleaning and analysis. Now that I want to develop myself to build a data warehouse I am curious on what programming style you would advise me to use on building one?

On GitHub most of the code I find start with the following structure:

1. Importing modules
2. Defining functions
3. Defining a Main function that executes the functions one after another.

I have the following questions:
1. For building a datawarehouse would you advise OOP or Functional programming and why?
2. Could you provide me a structure that I can follow to build a datawarehouse? E.g. write a file with all functions, write a main file that executes the functions.


Would appreciate any help",1586899138,9,0,5,dataengineering
Data Engineering Syllabus,"So I'm someone with little to no technical background (economics and psychology majors) outside of what I've picked up messing around with various libraries (python) on my personal projects. I've become evermore interested in data engineering and want to learn more. Problem: I have no idea where to start if I want to develop a first principles understanding of databases, querying, and optimal systems. I've taken it upon myself to build something close to 'a course', but it's mostly Medium articles, YouTube videos, and 'how to' guidelines sporadically arranged in a bookmarks folder. My one stipulation is that I don't want to just learn how to use a tool like AirFlow or Hadoop. I'm often finding websites that provide 'how to' links but not 'why' links. I want to have the knowledge and understanding that the engineers behind AirFlow and Hadoop had that allowed them to build those tools.

What I'm looking for is something like a syllabus. If you already know a place where this exists, please share! If not, taking a few minutes to write up your learning scaffold would be awesome for those of us just learning! The start can go as far back as you would like.",1586897712,19,0,28,dataengineering
Free Webinar on The Essentials of Data Science and Machine Learning,,1586870102,0,0,1,dataengineering
Pros cons OS,"I start a new job and have to decide between mac and Windows. I have used windows all my life but I've started realising that mac is a better option looking at the number of excess steps you have to take to do anything in windows. 
Thoughts ?",1586864571,8,0,3,dataengineering
Data architecture design guidelines,"Hi all,

I am part of a big organisation which has just started to take baby steps towards a data driven company. I would like inputs from data architectures who can guide me or point towards relevant references on

\- design guidelines on data architecture which includes data modelling, data governance, technological data architecture and data warehousing",1586862373,7,0,6,dataengineering
Tools to visualize database schemas from a sheet of tables and columns,"i have a sheet that has all tables and columns in database, and primary key columns.

is there an easy way to visualize them? tools i found online don't import and eat from a sheet",1586856729,7,0,5,dataengineering
Can someone help me use an Airflow hook?,[deleted],1586854077,1,0,2,dataengineering
Projects to work on,"Hi All,

I have a lot of exp in data engineering building real time platforms + cloud + DW + data lakes. I get some time ( \~ 1 hour ) everyday after completing my day to day task in my company. I am open on working on open source projects/ help/ blog, which ever way I can help utilising my 1 hour. Let use penny-university workspace in slack. I am there as Nipun agarwal
https://www.pennyuniversity.org/",1586853678,32,0,19,dataengineering
Switch to Data Engineering Role,"Hi all, I'm trying to make a switch to Data engineering and would love any advice on this. Currently I'm a Data Analyst, working in reporting. I know SQL quite well and python at a functional proficiency. I build reports in ssrs, dashboards on tableau, write python programs to run and automate data exports , and build python ETL code for some of the smaller transactional processes in my company. I'm no expert in programming,but I know what tools to use to get a job done. I have a master's in engineering and have been working for a year now. I feel I'm being underpaid for my skillset and also not very satisfied with my job (internal politics and company culture in general).
What can I do to get into Data Engineering?. I've heard bi engineer role is kind of a gateway into Data engineering. Is this true, or am I qualified enough to get into Data engineering? I see that distributed computing (spark,Hadoop etc) and DAG(airflow etc) tools are in high demand these days. I am still learning these online, but is it necessary to know these ? 
What salary can I expect for a data engineering role with my level of experience? (Los Angeles Area).
Any advice is appreciated.",1586849152,2,0,0,dataengineering
Webinar: How are top data teams making the move to remote?,[removed],1586810947,0,0,1,dataengineering
Data processing with Akka Actors: Part II,"Just finished Data processing with Akka Actors blog series, check it out. Source code available!

[https://aleksandarskrbic.github.io/akka-actors-2/](https://aleksandarskrbic.github.io/akka-actors-2/)

\#scala #akka #jvm #bigdata",1586793584,10,0,7,dataengineering
Free Webinar on The Essentials of Data Science and Machine Learning,,1586788191,0,0,0,dataengineering
"Kicking the Tires on Airflow, Apache’s workflow management platform – Architecture Overview, Installation and sample Azure Cloud Deployment Pipeline in Python","Part 1 - [http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-1/](http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/)

Part 2 - [http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/](http://bicortex.com/kicking-the-tires-on-airflow-apaches-workflow-management-platform-architecture-overview-installation-and-sample-azure-cloud-deployment-pipeline-in-python-part-2/)",1586777103,0,0,6,dataengineering
SQL to DE Pipeline transition,"Hi,

We are currently using AWS RDS Postgres DB to capture the data from our application and the major portion of data is in a very dynamic state and is stored in JSONB format  in postgres. We are currently using SQL queries on RDS read replicas to fetch the data for our analytical needs.

Some of the queries are very complex with 1000+ lines and this keeps on rising when new scenarios are added in the application.

We would like to transition this to a data engineering pipeline. Our current data volume is low(100k rows) and is not expected to rise exponentially.

The objective of moving out of SQL based approach is to reduce complexity, better maintainability &amp; to achieve error handling. What will be a good approach to handle such scenarios ? We dont have any real-time data requirements and the current thought is to establish a data pipeline and export data to S3 data lake with athena to query the data.

Can we handle this effectively by creating python/panda based data processing modules with a cron based scheduling in a EC2 box or should we be looking at pyspark based solution(Glue/EMR+Airflow etc.).

Please share your thoughts.",1586750673,7,2,4,dataengineering
What do data engineers do?,Give me examples of what projects and assignments you do all day. Give me an explanation of what you do in simple terms. Is your job stressful or fun?,1586744942,20,0,21,dataengineering
How do you save current files in entire Airflow directory to AWS S3?,"I'm running tasks on an EC2 that produce files like csv, ipynb, and pdf. I want to save all of these to an S3 bucket. Is the best way to do this via an S3 hook? https://airflow.readthedocs.io/en/stable/_modules/airflow/hooks/S3_hook.html",1586742091,5,0,3,dataengineering
What do ya'll think of SQLAlchemy?,I've heard lots of conflicting opinions on ORMs. It seems like these opinions are mostly driven by big ORM frameworks like NHibernate and Entity Framework. The one I'm most familiar with is SQLAlchemy. I've found it to be a reasonably good way to get my relational data into Python classes. I'm curious if there are any strong opinions or criticism of it from knowledgeable people.,1586719318,19,0,17,dataengineering
Learn Apache Airflow through 12 hours of hands-on focused videos,,1586716701,0,0,1,dataengineering
Master Thesis Survey,[removed],1586699498,0,0,1,dataengineering
Anyone know a good tutorial or repo that details how to build a backend for google maps?,Or something similar,1586679622,3,0,3,dataengineering
DE interviews for Big N companies,"I am currently working as DE at a big consulting company my stack is PySpark and SQL, I am most proficient in those. I do have experience in Python in general and currently learning Cloud. My background  is in CS, so I can code, but I wouldn’t say I have amazing programming skill, one of the reason I shifted for SWE to DE. My ultimate goal is to work at a Big N companies in some data centric role could be (I don’t care about title, my skills sometimes cuts through DE, DS positions). Recently I interviewed with a big fin tech company for a data engineering role, and I was kinda disappointed that I was asked Data structure questions such as heap, BST etc, and I ended up not getting it. I am sharpening my skills learning more big data (spark, Hadoop etc) and I am already pretty good in sql, and general python coding. but I am wondering if I am focusing on the wrong thing to accomplish my goal ? I know big N companies interview heavily using Data structure/algorithm for SWE, but is that case for DE/other data related position too ?
PS: Few years back I reached on site for one of the big N companies for “privacy analytic engineer” (I know,  mouthful title) position  and I was interviewed on sql coding, big data concept, domain knowledge, my skills wasn’t as sharp as now so ended up not getting it.",1586672774,5,0,12,dataengineering
Advice for transitioning from Senior Manager of a Product Analytics function to Data Engineering manager,"I’m a senior manager at a 200+ start up. Currently, I oversee a team that leverages SQL, Python, and Looker. I’ve come to the realization that this is kind of niche field and that I would be better off positioning myself to manage a Data Engineering team. I was wonder if anyone else has made a similar transition.",1586650124,1,0,1,dataengineering
Data Engineer and Python (Numpy),"As a newbie, is Numpy part of the python skills i need to know? Do you as a Data Engineer make you of it?",1586632653,16,0,2,dataengineering
BEST DATA ENGINEERING COURSE,[removed],1586624514,0,0,1,dataengineering
Masters degree for senior roles?,"Hey guys,

I worked in Data Analytics/ Business Intelligence for more than three years and shifted to a Data Engineer role one year ago. Going forward I was wondering what weight a masters degree might have if I go for a senior role in a year or two. The other Data Engineers in my company all hold a masters, even though everyone one of them told me that this is absolutely not required. Might still be a relevant topic for me if I decide to switch companies.

Whats your experience on this? Do you feel like companies value experience more or will they likely not consider you for senior DE roles if you don't hold a masters degree?",1586623994,6,0,4,dataengineering
Examples bigdata architecture,"Guys, do you know examples of big data architecture? Example: https://eng.uber.com/uber-big-data-platform/",1586622802,2,0,4,dataengineering
Is data engineering considered boring?,[removed],1586582655,0,0,1,dataengineering
Insight DE is no longer tuition free and now costs 7% of salary for 2 years. Curious about experiences from people who recently did the program.,"I had a call recently with someone from Insight, having recently done a virtual interview and the coding challenge. The guy explained to me that, starting with the Summer 2020 cohort, Insight will now require an income share agreement where, if a graduate finds a job within 6 months of completing the program and makes more than $100K, they'll pay Insight 7% over the next 2 years. He said that, this is so that (a) they can provide more alumni resources like career coaching and mock interviews, and (b) they had students who were graduating and not taking jobs with partner organizations, therefore Insight was not getting referral fees. I assume that the income share is for all of their programs, and not just DE. 

As early as 2 weeks ago, and well after I applied, Insight had a FAQ page where they stated explicitly that the program has no cost (unfortunately, they updated it within the past few days and I can't find the older version on the Wayback machine). So, this obviously makes a huge difference in my interest. I'm definitely not considering this and feel like I totally wasted my time applying. Come on, they need the money to provide mock interviews and a referral network? We all know that's bullshit. 

Just out of curiosity, has anyone done DE or any of their other programs lately? What did you think? The latest Reddit thread I've found about Insight is from a few years ago. The biggest thing I'm wondering about is: if they need students to pay because they aren't getting referral fees, is that because students aren't interested in their partner companies? I figure if the partner companies were desirable, students would have no problem working for them.",1586571138,30,0,1,dataengineering
Any plans to get best out of master’s final semester?,[removed],1586555457,0,0,1,dataengineering
Best place to practice SQL for Amazon Interview?,"Hello Folks,

I have interview on Tuesday for DataEngineering - BI position with amazon. I believe for first round will be in SQL and some Database/BI basic design questions. 

At this point in time, I want to make best use of my available time. Any recommendations to practice on SQL? 

Currently, I am working on Hacker rank. With LeetCode am not comfortable writing query in Leetcode. 

Any valuable comments or past experience would be really helpful. 

Thanks!",1586552690,23,0,1,dataengineering
How we launched a data product in 60 days with AWS,,1586544636,4,0,1,dataengineering
Looking for Data Modeling Books and Resources,About to accept a role where there is a lot of data modeling is expected. I was wondering if anyone has any recommendations for books or resources to solidify data modeling and data architecture expertise.,1586539032,9,0,1,dataengineering
Free Webinar on The Essentials of Data Science and Machine Learning,,1586526223,0,0,1,dataengineering
Building a Generic Enterprise Application Integration Pipeline,[https://medium.com/@AS870/building-a-generic-enterprise-application-integration-pipeline-74f2e9a2aa80](https://medium.com/@AS870/building-a-generic-enterprise-application-integration-pipeline-74f2e9a2aa80),1586508987,0,0,1,dataengineering
Airflow: ERROR - Can't compile non template nodes,[removed],1586499701,0,0,1,dataengineering
Hive Standalone Metastore,I can't seem to get Hive standalone to work without Hadoop. Will it run without it?,1586489001,8,0,1,dataengineering
CTE vs Subquery,"Someone just offered me a keychain with the phrase ""CTEs over subqueries"" and I had a viscerally negative reaction. 

Then I did some introspection and realized the only reason I prefer subqueries is that the very smart person who first taught me SQL preferred subqueries in Postgres. 

Data Engineers of reddit, what in your experience are the relative merits of CTE and subqueries?",1586487737,24,0,1,dataengineering
Is data engineering a career where if you find yourself in the right industry / company you can be making a lot of money doing the same exact thing with the same experience as another in a different company / industry,[removed],1586482479,0,0,1,dataengineering
Job Market in Data Engineering vs Data Science?,"With the job market going down the drain, I'm set to finish a masters in Information and Data Science at the end of this year. There are enough fundamental and elective courses in my program that I could slide into a data engineering role after graduating, assuming I did some extra prep on the side as well. There's a lot of competition at the junior level for the Data Science job market, curious whether it may be easier to get an entry level data engineering job? Also, are companies more flexible with letting data engineers work remote compared to data science?",1586482263,16,0,1,dataengineering
Launching a new Knowledge Graph tool called KgBase - Coding skills optional!,[removed],1586467504,0,0,1,dataengineering
Ways to query CSV files with SQL locally,"I'm looking for suggestions on ways to query CSV files locally with standard SQL. Basically the functionality that Hive/Presto/Amazon Athena provide, but just something that's simpler, easy to run locally, not distributed, lightweight.  Something you might reach for instead of pandas read_csv() function when trying to quickly process a bunch of CSV files locally.  Any thoughts?",1586453172,10,0,1,dataengineering
"Best way to ""host"" a SQL database locally?","Hey all

At work I made a Tableau dashboard that's live on Tableau Server and currently connected to an excel file. I would like to have this dashboard connected to a live database, but the IT team won't let me add tables/databases to the servers. 

The goal is to have the dashboard be as fresh as possible... if I just made a local SQL database that is only refreshed when my computer is on, then it's not really any different than the excel file, right? 

Any suggestions/workarounds would be appreciated!

Thank you",1586450803,12,0,1,dataengineering
Six + 1 Steps to Implement Event-Driven Architecture,,1586448138,0,0,1,dataengineering
Why Architects Need an Event Portal; Designing a System that Disseminates Real-Time Data,,1586448009,0,0,1,dataengineering
AWS DATA ENGINEER SKILLS,"As i proceed with my Data Engineering training, i will like you to advise me on the AWS skills i require to work. I am not planning for AWS certification yet but i want to first acquire the skills to be able to work first then i can think of certification. I will appreciate your advice",1586446368,22,0,2,dataengineering
How can I make the most of the AWS Free Tier to learn how to Data Engineer,"Wondering if anyone has any tips/recommendations on how to approach the AWS Free Tier account and make the most of the resources in order to get the biggest learning ROI?

What type of projects could I do? I've got some Python/Bash experience doing ETL stuff.

Thanks",1586427503,10,0,1,dataengineering
Kafka connect cluster,"Hi,

I have a confluent kafka cloud cluster running on the cloud. I have taken their basic plan. Now I need to setup my kafka connect cluster. Is there a easy way ( ansible, terraform ) which can help me create this kafka connect cluster? Please help me with any references",1586426637,0,0,1,dataengineering
Help creating sql alchemy conn without password showing,"In the .cfg file, I connected sql alchemy to Postgres with user: airflow_admin and password: pass: 
`sql_alchemy_conn = postgresql+psycopg2://airflow_admin:pass@localhost:5433/airflow_backend`

How do I anonymize this so that the password doesn't show? Do I create a `.env` file and store the password as a variable and then reference that variable in .cfg? 

I read the following but I still don't understand: https://airflow.readthedocs.io/en/stable/howto/set-config.html",1586424019,0,0,1,dataengineering
Remote Internships,"Hello , i'm a CS student currently on the data engineering road . I'm wondering about the availablity of remote Internships ? Is there any companies open to this option , considering the problem of confidentiality ! ?
And if yes what are good platforms to land one ? 
Thank you !",1586403647,4,0,1,dataengineering
Bash Scripting and Python Automation for Data Engineer,[removed],1586392452,0,0,1,dataengineering
Thoughts on Udacity's Data Engineering Nanodegree?,I saw some mixed reviews on here of it from 8-10 months back. Wondering if anyone has taken it more recently and has insights on whether it's improved or not.,1586386420,7,0,1,dataengineering
Data bricks self paced training courses.,"Has anyone tried these? They are about 75 USD each. Are they worth while doing? I have experience using other tools, but recently I've had recruiters asking about Apache spark/data bricks. Thanks.",1586383636,8,0,1,dataengineering
SQL and Pandas,"As i progress in my Data Engineer career, i only continue to see competition between SQL and Pandas rather than being complementary. What i imply is that they both seems to do exactly the same thing. I am yet to discover any major difference. I checked google but did not really find anything. 

1. What exactly is the difference between the two?
2. As a Data Engineer, when exactly do you use one instead of the other and why?

I will be glad if anyone can point out how they complement or why i actually need both",1586377635,27,0,1,dataengineering
Assessing &amp; interviewing data engineers from a distance,,1586375966,9,0,2,dataengineering
Webinar on Data Science in 2020: Myths &amp; Facts,,1586349791,0,0,1,dataengineering
Would a data app marketplace work?,"If there were some platform that allowed data engineers to write apps and sell them would you potentially use it?   Would you become a ""Data App Developer""?  I think there are some marketplaces that are specific to things like Salesforce, Hubspot etc.. but it feels like this could be done in more of a technology or company agnostic fashion. 

Build a web scraper for a particular site - sell it for $50 a month, 

Build a Mongo to MySql converter for $25 a month ( I'm making up prices. Could be free. ) 

Whatever tools you've ever built would it be interesting to potentially let people subscribe to your tooling? 

If something like this were to exist what would the platform have to do in terms of delivering the ""apps""?",1586319442,16,0,1,dataengineering
DBA to Data Engineer,"Has anyone made the transition from DBA to Data Engineer?
 
Was it a big leap or an easy move?
 
Did your DBA experience help?

What other skills did you find you needed to pick up quickly?",1586298745,12,0,1,dataengineering
Between LPI Linux essentials and System Administrator's Guide to Bash Scripting,[removed],1586284378,0,0,1,dataengineering
An interview about how Cherre builds and maintains a knowledge graph of commercial real estate data and how it enables them to answer valuable questions,,1586269183,0,0,1,dataengineering
I Need Explanation on this SQL Query Please,"Question

How many **accounts** have more than 20 orders?

Solution Query Provided by My Class

`SELECT` [`a.id`](https://a.id)`,` [`a.name`](https://a.name)`, COUNT(*) num_orders`

`FROM accounts a`

`JOIN orders o`

`ON` [`a.id`](https://a.id) `= o.account_id`

`GROUP BY` [`a.id`](https://a.id)`,` [`a.name`](https://a.name)

`HAVING COUNT(*) &gt; 20`

`ORDER BY num_orders;`

&amp;#x200B;

My questions are

1. while i can understand the reason for joining both account and order table, there was no column selected from the joined order table. So what is the essence of joining order table in this case beyond the joining
2. Even though the question ask the number of orders which seems obvious that the column to give us the number of order will be in the order table. The solution doesn't however select such column from the order table.
3. Lastly, i feel the magic here is in the COUNT(\*). My understanding of what it does here is to count the number of time an [a.id](https://a.id) appears for each name (or is it the other way?), i will appreciate a bit clarification on this.

I will appreciate explanation like i am 5 years old",1586248976,12,0,1,dataengineering
Real-time / near real-time data warehouse,"Hi DataEngineers,

I am trying to figure out how people build real-time data warehouse solution.  It seems to me that everyone is talking about it but I am unable to find a good reference that can help me understand how it can be done.

I come from the traditional data warehouse where we build dimension and fact tables in daily night job and the result is consumed. Now it seems that everyone is saying that this data warehouse should be updated in real-time with the current data integration technology landscape.

Is it really being done? How do you guys do it?

&amp;#x200B;

\*the real-time here refers to &lt; 5 minutes SLA from source to target",1586226702,30,0,1,dataengineering
Part time projects,[removed],1586221596,0,0,1,dataengineering
"Regular PLC Programmer, Advice Greatly Appreciated","Background:
I have been tasked with a huge Data Engineering project at work: Connecting our many PLCs (computers that run Industrial Machines) to some sort of database. Coming from supporting PLCs in the middle of the night, I want to keep the project as simple as it needs to be while still being usable to the people who will play with the data. This is challenging as there's a ton of software (MQTT protocol, Kafka protocol, CoAP protocol, etc...), so I would appreciate input from some people who do this on a regular basis.

The other task I really want is to use this edge device to allow others to look at / trend this data, but they would not have any SQL knowledge whatsoever.

Current Layout:
PLC -&gt; Server -&gt; Postgres / TimescaleDB &lt;- Visualization Application

I am designing the PLC &amp; Server software, so I have control over what protocol to use (Currently TCP since I'm just testing and sending the binary data over TCP is less packets than HTTP).

Data Rate: 150 - 300mb / s.  I want to be able to support ~500kb @ 20ms cycle with 6 - 12 clients to 1 server / edge device (but in all reality, this will probably be more like 100kb @ 50-100ms -- This is a project goal but I think storage costs will limit this).

Database:
Postgres with TimescaleDB extension. The table layout is column for time, column for module, and then each value from the PLC (~1000) of them is also a column (I've been asked by the team to avoid NoSQL databases like InfluxDB due to the lack of structure). The data will be analyzed by another team with AWS, but they will pick and choose the data they want to move there to use their tools on, so I figure postgres should be super easy for them to copy from.

Specific Questions:
1) Do I need a message broker like Kafka? I don't think so, since I know exactly where data needs to go, and a static data source so it should be easy to plan.
2) Does my database layout below sound normal? I know it approaches the max columns in postgres, but columns as tag names seems the simplest for any visualization software to display to the user.
3) Is there any good Data Visualization tools already built? I need it to be super super simple: Select a few values (With a description, if possible), select a time range, then submit. Get a chart that you can zoom in/out and see the values at certain times, like an oscilloscope. I have tried Grafana and it doesn't seem designed for what I need.

A goal of mine is to remain as flexible as possible. The amount of data proposed here boggles my mind, so I worry that testing works good with approach A, but then having implemented on half the lines we run into problems so we can still switch to approach b or so on.",1586220696,6,0,1,dataengineering
How much data engineering should a data analyst know?,"Im currently working in the marketing (media agency) space but would like to transition into a more analytics management position over time. But with that, how much data engineering should I know to be well-off? When engaging with analytics management (or even data analysts) what do you see as something they should be more aware of that affects your job?",1586217299,2,0,1,dataengineering
Data Horror Story Time: Rookie Data Science Mistake Invalidates a Dozen Medical Studies,,1586208894,2,0,1,dataengineering
Director of Engineering opportunity - Media company - NYC,"An AI podcast guest has asked for our help with an engineering role they are working on so I thought I would reach out to this group! 

They are a media company based in New York City. They are seeking a Director of Data Engineering to play a crucial leadership role, responsible for planning and executing the engineering needs as they go through rapid transformation. 

Their core team is made up of Machine Learning (ML) engineers and data scientists who are building and deploying ML solutions - They're looking for someone with:

* 4+ years of professional software engineering and programming experience (Java, C++, Scala, Python)
* 3+ years of architecture and design infrastructure (reliability, scalability, quality)
* 4+ years of experience as a people manager and hands-on experience leading. 

You can reach me at [philip@alldus.com](mailto:philip@alldus.com)",1586202759,0,0,1,dataengineering
Open Source Data Lineage App in Python,,1586189328,9,0,2,dataengineering
Webinar on Data Science in 2020: Myths &amp; Facts,,1586182231,0,0,1,dataengineering
Data processing with Akka Actors: Part I,"Here is my first blog post, where I described how to design actor based apps with a simple yet representative example. Anyone interested in Scala or Akka feel free to check it out.

[https://aleksandarskrbic.github.io/akka-actors-1/](https://aleksandarskrbic.github.io/akka-actors-1/)",1586173428,0,0,1,dataengineering
Simple SQL Query (Teach a Beginner),"**SELECT** [r.**name**](https://r.name), w.channel, **COUNT**(\*) num\_events **FROM** accounts a **JOIN** web\_events w **ON** a.**id** = w.account\_id **JOIN** sales\_reps s **ON** s.**id** = a.sales\_rep\_id **JOIN** region r **ON** r.**id** = s.region\_id **GROUP** **BY** r.**name**, w.channel **ORDER** **BY** num\_events **DESC**;

&amp;#x200B;

I am just trying to reconfirm my thoughts. Based the above query;

1. COUNT(\*) is applied to the column just before it ie [w.channel](https://w.channel). If yes, is this generally applicable?
2. Will it make a difference if i instead write COUNT(w.channel)?",1586124839,12,0,2,dataengineering
CDC pipeline with reconciliation for MongoDB data,"I'm working on ingestion pipelines. My company has multiple database sources used to store OLTP data. MongoDB is one of them. We need to bring data from MongoDB to Datalake for analysts to run reports on. I tried plugging Debezium - MongoDB connector for this use case. Debezium gives me complete row for create and delete records. However, for updated records I get only the changed attributes along document ID. This is causing problem in reconciling change data into snapshot. As an alternative I'm running **mongoexport** tool to fetch collections' data incrementally. But I'm looking for a solution involving CDC and reconciliation.   
Have you guys tried bringing MongoDB data into DataLake through CDC? If so, could you please help me understand how that can be achieved?",1586121615,12,0,1,dataengineering
The role of external data in navigation through crises,,1586103425,0,0,2,dataengineering
Data engineering blog,[removed],1586060149,0,0,1,dataengineering
My Journey Towards Data Engineering,"Dear all,

I am a newbie without Data or IT related background. I suddenly fell in love with Data and decided to kick start a career in Data Engineering and progress from there. So far, i have been learning python and SQL (since couple of months ago) on my own. While i still feel very shaky with what i have learnt so far, the only way to progress is by doing, breaking, making things. I am ready to push myself. I really want to learn.

I hereby seek opportunities to engage in projects for knowledge purpose. I am available 100% for free. I need you to support my career by engaging me. Plus, i will be more than happy to hook with someone who can also mentor me along the way.

Thank you",1586057631,11,0,1,dataengineering
Project Ideas for Beginner DEs,I’m a data analyst who is striving to become a data engineer. What are some aws project ideas that I could start working on and gain some DE experience?,1586057504,12,0,1,dataengineering
Is a junior DE position with cloudera good enough?,"Hi,

I'll be starting a job as a junior DE for a online gambling company in central Europe. The stack I'll be using will be:
Spark structured streaming, Apache, nifi and Apache kudu all on cloudera on azure cloud. Is this a good entry to becoming a cloud DE? I have my doubts because Cloudera is slowly becoming less important than aws.
Any thoughts?
Thanks.",1586023472,4,0,1,dataengineering
"[Question] What are the choices you pick if you want to build Stream Processing pipeline, today?","Hi,

I am working on a new pipeline and I would like to hear from some of you about the best choices to develop Stream Processing pipeline. I am getting incoming data every on milli second intervals.

I have built similar pipeline before. I used the following technologies: 

0/ Listening from RabbitMQ in older days, and applying micro-batching operations

1/ Kafka-Spark. something like this [one](https://www.baeldung.com/kafka-spark-data-pipeline) 

2/ benchmarked  AWS Kinesis. We didn't have good performance using it. 

3/ we tested Flink, but it is mainly designed for faster streaming.

&amp;#x200B;

I would like to hear about new ideas and emerging technologies in this places.

What would you use today, in April 2020, if you build a new pipeline? I would like to hear your thoughts.",1586022906,13,0,7,dataengineering
Airflow DAG produces no result despite 'success',[removed],1585954155,0,0,1,dataengineering
Webinar on Data Science in 2020: Myths &amp; Facts,,1585921651,0,0,1,dataengineering
How to properly use Apache Airflow to transform data?,I have a set of text data that I wrote a python script program (it has to call some sklearn models located in a directory within the python module). Am I supposed to call import in the DAG and place the python module with the DAG in the dags folder? Cuz airflow for some reason doesn't recognize the import and I have no idea why. I followed a bunch of stack overflow questions that said to either zip the file which still doesn't work or add airflow home and python home to path or use sys to make sure the path is set but that doesn't work either. Am I doing something fundamentally wrong?,1585900193,9,0,1,dataengineering
ELI5 the job description of a Data Engineer,"I'm a Data Engineer by trade, and I constantly find myself struggling to explain what I do to non-technical friends and family. When I'm asked about my job, most of the time I just say I'm a Software Engineer, and leave it at that. If I'm asked to explain further, most of my explanations follow something close to the following, obviously being more concise and without the technical jargon:

&amp;#x200B;

&gt;Data Engineers are responsible for the creation and maintenance of analytics infrastructure that enables almost every other function in the data world. They are responsible for the development, construction, maintenance and testing of architectures, such as databases and large-scale processing systems. As part of this, Data Engineers are also responsible for the creation of data set processes used in modeling, mining, acquisition, and verification.  
&gt;  
&gt;(from  [https://blog.panoply.io/how-to-become-a-data-engineer-a-guide](https://blog.panoply.io/how-to-become-a-data-engineer-a-guide))

&amp;#x200B;

So I challenge my fellow DE's, how would you succinctly explain to a friend / family member what we do for work?",1585894842,14,0,1,dataengineering
Help with understanding simple pipline,"I'm currently learning about DE and I need help understanding my data ""pipeline"". So, I've had simple exposure to what AWS is and we've just begun learning Apache Airflow. 

When I watch videos on pipelines, they get a little complex because they go over programs that I don't know about. For example, Spark, Hive, etc. 

I'm supposed to a simple project where I get data from the web (either a .csv from Kaggle or via an API) and then we I have to analyze the data. Because I don't know about the other products, my ""pipeline"" looks like this: 

Source (.csv) &gt; Collect (?) &gt; ETL (Airflow) &gt; Analytics (?) 

I don't know what to put in the Collect step. Since my data is already in a CSV, would this be placing it in a SQL server, for example? And for the Analytics step, would that be in Jupyter? I'm a bit confused with how Amazon would fit in here.",1585885994,2,0,1,dataengineering
How much ML should a data engineer know?,[removed],1585861910,0,0,1,dataengineering
Job Interview! SQL Server Integration Services Projects needed!,"Hello Everyone,

&amp;#x200B;

I applied for an internet providing company for data engineering. I'm mostly an expert with front-end development but I'm willing to transfer my career to data engineering. In my interviewer gove me one week to learn about data engineering specifically  SQL Server Integration Services and show him some projects.

&amp;#x200B;

Here is my plan!

1. Subscribe to a [Udancity](https://www.udacity.com/course/data-engineer-nanodegree--nd027) course for data engineering. ( I feel it would teach how to be a data engineer and put a good impression about how serious I'm about this job ).
2. Make a weather data engineering project where I'm going to collect data from weather APIs and do something with it.

&amp;#x200B;

Any suggestions. Any comment would be helpful!",1585851790,4,0,1,dataengineering
Must follow list on twitter,"Hey guys!

Noob Question, but what do you suggest to follow on twitter?",1585848210,6,0,1,dataengineering
Webinar on Data Science in 2020: Myths &amp; Facts,,1585833838,0,0,1,dataengineering
Webinar on Data Science in 2020: Myths &amp;amp; Facts,,1585833811,0,0,1,dataengineering
Apache Airflow Nested Task List,"So I have an airflow `dag` like this
```
task_list = [task1, task2]
start &gt;&gt; task_list &gt;&gt; end
```

How it works is that after `start`, list of tasks in `task_list` runs in parallel.
Now I want to do something like this:
```
task_list = [(task1a &gt;&gt; task1b), (task2a &gt;&gt; task2b)]
start &gt;&gt; task_list &gt;&gt; end
```
Which should make the dag look like pairs of tasks are run in parallel like [here](https://ibb.co/fdzDDqV).
But instead, it's looking like [this](https://ibb.co/9hZm2T4).

Is there any way to achieve what I am trying to achieve? If yes, what am I doing wrong?

Thanks.",1585811276,0,0,1,dataengineering
What does a data engineer/scientist in a political party do? - thoughts,[removed],1585785652,0,0,1,dataengineering
"What are the best choices to build Stream Processing pipeline, today?",[removed],1585772863,0,0,1,dataengineering
"Why You Need to Look Beyond Kafka for Operational Use Cases, Part 1: The Need for Filtering and In-Order Delivery",,1585770879,0,0,1,dataengineering
Need Guidance towards learning Cassandra,"Hi, I started learning Cassandra a week ago from linkedIn learning. Completed the Essentials of Apache Cassandra that covered: Architecture, Data Modeling, Data Types, Table Designing, Consistency level, and Materialized Views.

I want to deep dive further. Can anyone please guide me what resources I should see and what projects I should implement to learn more and experience the power of Cassandra?

Thank you.",1585767679,5,0,1,dataengineering
Validating data science pipelines,"How do you validate your data pipelines? 

From the engineering side, I see some push for testing on synthetic data. However, creating synthetic data is hard ( and might look completely different from your real data).

From the data science side, I see that a lot of ""data scientist testing"" is actually inspecting the results or creating visualizations. Which is kinda hard to replicate on daily batch runs of pipelines. 

I've preferred doing tests on the results, (even wrote a post about it here: [https://medium.com/me/stats/post/952c5985e070](https://medium.com/me/stats/post/952c5985e070) ), but curious about other approaches",1585765363,4,0,1,dataengineering
Having a data warehouse vs Querying data lake directly- what do you prefer and why?,"Hello,

Did any one evaluate querying data lake directly in contrast to having a need of a designated data warehouse?

What do you prefer?",1585751051,17,0,1,dataengineering
Anyone working with Ruby?,"Python is everywhere in data, but I was wondering if anyone uses Ruby?

I find it much more pleasant for writing anything custom and wrangling data.

Does Python rules due to network effects (and subsequent libraries) only? Or is it objectively better than Ruby?

Do I hinder my career opportunities by leveraging Ruby over Python?",1585740474,5,0,1,dataengineering
How do you keep development and production environments separated?,"I manage data pipelines at a company that has been scaling up over the last months. At first, I was asked to design a very flexible architecture to match the product evolution, but now this flexibility comes at the cost of reliability. When I had only a few customers, changing a thing to the pipe and rebuilding dependencies was not a big deal, but  as the product gets more and more complex, screw ups become more visible. Besides, the business now wants releases, just like any other software. So, enough with the YOLO.

I would like to know from your experience how your manage to keep development and production environments separated. Do you keep versions of your code along with versions of your data? Are there tools around that help you do that ? (I'm working with Dataiku DSS for the moment, but I'm open to alternatives).",1585729173,19,0,1,dataengineering
Using federated queries on GCP from AWS??,"hello all, our company stack uses AWS S3 for our storage of raw events logs which we then process and transfer over to GCS and finally to BQ.

We want to try and reduce costs in terms of storage on the BQ side, so we are looking at using federated queries in BQ to get the data we need from AWS within our ETL pipeline and, therefore, avoid the need to transfer some data over from AWS.

Has anyone tried using federated queries, or used them to take data from AWS? In the Google documentation I can only find guides on mysql databases, but nothing specifically on AWS?

Any help would be much appreciated!",1585721280,7,0,1,dataengineering
"Google Cloud Data Engineer, Professional Services",[removed],1585677234,0,0,1,dataengineering
Cohesion: Rethinking Workflow Development,,1585670589,0,0,0,dataengineering
Ways to automate reading emails and attachments - discussion thread.,"Hello, How do you automate reading emails and attachments? I know that you can do it with Nifi and that would be my first try since I have some knowledge on it.

What are other ways and tools which you used or would use?

Thanks.",1585665284,1,0,1,dataengineering
Webinar on The Emergence of Big Data and Its Solutions,,1585660213,0,0,1,dataengineering
An interview with Tyler Colby about his experiences working as a data professional in the non-profit sector and the challenges that are unique to that domain,,1585655794,0,0,1,dataengineering
Can I install and use Apache-airflow in Windows?,"Hello,
I've been using Airflow in my couple of projects for a while. I have always used Linux for that. Recently I had to switch on Windows and couldn't install Airflow. So, is it possible to install airflow on Windows and if yes , does it make it's usage any different than in Linux.",1585648398,4,0,1,dataengineering
Monitoring Databrick Jobs with Datadog,"Does anyone here have any experience with using Datadog to monitor Databricks jobs? If so, what's your experience like? 

My team and I are currently using it but we find it very limiting in regards to monitoring individual Notebooks.  Datadog seems better at monitoring ""general"" infrastructure metrics such as if the cluster is up or down, overall Job/Stage counts, etc but we need more depth, down to the notebook cell running and read/write speeds.

Prior to Datadog we used Slack Webhooks and the Jobs UI to ""monitor"" individual job/cells for failure and what not but our boss wants us to use start using Datadog for whatever reason. 

Just curious about other people's experience.",1585615356,0,0,1,dataengineering
How important is domain expertise/knowledge for data engineering?,,1585612223,2,0,1,dataengineering
Prefect Now Fully Open Source,,1585603343,13,0,1,dataengineering
Easy way to manage your Airflow setup,,1585602427,0,0,1,dataengineering
Robust Apache Airflow Deployment,,1585602414,0,0,1,dataengineering
Robust Apache Airflow Deployment,,1585587619,0,0,1,dataengineering
Do we really need Open Source Time Series Databases?,,1585581631,1,0,1,dataengineering
Webinar on The Emergence of Big Data and Its Solutions,,1585576581,0,0,1,dataengineering
Webinar on Hopsworks Feature Store for Databricks,,1585570723,3,0,1,dataengineering
Kafka Connect JDBC Sink: tips &amp; tricks (video walkthrough),,1585566323,0,0,1,dataengineering
Data architecture guidelines,[removed],1585564497,0,0,1,dataengineering
Data Engineer at Amazon,"I applied for Data Engineer role at Amazon. The requirement is mostly Data Modeling, Data Warehousing, building ETL pipelines and Sql. Recruiter didn't provide much information or prep materials but mentioned the first phone round is scripting. I would like to prep hard and do whatever it takes to make it in the first round. 

Any suggestions on what to expect and specific areas to focus ?",1585543341,11,0,1,dataengineering
Small-medium business - is snowflake overkill?,"Hi

I’ve been working with a small but successful production business who have a need to overhaul their reporting - initially to automate what they already have, but with a view to increasing their data science in the near future. They have several application databases that data needs to be drawn from, and it’s currently wrangled manually in excel. They’re interested in Tableau/powerBI. 

I’m wondering whether you would generally advocate for implanting a basic cloud warehouse eg Snowflake, to land and conform the data from the apps and model it before using it in the BI tool. My instinct tells me that it’s be better to start to implement this layer of separation and storage now. Agree/disagree?

Thanks",1585533571,8,0,1,dataengineering
Is data engineering a highly lucrative career to pursue?,"I'm a software engineer undergrad, considering to specialise in something rather than being a generalist

I looked at stack overflow 2019 list of highest paying skills and Scala is #1

Dice also has a 2020 report which puts Cloudera and MapReduce as 2 of the 4 highest paying skills

Everyone keeps talking about data science and machine learning being the next big thing but for that to machine learning to occur, we need data engineers to build the pipelines first

Should I pursue data engineering? My aim is to get a really high paying job\* like working at Jane Street/ hedge fund. If I aim to be a quant atm, I don't think it'll work out because I need to learn a lot of maths and stats and finance before I graduate and also get amazing grades somehow. However, I already have big data work experience and could double down on that.

\*I know I might get backlash for saying that and people telling me to not aim for money but computer science is my passion and I'd rather work at a hedge fund and get 300k for doing the same work I'm doing at a tech company.  I dont have rich parents so building a financial foundation is really important to me",1585529402,16,0,1,dataengineering
"CALLING ALL DATA ENGINEERS! Four minutes of your time can save thousands, if not millions, of lives!",Appreciate your feedback and support on how to engineer the data extracted from this survey: [https://www.surveymonkey.com/r/QFQJH7V](https://www.surveymonkey.com/r/QFQJH7V),1585516293,1,0,1,dataengineering
"How to implement HDFS in user space, so that FUSE can work with it?","https://en.wikipedia.org/wiki/Filesystem_in_Userspace says 

&gt; Filesystem in Userspace (FUSE) is a software interface for Unix and Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by **running file system code in user space** while the FUSE module provides only a ""bridge"" to the actual kernel interfaces. 

*Hadoop The Definitive Guide* says:

&gt; Filesystem in Userspace (FUSE) allows **filesystems that are implemented in user space**
to be integrated as Unix filesystems. Hadoop’s Fuse-DFS contrib module allows HDFS
(or any Hadoop filesystem) to be mounted as a standard local filesystem. Fuse-DFS is
implemented in C using libhdfs as the interface to HDFS. At the time of writing, the
Hadoop NFS gateway is the more robust solution to mounting HDFS, so should be
preferred over Fuse-DFS.

When using FUSE with HDFS, does it require  implementing HDFS in user space?

Given HDFS is a distributed parallel file system, what does ""implementing HDFS in user space"" mean specifically?

Thanks.",1585505912,0,0,1,dataengineering
Advice for someone starting out?,"Couple of things about me: 

\-Out of college, I'm working at a mid-sized tech company in a role called ""Data Science Practice"" but it is mostly a combo of Data Engineering/Analytics, I don't work w/ algorithms. 

\-I really enjoy the engineering side of it and want to pursue that, but I'm not a CS major so I do lack certain fundamentals in things like Data Sturctures and Algorithms. 

\-I'm proficient in SQL and getting better w Python but no other technical skills. 

&amp;#x200B;

Basically, with the whole lockdown, I'm trying to create a plan to both learn the theory behind the field while also having projects or certificates to put on my resume outside of work. I want to try and move to a more focused DE position in a year. What would you guys suggest?",1585505095,10,0,3,dataengineering
I created a discord server specifically for those starting out or interested in the field of Data Engineering!,"I am sure there are similar people out there looking for help/resources/a place to ask silly questions/ find study buddies, etc in this field! Let's help each other out...here is the link: [https://discord.gg/aUawNPm](https://discord.gg/aUawNPm) 

It would also be awesome if experienced Data Engineers joined and helped us out. 

Disclaimer: 

1. I am not a Data Engineer; just pushing this cause along to get into this field myself. 
2. This is my first discord server - bare with me while I figure things out.",1585436015,4,0,4,dataengineering
Change data capture,"Our company is buying CDC tool. Few companies approached and introduced us their product. Our main goal of this tool is to get incremental update from Oracle database. They all have their own advantages. But I would like to hear the feedback from the users  who used this tool. Could you share your feedback on products such as Striim, attunity, oracle etc.",1585416344,28,0,1,dataengineering
Do the things (files?) being replicated included in the state of a server?,"&gt; The central  role of the file service in distributed systems makes it 
&gt; essential that the service continue to operate in the face of client
&gt; and server failures.  Fortunately, a moderately fault-tolerant design
&gt; is straightforward for simple servers. To  cope with transient
&gt; communication failures, the design can be based on at-most-once
&gt; invocation  semantics  (see  Section  5.3.1);  or  it  can  use  the 
&gt; simpler  at-least-once semantics with a server protocol designed in
&gt; terms of idempotent operations, ensuring  that duplicated requests do
&gt; not result in invalid updates to files. **The servers can be  stateless**,
&gt; so that they can be restarted and the service restored after a failure
&gt; without any  need to recover previous state. **Tolerance of
&gt; disconnection or server failures requires file  replication**, which is
&gt; more difficult to achieve and will be discussed in Chapter 18.

What is the state of the server?

Do the things (files?) being replicated not included in the state of the server?

Thanks.",1585403621,1,0,1,dataengineering
Ideas for designing a data pipeline.,"Let's say I have a set of microservices (let's assume 5) which are emitting events for each transaction. Let's say microservice 1 emits an INIT event with ID1. This event is consumed by the service 2 and adds its own meta data and maintaining the ID sent by the service 1. Likewise, all events flow through all the services if a transaction is successful otherwise a service which marks the transaction as failed, those events will be ignored by further services. All the events are emitted in Kafka. I wish to assign a unique id to all the events belonging to a particular transaction and stream them to hadoop using spark/flink. Which one will work the best for this use case? 

Also, I assume I will need a temporary cache to store all the events of a transaction for some time until a unique id is not assigned to all the events of a transaction. Is there a better way of doing this using streaming engine like spark or flink? I read about stateful streaming and windowing. Will that help here?",1585392275,2,0,1,dataengineering
Ideas for designing a data pipeline.,"Let's say I have a set of microservices (let's assume 5) which are emitting events for each transaction. Let's say microservice 1 emits an INIT event with ID1. This event is consumed by the service 2 and adds its own meta data and maintaining the ID sent by the service 1. Likewise, all events flow through all the services if a transaction is successful otherwise a service which marks the transaction as failed, those events will be ignored by further services. All the events are emitted in Kafka. I wish to assign a unique id to all the events belonging to a particular transaction and stream them to hadoop using spark/flink. Which one will work the best for this use case? 

Also, I assume I will need a temporary cache to store all the events of a transaction for some time until a unique id is not assigned to all the events of a transaction. Is there a better way of doing this using streaming engine like spark or flink? I read about stateful streaming and windowing. Will that help here?",1585392275,1,0,1,dataengineering
Webinar - Conducting Remote Interviews: Engineering Edition,"Register here: [https://zoom.us/webinar/register/WN\_tuBKtmc1Tm67dMJcfina0A](https://zoom.us/webinar/register/WN_tuBKtmc1Tm67dMJcfina0A?fbclid=IwAR1XosLKnR-Sq4KADlWRXtuaCPvBZwo1NPZTdNSrTDpcKzHzvoXpQNpbulk)

https://preview.redd.it/i5nu9txckap41.png?width=1999&amp;format=png&amp;auto=webp&amp;s=fd41642a36b95564e21e7affb79945963b3b9df0",1585348172,0,0,1,dataengineering
Learning buddy needed,"Hello ,
   I would like to learn data engineering skills and like to join or build a group of learning buddies that helps to work on projects and bring up ideas that solve smaller probelms in data engineering focussing on databases, ETL and building data lakes and workflows using spark and airflow like tools. Any leads are appreciated. 
Thanks",1585337705,92,0,1,dataengineering
Are the servers behind a load balancer replicas or partitions ?,"When talking about load balancing https://en.wikipedia.org/wiki/Load_balancing_(computing),
do people assume the servers behind a load balancer are replicas or partitions or something else? 

- With replication, load balancer can direct a request to any server.

- With partitions, load balancer can direct a request only to one server. I imagine a need to adjust what data are stored on each server, so as to balance loads between the servers.


Thanks.",1585329624,3,3,1,dataengineering
Webinar on The Emergence of Big Data and Its Solutions,,1585313673,0,0,1,dataengineering
Does database replication require some routing for client to read or write a replica?,,1585278821,3,0,1,dataengineering
3 Steps to Becoming an Event-Driven Enterprise,,1585253555,6,0,1,dataengineering
Understanding the Concept of an Event Portal – An API Portal for Events,,1585253521,0,0,1,dataengineering
I am lost - need help designing a beginner DE project,"Hi DE, 

As a current Analyst,  I am trying to make a switch to DE and now that we are WFH, I have all the time in the world to level up. I want to design a *reasonable* capstone project and eventually put it on GitHub that shows that I can use DE/cloud tools and also SWE best practices. 

My details: 

\- I know SQL quite well. My work situation does not necessarily accommodate my role to venture out into DE-related tasks. (I know a fair amount about: postgresql, SQLalchemy, SQLITE3) 

\- I know python at basic - intermediate level (continuing to learn as I go) 

\- Was doing the data engineering course on DataQuest for a few months (found it quite frustrating tbh; but learned stuff nonetheless) 

\- learning Spark  

\- learning AWS through free udemy course as well 

I found Common Crawl an interesting source of data (since it's TBs in size) but after days of trying to connect their S3 bucket to pyspark, it's showing that it's perhaps not beginner friendly. 

**What are some reasonable DE projects I can do with my skills above, to show what I've learned in DE, cloud tools and show SWE best practices?**",1585238729,6,0,2,dataengineering
What Order to Learn?," 

Hello!  I was hoping to get some general guidance on what to learn first.  I recently accepted a position at a software company as a Site Reliability Engineer.  I will be supporting their analytics product which heavily relies upon the following technologies:  

&amp;#x200B;

\- Map Reduce

&amp;#x200B;

\- Spark

&amp;#x200B;

\- Hive

&amp;#x200B;

\- Kafka

&amp;#x200B;

\- Druid

&amp;#x200B;

\- HDInsights

&amp;#x200B;

  My question is: Given the tools/technologies above, what order should I learn these in to best understand how they all work together?  

&amp;#x200B;

I have found some good resources to learn the majority (with the exception of Hive and Druid) but need some guidance on what to learn first.  Essentially, I am just looking to establish familiarity with these products so I have a good foundation going in.  I have about 10 days to study these topics before I start so I'm trying to be as efficient as possible.  

&amp;#x200B;

Thank you!",1585232240,14,0,1,dataengineering
Need review about SimpliLearn Big Data Engineer masters program,[removed],1585219311,0,0,1,dataengineering
"Data Teams Going ""Remote"" - Challenges, Learnings &amp; Observations",,1585209045,0,0,1,dataengineering
Tokenization in a Data Lake environment,"Given that there is a lot of momentum around privacy laws (GDPR, CCPA), I am thinking that I'll have to start building in ways to tokenize PII data before I store it. Has anyone developed any patterns that they use to tokenize PII when developing data pipelines? Off of the top of my head, it seems that you would have to:

1. Read the PII value
2. Apply a hash function to the PII value
3. Store the PII value and resulting hash value into a low latency key/value database
4. Store the resulting dataset without PII in the data lake

I'd love to hear ideas on this!",1585186069,6,0,1,dataengineering
Managing JSON data (+ integration with AWS S3),"Hope I'm in the right corner to ask this. I have around 150 GB of data organized in thousands of JSONL files. Altogether, there are hundreds of millions of items. The files will be downloaded from an S3 bucket and stored on the work server. 

I need to perform fast queries on these items. The ultimate goal is to get the relevant JSON objects. I usually work with Python. I should also say that I am in a research context, not a production one. However, speed is important.

What is the best, fastest solution for this? I heard MongoDB would be the right solution. I don't think SQL would work, because the objects have nested fields and the fields coverage varies quite a lot. Are there drawbacks to MongoDB? How fast would the queries be?",1585175681,6,0,1,dataengineering
COVID-19 calculator and data,"The medical research side of this pandemic is advancing quickly. Sadly the same can not be said for political and strategic understanding. The more we know about the geographic distribution of medical conditions, the better decisions our governments can make. By filling in the COVID-19 Survival Calculator, you get an insight into your own personal risk as well as providing data that will be used to save lives.

Your privacy is important. We do not need identifiable information. You can fill in the calculator anonymously. 

Find out more visit.

[https://www.covid19survivalcalculator.com/](https://www.covid19survivalcalculator.com/)

Fill in the calculator. 

[https://www.covid19survivalcalculator.com/calculator](https://www.covid19survivalcalculator.com/calculator)

Download the dataset.

https://www.covid19survivalcalculator.com/data/download.csv

This project is being run by Nexoid, United Kingdom. We are a data and business systems company, usually working in the finance sector. We have pulled our resources to assist with the COVID-19 pandemic.

Find out more about Nexoid 

[https://www.nexoid.com/](https://www.nexoid.com/)",1585173749,0,0,1,dataengineering
how do I compare two pandas dataframes in Spark?, I need to compare two dataframes and point out columns which are different between the two dataframes. How would I accomplish this?,1585164142,3,0,1,dataengineering
SparklyClean - efficient data deduplication and supervised learning pipeline using Apache Spark,"Hi all!

I built this framework based on a VLDB paper ([http://www.vldb.org/pvldb/vol9/p864-chu.pdf](http://www.vldb.org/pvldb/vol9/p864-chu.pdf)), for the specific purpose of parallelizing data deduplication tasks. It is my very first time building something like this, so I would very much appreciate any feedback/suggestions!

[https://github.com/david-siqi-liu/sparklyclean](https://github.com/david-siqi-liu/sparklyclean)

Cheers!",1585154396,0,0,1,dataengineering
Best practice for scheduling Python ETL scripts,"We currently utilize Snowflake as our DW and would like use the Snowflake connector for our Python scripts. 

We are completely on AWS for our environment and would like to schedule some Python scripts for daily ETL. Without having someone run the scripts manually on their laptops, what is best practice to get things running on a regular basis? 

  
Let me know if I could provide any additional information.",1585150564,28,0,1,dataengineering
Self-Hosted or Standalone Spark Server?,"It looks like my company won't be approving expenses anytime soon given that our revenue is being impacted by the COVID-19 situation. However, I still want to move forward with transitioning some of our ELTs to Apache Spark (and thus convert them to ETLs), as there is actually a need for it. Does anyone know of any good tutorials or guides that they've used to set up a standalone Apache Spark instance, preferably on Docker? I've already done some searching but there doesn't seem to be a comprehensive guide on how to do so, with some steps missing here and there.",1585150393,15,0,1,dataengineering
What kind of calculations can you do with ticker data?,"Hey everyone, 

I'm currently building a streaming pipeline using Pub/Sub and Beam to ingest and stream ticker data into BigQuery for analysis as a personal project.

I thinking of also adding calculations in my Beam pipeline to add some complexity to it but not quite sure what kind of calculations I can do with this kind of data. 

The following link shows the type of data that I'm currently pulling:

 [https://docs.pro.coinbase.com/#the-ticker-channel](https://docs.pro.coinbase.com/#the-ticker-channel)",1585061753,7,0,1,dataengineering
Airflow Kubernetes Executor on AWS Help,"Hi everyone, 

I am in need of some guidance with regards to scaling our airflow deployment at my company. Does anyone have any good resources for deploying Airflow with the Kubernetes Executor on AWS?

Thank you in advance!",1585051559,0,0,1,dataengineering
Is Hadoop still the way to go?,"I’m a graduate data scientist at a company that is trying to be more innovative with their data.
There’s a lot of research here bringing in a lot of data and the current management system isn’t ideal

I’ve been looking into setting up Hadoop as a way to manage the data. We have data coming in in 10 minute chunks from a variety of sources 

What I was wondering is if it is still worth going down the Hadoop route? Or should I be looking more into Databricks and things like that?

I’m really new to this stuff so any help at all would be appreciated",1585051187,35,0,1,dataengineering
Big data or data engineering for NLP?,,1585048824,0,0,1,dataengineering
"For those of you who manage data engineers , what’s your day to day ?",,1585017905,9,0,1,dataengineering
"Where shall I install HDFS filesystem: on my existing hard drive, partition, filesystem or ...?",,1585013028,1,0,1,dataengineering
Overwhelmed :/,"I recently got my first job in both a data analyst and data engineering capacity. I'm also the first grad student they've hired,  so the onboarding was :/. 

I've completed a couple of other internships (as data analyst)  but data engineering seems like a whole new ball game. 

I got given the opportunity through a connection in the company. I'm also on a 6 months contract so if I want to get it extended I'm gonna prove to have held my weight come review time. 

Working from home because of COVID-19 definitely hasn't made my transition smoother but oh well, it is what it is, I guess.

The initial task I've been given is to understand the data pipeline and learn how to use some of the services we use on Azure (BLOB, Logic Apps, Data lake, data factory, etc). 

So my fellow Redditors, if you have any tips for me to be mindful of or any resources I could use, it'd be much appreciated.",1585006325,2,0,1,dataengineering
New data intern looking for some advice!,"Hello,

First post on a subject such as this, so please guide me to a different place if needed. I have a situation I could really use some suggestions with. I graduated college with my degree in management information systems and now I got a job as a data intern. I do not have anyone in my company to teach me as i am the first data position this company has had. 

What the company does now:

Has forms which internal clients request a job on. These forms store data in SharePoint list data. On the employee side they can continue the form to add other information needed. I have not created a data model yet but I think I may make a relational data model to help with solutions. Right now there is a JobID and a whole bunch of attributes as the data is very flat. An employee has an automatically updating excel file with this data but it takes 40 min to update and it is so impractical to work with due to how unorganized and slow it is. 

&amp;#x200B;

What I have been thinking:

Set up an azure SQL database and use flows to move the data from SharePoint into SQL and then from there into powerBI. This way I can query useful data way quicker. However the data is still very flat and unstructured and I have not done this before so I am trying to learn. I am looking for suggestions on some sort of data pipeline. I was told the data must stay in SharePoint but I can move it from there. For reference the excel file that is being used has 120,000 cells. I know this isn't that many comparatively but for how unstructured this system is it is very difficult to deal with.",1585005885,3,0,1,dataengineering
I need data engineering learning resources,"Hi there, can someone list any resources could be books or course to learn data engineering",1585000533,4,0,1,dataengineering
"Is blockchain useful for data engineering, big data and data science?","Is blockchain useful for data engineering, big data and data science? Or will it be?

I am considering whether to study its concepts, among and relative to the others (NLP, Spark, ...).
Thanks.",1584996992,12,0,1,dataengineering
Data Engineer Macbook setup- Essential tools and software,"Hello everyone,

I move to work as a data engineer at a start-up and they gave a mac book pro 13.3 inch with touch. I used windows 10 in my previous role for 2 years.

Can you suggest me tools which you use to make daily job responsibilities much faster?

How does your machine setup look like?

Thanks",1584988288,19,0,1,dataengineering
An interview with the project lead for Linode's recently released object storage service about the challenges involved in building a provider grade S3 compatible service.,,1584973020,0,0,1,dataengineering
Building your first Cadence Workflow,,1584910569,0,0,1,dataengineering
"Delta Lake, Iceberg, Hudi and Hive: Which can actually reshape Data Lake?","* Delta Lake has the best momentum
* Iceberg has best design
* Hudu has awesome performance
* Hive is fading away

[https://medium.com/@eric.sun\_39815/rescue-to-distributed-file-system-2dd8abd5d80d](https://medium.com/@eric.sun_39815/rescue-to-distributed-file-system-2dd8abd5d80d)",1584907340,0,0,1,dataengineering
Data engineer laptop setup on MacBook 13.3,[removed],1584833103,0,0,1,dataengineering
Digitalisierte Papierunterlagen Sparen Platz &amp;amp; Geld - Staub Scanning,,1584832338,0,0,1,dataengineering
"Tips/tutorials to build skills in core networking concepts (proxy, NAT, ssh tunnels, port forwarding...)","Even though I've got 7 years exp. in Big data (Hadoop, spark) &amp; Micro Svc. (k8s, docker, node.js), my background is Economics. Sometimes, I feel uncomfortable with core networking topics.
Would be great if you could share tutorials, starting from basics to advanced topics. Ideally hands on (Linux/Mac os). Thx in advance",1584806817,4,0,1,dataengineering
Do data engineers work on the field?,"I saw a diagram that showed the hierarchy of data needs, the very bottom is data recording like all the recording that would be done on field like volumes, quality tests etc.  And then the second is reliable data flow, infrastructure, storage and pipelines etc.  So I am wondering as far as organizing all of the logistics for gathering data at the lowest level, are any of you working a position where you are partially responsible both for the project management of the testing equipment etc, AS WELL as the more technically computer work of organizing all the data itself?  I don't necessarily mean hiring the crew/supervising to install the testing equipment, but in terms of being the person who oversees that all of that testing equipment is indeed installed etc?",1584733251,6,0,1,dataengineering
Big Data Analytics with PySpark + Power BI + MongoDB,"If you want to, check it out below:

[https://www.udemy.com/course/big-data-analytics-with-pyspark-power-bi-mongodb/?referralCode=F8D077DD33FFBF7B7077](https://www.udemy.com/course/big-data-analytics-with-pyspark-power-bi-mongodb/?referralCode=F8D077DD33FFBF7B7077)

https://preview.redd.it/wwyvxkfc1tn41.png?width=2560&amp;format=png&amp;auto=webp&amp;s=e345d7138bb746fcfec293574b7f2889dd6a71fa",1584700068,0,0,1,dataengineering
What's the difference between Data Analyst and Data Engineering?,,1584664957,8,0,1,dataengineering
Why It’s Vital for Companies to Focus on Data Engineering?,,1584602223,3,0,1,dataengineering
Why It’s Vital for Companies to Focus on Data Engineering?,,1584599599,0,0,1,dataengineering
airflow k8s executor vs k8s operator,[removed],1584557054,0,0,1,dataengineering
"Brown columbia is offering two micro-grants for projects that help inform the public about #COVID19. Each is $5,000, and is for journalists, technologists, health researchers, data and social scientists —any and all communities involved in covering the virus. More at 👇",,1584555191,0,0,1,dataengineering
"I’m a Data Scientist, Not Just The Tiny Hands that Crunch your Data",,1584544521,0,0,1,dataengineering
Orchestration for running real time API polling and Websocket jobs,"Hi Guys,

&amp;#x200B;

So I want to pull data from many external web sources. Examples might be a stock market data feed (using WebSockets) or polling an API every X seconds. I'm having trouble finding a tool which will manage and run these jobs for me and then publish into Apache Kafka topics.

&amp;#x200B;

Simply put I want to do these things:

* Collect data from multiple data sources (either via scripts written in Golang/Python or direct integration into the platform)
* Automatic restarting of ingesting jobs on failure
* Act as a producer into Apache Kafka Topics
* Dashboard where I can manage jobs and see any metrics about failures etc.
* Optional: flexibility in running real-time and batch processing jobs

&amp;#x200B;

I've done some research and there's a lot of different approaches: Kafka producers directly (no idea how these are run asynchronously), Jenkins to manage script jobs (although WebSockets would be long standing tasks with no end unless there's an error), Apache NiFi (had some difficult experiences with this in the past), Apache Airflow (seems to be more batch processing), Azkaban, Apache Flume etc.

&amp;#x200B;

Any help would be great, thanks.",1584527652,8,0,1,dataengineering
Real-time Stream Processing on AWS personal project,"Hi,

I'm looking to get my hands on data stream processing tools a gain some experience in cloud computing as well. At the same time I should pick my master thesis topic in the next couple of weeks. It would be really awesome if I could come up with some topic which will also help me to get experience I want. That's why I think it would be nice to build some data pipeline on AWS, which will allow me to achieve both goals. Considering my inexperience with both AWS platform and processing tools I'm interested in, it would be nice to hear your remarks concerning my plan. Feel free to address any flows or give me any advice. I will be very grateful.

My first concern is project size. My solution should be big enough (meaning I'd like it to be distributed and working with enough data so use of big data tools could be justified). On the other hand I want to keep project price as low as possible ($100 a month tops during next 6 months). I thought I could take a look into real time streaming both because it's interesting and could be potentially cheaper since I won't need to keep large amount of data stored and could anytime spin up and down my project cluster.

Real time streaming brought me to my next problem. What data could I use? There is obviously twitter streaming API, which seems like a possible choice. Do you guys have any idea where I could get some interesting, free streams which will have high enough volume? Any examples where people produced the data from IoT devices or any APIs I failed to google?

My other concern is tool selection considering I only have experience with spark batch processing from the tool set I thing I could use. I bumped into blog on AWS and one of their use cases describing [real time stream processing Apache Spark Streaming and Apache Kafka](https://aws.amazon.com/blogs/big-data/real-time-stream-processing-using-apache-spark-streaming-and-apache-kafka-on-aws/) . My project prototype will then  probably store the data somewhere (not sure what DB I will choose yet, since there is a big question mark concerning data source) and then I wanted to create some simple web app where some basic info/charts will be displayed to illustrate pipeline functionality (let's say some word trends if it's twitter data). Would Apache Kafka make sense for both using with Twitter stream or let's generating my own data through Kafka producer app? Is Spark Streaming on EMR wise choice for data processing? What should I consider when choosing DB? Do you know any good examples of personal projects that leverage some of the mentioned tools? I know this question is difficult to grasp, but I feel like I need some feedback after initial googling couple of sessions. Thanks in advice!",1584484211,12,0,1,dataengineering
"A blog for those that have to handle vague and constant stakeholder requests: ""Just One More Stratification! Or: How to say “no” as a data person""",,1584481254,2,0,1,dataengineering
An interview about the CouchDB document database and the work being done to rearchitect it to run on top of FoundationDB,,1584472820,0,0,1,dataengineering
SageMaker: automatically stop your instances when idle,,1584464838,0,0,1,dataengineering
COVID-19 vs History - R Script (If you have questions or want the script just ask me),,1584444313,2,0,1,dataengineering
Flexible and rapid data modelling,"Hey guys,

We're designing our DWH stack from the ground up pretty much. This is an environment which sees rapid change both in terms of data sets and data sources (but mostly data sets and shifting user requirements). Were intending to use Snowflake with an ELT tool such as FiveTran or Matillion.

We're currently deciding how to model, at the moment our legacy DWH uses the fact/dimension star schema model and an ETL process. This isn't very flexible and it's time consuming.

We've considered Data Vault 2.0 as it seems a lot of the benefits align for us, I'm just concerned that whilst it may be flexible, its actually not going to be quick, and it'll take ages to implement!

What are you guys using? - The key points here are very rapid time to insights and flexibility above everything else... tempted to just use Views in Snowflake tbh...!",1584436272,9,0,1,dataengineering
What is a good resource to learn about spark/Hadoop configuration,"I recently started my DE job but still have not had a good grasp of topics on Hadoop/spark such as

&amp;#x200B;

1. Clusters
2. Memory allocation
3. executor memory
4. executor cores
5. driver memory
6. memory overhead
7. memory.fraction
8. dynamic allocation

&amp;#x200B;

what is  a one/two stop resource I can use to gain a good understanding of these?",1584383316,5,0,1,dataengineering
"Been in data science for years, want to know more about data engineering. What should i learn?","Basically for years, ive been writing python and r code on my local computer doing regression modeling and some occasional machine learning techniques.

I mostly just have used basic sql queries with a sql server database or accessing redshift thru sql workbench to query.  Nothing more advanced.

So, obv have a basic understanding of databases. SQL, python, and R.

What do i need to learn to become more knowledgeable about data engineering?",1584383231,5,0,1,dataengineering
Big Data and Analytics in the COVID-19 Era,,1584377188,1,0,1,dataengineering
[Newbie Question] Need help with public S3 bucket,"Hi all, 

Beginner DE here. I am having trouble finding a dataset \~100gb to mimic real big data processing. 

I found the AWS Open Data Portal and a dataset I want to explore further; link here: [https://registry.opendata.aws/ichangemycity/](https://registry.opendata.aws/ichangemycity/) 

My issue: 

The readme says the data is available to access for free on S3. I've created an AWS free tier account and I am on the S3 console but have no idea how to extract the above dataset. Any help is appreciated!",1584315926,7,0,1,dataengineering
Discord Server for Engineers,"Hi,

Check out this engineering study discord server, there are many computer engineers on there, and on top of that there are other engineers such as electrical engineers, computer engineers, aerospace engineers, mechanical engineers, civil engineers and so on!

https://discord.gg/UCbmAyv

I found this server the other day, and I use it to ask for careeer-advice. I'm currently in my third year in uni studying to become an engineer. There was a conference meeting on this server, which was pretty cool and the best thing about it was that the conference meeting was held by a professor in engineering. I asked him careeer-advice and much more, and I really learnt a lot.",1584098321,0,0,1,dataengineering
Looking for a little advice,"I've been working as a software engineer for about 8 years on full stack, though I concentrated mostly on the server side. About a year and a half ago while working for a start up I joined their data engineering team in a ""why not"" moment and now I lead the team. I still feel like I have some huge holes in my resume, but am wanting to pursue further into data engineering as I'm enjoying the mix of technologies.

I'm currently maintaining a pipeline on AWS that consists of kinesis/firehose, s3, lambda, rds, elasticsearch/kibana, quicksight, node js on docker ec2, a react front end application, step functions and some glue (pyspark). I'm working right now on designing a new data lake using AWS lake formation with glue workflows and event bridge for operational analytics.

Most of the work I've been doing has been in pyspark and terraform, with some typescript in lambdas and such.

My question is.....is what I've been working on technology wise valuable? Because I'm pretty much solely responsible for our data pipelines, I don't really have any other viewpoint and I have nothing to go off of. I started taking a data engineering multi-quarter certificate at a local university trying to fill in any gaps, but it's been hard to meet the commitment with my work responsibilities. What am I missing in my knowledge base and what skills should i be trying to shore up?",1584082704,7,0,1,dataengineering
Brooklyn based Start-Up looking for a Lead Data Engineer,"A Brooklyn based Startup is looking for a Lead Data Engineer as a first hire to build their in-house real time streaming platform! 

This role will have a significant impact on the firm's success and will have a direct path to leadership as the project scales. 

Tech stack: Python primary coding language, spark, hadoop, Kafka for data streaming, Snowflake, Airflow / Luigi  

Team: Total engineering is a team of 12 at the moment that will offer support to various aspects of the project but this is the first hire on the team directly responsible for building this platform. 

I would love to hear from anyone who wants to learn more about the position! 

Contact: philip@alldus.com",1584035821,0,0,1,dataengineering
Fav. tools/resources for system marrying/data integration?,"Hi there!

got  a super exciting interview for a data scientist job. Going to be  working with older school IT and senior programmers. I am a data  scientist - modeler and statistician at heart - so not super versed in  data engineering.

Going to have to  confront flat files, old Oracle databases, billing databases, and huge  amounts of time series from  automated meters (it's a medium big  electric utility) likely out of a commercial product. I have a hunch that this utility is literally drowning in data and doesn't know what is useful/not. 

Any  advice on tools or processes I should look into and brush up on for  data systems integration? I need a crash course. I won't have to do all  this work myself, but I *really* need any kind of guidance so I have some familiarity.

Thanks!",1584031405,4,0,1,dataengineering
Fav tools/resources for marrying systems/data integration?,"Hi there! 

got a super exciting interview for a data scientist job. Going to be working with older school IT and senior programmers. I am a data scientist - modeler and statistician at heart - so not super versed in data engineering. 

Going to have to confront flat files, old Oracle databases, billing databases, and huge amounts of time series from  automated meters (it's a medium big electric utility) likely out of a commercial product. 

Any advice on tools or processes I should look into and brush up on for data systems integration? I need a crash course. I won't have to do all this work myself, but I *really* need any kind of guidance so I have some familiarity.  

Thanks!",1584031276,0,0,1,dataengineering
Sending data from BigQuery to REST API endpoints with Google Cloud Functions,,1584017107,1,0,1,dataengineering
[Academic Survey] Decision making &amp; Data Analysis,[removed],1583991450,0,0,1,dataengineering
[Academic Survey] Decision making &amp; Data Analysis,[removed],1583991241,0,0,1,dataengineering
Aws vs Azure for data engineering,"Hello everyone 

I joined reddit a few days ago as I have started to train for data engineering. Recently passed the AWS Cloud Practitioner exam but am torn between choosing Azure vs AWS as the primary cloud for data engineering. From my research it appears that Azure is easier to work with since it’s GUI based and is heavy on T-SQL. Besides Microsoft has always been the top vendor for business intelligence and data warehousing. However AWS is the dominant cloud platform but is more Linux and Python based as it’s more open sourced. I have a background in business intelligence on sql server (around 2 years). I know python but don’t have comparable proficiency to sql. 

Just wanted some advice on which platform to choose which will be easier to adopt to as well as be job safe

Thanks!",1583981036,6,0,1,dataengineering
Amazon Redshift launches pause and resume,,1583966062,3,0,1,dataengineering
questions about an application,"It's not really related to the work of data engineer but I don't know where to ask this question. 

I applied for a Hadoop data engineer position

The first interview went very well I met a data scientist

The second interview was technical, I had Python and ... data science questions.

According to the feedback from the data scientist, I succeeded very well in python questions but not in data science. I tried to justify myself by saying that I had prepared myself for Hadoop, SQL, Spark, Scala questions etc.

I had this interview last Wednesday and she told me that I would have an answer on Friday next week.

First of all what does this mean? Did they not like my profile and tried to give themselves as much time as possible before making a decision and if possible seeing the other candidates?

And today I just saw that the offer for the job has resurfaced in the recruitment sites. Does that mean they even prefer to find other people? I don't know if they recruit several people but I think only one.",1583959667,3,0,1,dataengineering
Last minute crunch for technical interview,[removed],1583943196,0,0,1,dataengineering
Database/model schema synchronization has arrived in ERBuilder 3.4.,,1583932660,0,0,1,dataengineering
Data Infrastructure Setup in Your Firm,[removed],1583913028,3,0,1,dataengineering
Is there any better ways to study distributing tools?,"I am a newbie in data engineering.

After building a simple data pipeline with apache spark, I have a strong feeling that there's so many tools to study in DE.

We need to learn both SQL and NoSQL databases, batch processing tools like Hadoop, Saprk, streaming tooks, Kafka, spark streaming, flink, scheduler tools like airflow, etc.

One thing is most of these tools needs a distributing architecture to have full functionality for processing big data, but how can we get a hands-on learning with these tools without a cluster? I mean of course we can use AWS but it still costs a fair amount of money.

Just want to know if there's better ways to learn these tools, especially see how they perform in a distributing enrironment.",1583906840,11,0,1,dataengineering
Need feedback on a project,"I am working on my first data engineering project that reads data from 3 different map service APIs and writes that raw API data to a Kafka broker with 3 topics, one for each of those APIs.

Here is what I want to achieve

1. Fetch API data as is and write it to Kafka
2. Use Spark to clean and transform, then write it to a database
3. Query DB / visualize data / make a recommendation engine

I have completed \[1\] of the 3 above. I want some feedback before moving forward and implementing this whole idea. Should I clean data before writing to Kafka (in that case, will I really need Kafka?). Should I choose a graph based database? Since I want to be able to derive meanings between two points.",1583905043,7,0,1,dataengineering
GCP or AWS for building portfolio and getting cert ?,[removed],1583852052,1,0,1,dataengineering
Do I have the skills for data engineering? What can I work on? Resume attached,[removed],1583851824,2,0,1,dataengineering
Need a tool to run queries over multiple sources of data?,"What are some tools that can let me run queries like sql on multiple sources of data. These sources are aws rds, excel and Google analytics.

Do I have to manually combine these sources first and then only can run analysis?",1583833555,3,0,1,dataengineering
BI Manager transitioning to Data Engineering Manager,[removed],1583802460,0,0,1,dataengineering
An interview about how a data hub architecture can reduce the overhead of managing data governance and compliance across an organization,,1583768827,1,0,1,dataengineering
Any production feedback on Prefect workflow management ?,[removed],1583766588,6,0,1,dataengineering
Language server protocol for query languages?,"In recent time, there have been a surge of language server protocol and rightfully so! It is amazing to aligned the tooling around a language.

I would love to see the same for sql/database standards where you can just download the psql extension or the redis extension on any ide and start smashing away.

Is there such a thing as a lsp for query languages?",1583736902,0,0,1,dataengineering
Apache Beam: How to input data from a URL instead of a file or other source?,"Hi

&amp;#x200B;

I am fiddling with Apache beam, is there any way to feed html scraped data directly into Apache beam pipelines instead of CSV files?",1583736526,0,0,1,dataengineering
Personal experience with Dagster,"Hey all, 

Just wanted to give my experience on dagster, as I 've seen a lot about it, but not a lot of personal experience using it. I wanted to try it on a simple mock data warehouse architecture I built out. Basically, a Postgres instance hosted on RDS, with some sample data, extracting it to S3, and loading into Redshift. From there, maybe some dbt models to create some additional views and tables.

Overall, I really like the approach and architecture of the project, but I don't think it's something I would use on the job in a production setting just yet. The tutorial goes in-depth, but still manages to skip some important things, such as how file storage/filehandlers work. There's zero documentation on their AWS plugin, and many of their classes are still only roughly documented. Some of the tutorials lack clarity as well. They might introduce an entire code block, but not point out which line they are referencing to in their description of  a process. Worse still, their airlines example jumps around from file to file, without giving a clear overview of an entire file. So many parts of their demo are not even covered. 

I really like the idea of being able to abstract out resources. Being able to point to a local Postgres instance vs a Redshift warehouse in production and having these configured appropriately is really appealing and makes testing so much easier. The problem is that it's not immediately clear to me how do properly configure resources. The documentation could use more explicit examples. There's configs, inputs, outputs, etc. In one case, the keys for an S3 object are Bucket/Key, in others, they are bucket/key (no caps). Again, it's really hard to look up documentation on these. I think part of the problem is there are many different ways to do things, that it gets hard to keep a good mental model of what I should be doing.

In the end, I got a very small working example  working of pulling data from postgres, to a local csv, and then uploading that to S3. I didn't continue any further, since I felt I had spend so much time on it already. With all that said, I'll be keeping an eye on this project, and really hope for its success. It hopes to solve a lot of pain points I have with Airflow, and I expect it'll continue to improve.

Let me know if you have any questions!",1583733392,3,0,1,dataengineering
Poll: Anyone here work for a company that has a SQL style guide and enforces it?,,1583720109,29,0,1,dataengineering
How to best prepare for DE onsite interviews,"Hi everyone!

This is my first reddit post so if the format isn't 100% I'm sorry :3

For a while, I've been working at a startup and managing everything related to data . I began as their intern who didn't have extensive knowledge in CS. I had taken a OOP, Algorithms (a very bad one) and Database 1 class before the internship. I knew close to nothing when I started and learned everything I know about data engineering on the job (aka google &amp; reddit). Didn't have a lot of mentorship since the team was was super busy and small. Felt like I was a chicken with its head cut the whole time I worked there but grateful for the experience because i learned a lot FAST. 

I'm currently on the job hunt and seem to be doing an OK since I have several onsite interviews coming up. I have about a week of preparation before they start and not sure what the experience is going to be like because I've never had a formal onsite interview before! These are what I believe are my current strengths and weaknesses:

*Weaknesses: big data pipelines, data modeling best practices and BI analytics and tools.*

*Strengths: SQL, Python, AWS S3 &amp; EC2, linux, OOP/algorithms, web crawlers and reg. sized pipelines*

Does anyone have suggestions on how to prepare based off the weaknesses I listed? Does anyone want to talk about their onsite interview experience? Does anyone have nice words of encouragement? 

Anything is appreciated!",1583645888,8,0,1,dataengineering
Accidental Data Engineer Looking for Advice,"Hello!

I started working as a data analyst for a relatively new company. Funny thing is that the company does not have a handle on their data. We have multiple APIs (around 15 APIs) that work as the back-end for our main product and almost every API has an associated database. I have convinced the higher-ups that we need to create an analytics environment. That means extracting data from the production databases and moving it into an environment that is analytics friendly. Luckily I have some experience with ETL and other data engineering tasks, but I am still unsure of the best option or what is common practice.

What I need advice on are tools that exist to facilitate extracting data from multiple sources and moving it into one location. I imagine that the data would have to be moved into a staging area, transformed into an analytics friendly format, and then moved to some sort of data warehouse. All the production databases are on Amazon Aurora, so using tools that are AWS friendly will make it an easier sell to the engineering team (AWS Glue is on my radar). Their main concern is going to be how will it affect the performance of their live services. Any advice is appreciated.

Another thing to note is that there are DBAs who work for this company, but they are all in another country. Initially I sought their support with this, but I hit some roadblocks that I will just describe as ""company bureaucracy"". I do have the support of the local engineers, but if I do not lead these efforts and push for it to be done, then no one will and it will not get done.",1583631370,0,0,1,dataengineering
What is your opinion on modern data engineering Traditional ETL tools like Informatica/Talend etc?,[removed],1583626278,1,0,1,dataengineering
Should I jump ship for a hedge fund?,"TLDR: would you leave your comfortable DE role to work for a hedge fund offering a significantly higher salary?

I'm fairly happily employed, id say an 8 out of 10. I do still give recruiters the time of day if they bring a position that sounds amazing to my attention. This week, a recruiter messaged me about a DE role at a hedge fund that primary focuses on Apache airflow with a total compensation almost double what I make now. 

Airflow is the tool I use the most at my current position and iv really taken the time to become the ""airflow specialist"" on my team so this seems like a great match. Also....the money!

My question is, what's the catch? I consider myself a mid level DE and have a masters in analytics. They seem to think I'm qualified for this job that pays significantly higher than my already six figure salary. Are the hours going to be crazy? Is there lots of stress and pressure working at hedge funds? Is the overall quality of life much worse?",1583591856,0,0,1,dataengineering
Just landed my dream data engineering job!,[removed],1583587003,0,0,1,dataengineering
Airflow vs Azkaban,What’s the max scale one has reached with Airflow in terms of number if DAGs.. I have read somewhere Azkaban scales really well and can run thousands if DAGs in parallel.. Has anyone tried both and what do you think puts Airflow in the advantage,1583579621,12,0,1,dataengineering
Label your data with machine learning,,1583578602,0,0,1,dataengineering
Airflow + Load Balancer (for tasks),"Hey all,

I am experimenting with Apache Airflow and the different executors it provides. Particularly, I am interested in the Dask executor, and pretty recently in the Kubernetes executor.

I've opted in for the Dask executor because of its load balancing capabilities;  to distribute the jobs depending on the current load of the worker nodes. From a layman's observation (just using htop), the jobs are distributed like fine, but, even when a node's CPUs are all occupied, it will receive new tasks (some lightweight, and some high CPU-bound).

I think this behaviour is governed by the `parallelism` setting in airflow.cfg.

My question is, is there a configuration parameter/setting either in Airflow or Dask that can put the tasks on ""hold"" (I know Dask doesn't support queues) until the resources are freed? Is switching to the Kubernetes executor a better solution to utilize the resources of the cluster\*?

&amp;#x200B;

\*  I am setting the Kubernetes cluster right now (first time also :P)

&amp;#x200B;

Any ideas and comments are most welcomed! :-)",1583518674,6,0,1,dataengineering
Data Modeling for Personal Project,"Hello DE Community,

I'm working on a personal project to build a relational database for some sports data and I'm confused about some of the relationships between the entities. I've attached an image with the model.

Where I'm getting confused is with the relationship between Teams and Games (business rules below).

If I wanted to query for all games for a specific team I'd have to join the games table twice, because a team could be home or away. In terms of cardinality, for a game to exist there needs to be a pair of teams, and only one pair of teams can play in a single game (one-to-one). 

Is there an approach I could take so that I could make the querying of games for a specific team simplified?

Business Rules:

* A team is a franchise during a specific season
* A team cannot exist without a franchise
* A franchise can have a minimum of one team and a maximum of many teams
* Teams play each other in games
* A single game is played by exactly two teams
* Pairs of teams can play each other multiple times
* Games are played in venues
* For a game to exist is needs to be played in a venue
* A venue could be used for a minimum of one game or many games

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/fpxef3u2g2l41.png?width=705&amp;format=png&amp;auto=webp&amp;s=5368709b65a1996e91f9b78027ca235ac3a51200",1583507395,15,0,1,dataengineering
Big Data Analytics with PySpark + Tableau Desktop + MongoDB,"If you want to, check it out below:

[https://www.udemy.com/course/big-data-analytics-with-pyspark-tableau-desktop-mongodb/?referralCode=348A25E57F2654D3F0DA](https://www.udemy.com/course/big-data-analytics-with-pyspark-tableau-desktop-mongodb/?referralCode=348A25E57F2654D3F0DA)

https://preview.redd.it/4769we4ta0l41.png?width=2559&amp;format=png&amp;auto=webp&amp;s=69d740fbbc307382ec260d47c0cab7e53c2bee06",1583480455,0,0,1,dataengineering
I Created A Free SQL 101 Course For Those of You Looking to Get Into Data Engineering,,1583471647,1,0,1,dataengineering
Route to data engineering,"So I’ve been a BI developer for about 10 years now. I’ve been pretty much in the Microsoft stack. I’ve been trying to get away from visualization and focus more on the data side of things like etl, data warehousing. 

How do I get into data engineering? I’m studying for the Azure data engineering certification to cover some knowledge gaps and because of my affinity to Microsoft my entire career. 

Couple of things I’ve noticed. 
As a BI developer, people want me for my visualizations which I’m trying to get away from. It’s just not for me. 
Companies hiring for data engineers won’t give me he time of day because I haven’t had that title even though my experience is almost similar to what they’re asking for.
Also companies that hire data engineers tend to do stuff on prem instead of in the cloud. Is my Azure certification going the wrong path?",1583431983,10,0,1,dataengineering
Got 3 offers in 4 days for data engineer after an year's stint of Jr.Data engineer role-AMA,[removed],1583426432,0,0,1,dataengineering
"How to Process and store data over time(ex: per hour, week, month, year)","I want to process a raw log file and get some info. For example unique device id/account id/request id connected over the last hour. I would run that every hour and build a model of how our app would be used.

How could I apply that to say 24 hours, a week, a month. If I just added the total of each category for each hour it wouldn't give me unique devices or accounts. As they would be counted multiple times. Say a device is connected for 5 hours. It would be counted 5 times.

My current idea would be store each unique device id/account id/request id per hour and then have a job run once a day/week/month that takes those lists for each hour and computes the data per specified time frame. That could take up a lot of space though. Is there a better way to do what I want?",1583423959,7,0,1,dataengineering
Start Learning Apache Airflow! 12 hours of videos from basic to advanced,,1583399674,0,0,1,dataengineering
Loading Data,"When loading data from a source system in your data lake(we’re staging data to s3 and accessing it via snowflake):

Do you load all tables from your source system into S3 or do you go through and identify the ones that you think will be used and only load those?",1583370639,9,0,1,dataengineering
Hiding database credentials in python script,"Hello,

Please your advise on how to hide database credentials  when pulling data out of a sql database using python?",1583348037,16,0,1,dataengineering
Breaking up the Airflow DAG monorepo,,1583340910,0,0,1,dataengineering
Data warehouse logical organization,"Would anyone be open to discuss or share some resources regarding designing logical units in data warehouse? More precisely, I am thinking about different layers and who has access to which data: 

1)  Data lake - we import all of our raw data here. Should we allow any external tool or report to access this layer? What about ad hoc queries? 

2) Transformed raw data - we clean raw data and convert it to columnar formats. Should BI tools access this data? 

3) Various transformed data - this is basically data transformed for easier querying - star schemas. 

4) Reporting data - tables used for various BI tools, usually aggregated 

5) Serving data - data used by product teams -i.e. product recommendations, churn prediction, etc  

What I am unsure about is: 

\- For example, BI tools sometimes need data from transformed and raw layers. What should I do, allow that or copy that data to reporting? We are talking about large tables so copying would increase costs and using them directly breaks encapsulation - we can't easily modify that tables without breaking lots of reports.

 \- Does this separation make sense, is there something I am missing?",1583330932,3,0,1,dataengineering
Understanding the Customer Journey Through Data,,1583321049,0,0,1,dataengineering
Beginner Project ideas recommendations ?,"Hi all, I am a beginner to the field to data engineering. I have learnt how DFS works and navigating through HDFS, applying mapreduce jobs to files on HDFS, write Hive and Pig queries for the same. I am also now familiar using spark (pySpark) to work with data in AWS S3 using AWS EMR. But I haven't been able to think of a project that can integrate all of this together. Any recommendations of ideas to implement that is capable enough to be put on the resume for entry-level jobs?",1583294081,3,0,1,dataengineering
Data Engineering course: trying to gauge interest,"Hi all, I am a data engineer with 6 years of experience. One thing I have noticed is the lack of a course that covers the basics of data engineering for beginners (the ones that do are very expensive). I have always been interested in teaching technical concepts, and wanted to see if there would be any interest in such a course. Did not put a lot into design as I wanted to get feedback only on the **CONTENT**.

Would really appreciate any feedback on  [https://josephs-blank-site-4bcce3.webflow.io/sde1](https://josephs-blank-site-4bcce3.webflow.io/sde1) 

Thank you all :)",1583290355,90,0,1,dataengineering
Working with image data,Does anyone have any advice or resources for working with large image data? New to the field and just looking to catch-up,1583286223,0,0,1,dataengineering
Have any of you folks used Pentaho kettle/ PDI?,Started a new position at a FAANG company using it. Not sure how I feel about it and wondering what others’ thoughts are.,1583283527,3,0,1,dataengineering
I’m hiring a Data Engineer III,"Hi Everyone, 

I’m an IT recruiter, and I am looking for a Data Engineer III with USA work authorization who is in Seattle or will be willing to relocate to Seattle. This position is a 6 month contract. 

The ideal candidate has skills with: 

- ETL 
- Python 
- Data Modeling 
- 5+ years SQL, SQL Tuning, Oracle, OLAP Big Data technologies 
- Proficiency with Linux 
- Experience with Hadoop based-technologies like HBase, Pig, Hive, and Spark 
- Experience with Amazon Web Services 

If you’re interested please reply to this post. Here is the link to the website of the company I work for - S.com-USA.com",1583264969,14,0,1,dataengineering
Career Advice to switch to Data engineering.,"Hello , first time poster here, I need some career advice.

I currently have been a ""business analyst"" for 5 years now, 28 y/o.
Apart from the domain knowledge/business knowledge I have skills in VBA, SQL, Python(mainly pandas and numpy), and BI tools(tableau and some others).

I currently make below avg salary in my city and figure I need to switch elsewhere where I can grow. I believe there is little growth in my current role.

I have been applying to data engineer/data analyst roles with no bites. I feel like I would be good in a data analytics type of role.

Is their anything else I can be doing/learning to better my shot at landing something? I kind of am opposed to doing my masters at this point and need some advice.

Thank you.",1583254078,15,0,1,dataengineering
An interview about the ksqlDB platform and the unified experience that it provides for building stream processing applications on top of Kafka with SQL.,,1583202663,0,0,1,dataengineering
The drivetrain approach to create data product,,1583148271,0,0,1,dataengineering
Resourcing a data engineering team,[removed],1583141405,0,0,1,dataengineering
Develop python packages for Databricks,"A common struggle that I have seen in several companies is the way teams manage tons of code in Databricks notebooks.

I’ve tried to introduce a better way of working by converting notebooks into python packages. The advantages are worth it in my opinion, but my team is less enthousiastic. 

I wrote a blog post about this way of working, and I thought I’d share it here to get some feedback.

https://menziess.github.io/howto/enhance/your-databricks-workflow/

The advantages I see using python packages over Databricks notebooks:
- No complicated cross dependencies between notebooks anymore (someone editing some utility notebook could cause multiple other notebooks to fail)
- Full git control in your IDE
- Iterating faster by developing locally with small data
- Being able to run code directly on Databricks from your ide to prove that your code scales
- Package versioning over notebook copying
- Linting
- Tests

Any advice on how to motivate my team to try this is also appreciated :)",1583111493,9,0,1,dataengineering
Help understanding airflow,"I haven’t used it but I’ve seen a lot of people talking about automating tasks using Apache Airflow. 

I’m trying to make sure I understand the differences between it and cron or APScheduler. 

My understanding is it’s a cron job with a built In GUI (using flask to serve it up) and some error handling to retry tasks when they fail. Also having some extra sauce as well to come up with complex dependencies between different tasks. 

Is that about right?

————

Really I’m curious as to how it compare to an app that I have in production on a relatively small scale. My app does scheduled data pulls from an API that stores them in a postgres database using APScheduler. The API calls are quite slow so I don’t do it in real time (eventually I could async it maybe). And then create what amount to dashboards in a flask app. I have all of this deployed in a single Heroku app. 

The way I am understanding it, if I wanted to do the same thing with airflow I would need to separate apps. Is that right? Is there a way I could integrate airflow into my current Flask app as a blueprint?

Thanks. I hope this is a reasonable question to ask here.",1583107688,15,0,1,dataengineering
Batching data to S3 with fluentd,,1583099087,1,0,1,dataengineering
Checking for Data Issues,"Hi guys,

Let's say I've got some data from Google analytics and want to model the data.

What checks should I do to check the data quality/checking for any issues?

New to engineering, thanks!",1583086380,1,0,1,dataengineering
Does this qualify as a Data Engineering Project?,"I'm an undergraduate Software Engineering sophomore and I've been reading up as much as I can about DE. From what I've understood, I've set out to carry out a small project that involves some DE concepts (or at least I think they are) and I wanted to ask whether what I'm planning to do comes under the vast umbrella of Data Engineering.

This is how I've laid the blueprint out and what ""concepts"" I would like to implement. I understand that all these things are a massive overkill for the project but that's the main reason I'm doing them, so I can focus on the DE aspect of it more than the functionality itself.  

**Deployment**:

* Writing unittests to validate data
* Containerizing the API using Docker
* Creating a CI/CD pipeline
* Deploying the project on AWS

**A cron job that carries out the following tasks:**

* Parsing/cleaning data sets from the NORAD page
* Storing/removing parsed data into a SQL/NoSQL database

**API**:

* Flask RESTful API that returns a json object containing only the data that was stored in the db. This data will be used by said ""tracker"".

**Tracker**:

* Plots the data/coordinates onto a world map and updates in real time as the position changes. Shows positions in the past and predicted path as well.

I really appreciate any pointers/suggestions you may have. Please let me know if you'd like me to elaborate. Thanks!",1583079360,13,0,1,dataengineering
"Airflow read line on file, wait 1 min, read next line",[removed],1583077723,1,0,1,dataengineering
Anyone work in Data Engineering at a consulting company,[removed],1583046582,0,0,1,dataengineering
Does anyone here start in supply chain and end in data engineering or vice versa and ever try and apply abstract thinking from their past experiences( let’s say in supply chain ) to data engineering? I feel like there are a lot of parallels and surprised I have not seen this conversed.,[removed],1583006711,0,0,1,dataengineering
Opinions on a Kafka infra tutorial,"Hey all! I have been data engineer for over ten years now, still trying to learn something new everyday. I have recently started to log my personal experiments as I go along, here's some sample: [https://blog.sogdian.co.uk/posts/kafka-cluster-using-aws-cdk.html](https://blog.sogdian.co.uk/posts/kafka-cluster-using-aws-cdk.html). I know it reads quite dry and I don't have the expertise to do video, but any advice/help/questions would definitely be appreciated!",1583003278,5,0,1,dataengineering
Distributed ML Pipeline,[removed],1582970736,0,0,1,dataengineering
Dumb question: when do you know to use rank and/or partition by?,,1582924079,6,0,1,dataengineering
"Data Engineering, Big Data and Machine Learning on Google Cloud Platform Specialization",[removed],1582919145,0,0,1,dataengineering
Advice for a daily fantasy baseball data pipeline on AWS,[removed],1582906021,0,0,1,dataengineering
Advice,[removed],1582899613,0,0,1,dataengineering
"How to progress in DE career, want to switch jobs","I have been in software engineering for the past 6 years, it has been a mix of different domains/technologies. 

For the first 4 years, I had been into full-stack development with a little bit of PHP and later on large part into React, Nodejs stack.

Then I moved into Data Engineering in the same company, extensively worked on setting up Kafka pipeline, worked on writing ETL jobs on Apache Spark using Scala. The entire team was new into DE, so learned about things in DE majorly from the internet, trial and error, nobody had any prior experience, a lot of times burnt our hands while trying out different things. The data pipeline is set up and quite stable, later on, have been writing a lot of ETL jobs in Spark and Scala and using Oozie for scheduling. I am looking for a job where I can learn from people who are experienced and know DE. But the technologies that I am working on seem to be far less in demand. Everyone is asking for Python/Airflow. I have worked on Spark majorly using Scala API. I am not proficient in general Scala, like Actors/Cats/Akka and other libraries. I can't say I actually know Scala better since scala is a lot bigger than just Spark. I feel stuck in the current job right now. I have applied to many companies but it seems they don't care about the breadth of work that I have done. All I get is rejections. How do I proceed in DE career?",1582874226,3,0,1,dataengineering
How AppsFlyer uses Apache Airflow to run more than 3.5k daily jobs,,1582864784,1,0,1,dataengineering
"I know there a lot of click baits and websites that list / sell trade shows to data leaders, but anyone here know of any good trade shows for a junior data engineer ? Besides AWS reinvent",,1582862469,16,0,1,dataengineering
Data Engineering Interview Process,[removed],1582848189,0,0,1,dataengineering
Why You Need More than a Schema Registry to Become an Event-Driven Enterprise,,1582746861,0,0,1,dataengineering
How to use array elements in Pyspark sql statement?,"When working with Pyspark, I need to select fields from an array that gets updated periodically

&amp;#x200B;

    arr = ['age', 'occupation', 'id']

how do I use the array in the sql query statement?

    df = spark.sql(""""""select &lt;fields from arr&gt; from tablename"""""")",1582735706,6,0,1,dataengineering
Any opinions on what companies are preferred to build a career as a DE?,,1582697842,5,0,1,dataengineering
Thoughts on this school project?,"Hi! Very new and interested in the Data Engineering &amp; Data Management space, and wanted to ask you guys for your thoughts on a project for one of my classes. Here's some context (the actual question is at the bottom of the post).

In an analytics program, and this is from a class about Data Management. For our final project, our professor set up a partnership with a small startup. They're soon going to share a snapshot of their MySQL RDS containing data that we're going to perform analysis &amp; create a sales &amp; fulfillment dashboard for them on later on.

Overview: we need to do some dimensional modeling, pipeline their data into a Data Warehouse (we've been working with Fivetran and Snowflake in class), then create our dashboards using Tableau after connecting them to the DW. The dashboards we give them won't be ""deployed"" per se, they will be used as a reference for them to build their own charts/dashboard in their customer-facing app using React (I think). 

They currently only use Amazon RDS and are willing to invest in a DW and pipelining/streaming solution that we're going to pitch. Here's where it gets tricky: we need to come up with a solution to that fits within these asks...

* They want a 5-10 min latency from the transaction to when it can be available in their dashboards
* The solution should be scalable (they currently have 30 customers + 300 daily users and are expecting 5-10x more in the next few years)
* They want to allocate $100/month tops

I don't have any experience, but I've done a bit of research (no cost analysis yet), and here's what I've come up with so far...

* For the pipelining solution, I'm thinking of researching Amazon's Kinesis or Apache Kafka
* For the DW solution, I know Snowflake charges by usage, so it might be too expensive for the budget, so maybe Redshift might be a good choice with the added benefit of keeping their systems within the AWS ecosystem.

Thoughts/suggestions?",1582690167,4,0,1,dataengineering
Want to learn Data Engineering? Here are some Example Projects to get your hands dirty.,,1582679985,5,0,1,dataengineering
Would this project come under Data Engineering?,[removed],1582664986,0,0,1,dataengineering
Has anyone here completed any certifications that gave you a significant salary boost?,"What certifications did you complete and where are you currently located, and how many YOE do you have?",1582661056,1,0,1,dataengineering
"Data engineering: what skills/knowledge are expected from entry-level, fresh out of undergraduate??","Undergraduate in NYC graduating in May and interested in data engineering.     

Would it be a better idea to get a data analyst or software engineering job for a year before jumping into data engineering?",1582647312,13,0,1,dataengineering
Microsoft To Build First Cloud Data Center Region In Mexico,,1582640578,0,0,1,dataengineering
A conversation about the conflicts that lead to shadow IT in data and analytics projects and how to work toward resolving those tensions.,,1582635820,0,0,1,dataengineering
Data model for mobile game analytics,"Hello guys,

I am trying to redesign a huge table we use for analytics of our mobile game. It is a snapshot table where each day we copy all users (with their dimensions and their measures for that day), partitioned by date. It powers our BI tools and allows ad hoc queries with minimal joins.

Problem is, we have almost 200 millions users (much of those are inactive) and copying all those each day becomes expensive and slow. One idea we had was to keep only users who are active in the last 90 days or so but that makes some queries (1 year retention for example) difficult to calculate.

Did you ever had a similar problem and how did you solve it? We also have some redundant tables aggregated by some dimensions to allow fast aggregated queries but we also need the table at the user level for ad hoc queries.",1582619450,10,0,1,dataengineering
Data model for mobile game analytics,[removed],1582618940,0,0,1,dataengineering
Pyspark - how do I use groupby with lists?,"I'm running a groupBy operation on a dataframe in Pysaprk and I need to groupby a list which may be by one or two features.. How can I execute this?

My first thought was to create a list of lists and pass it to the groupby operaiton, but I get an error:

&amp;#x200B;

&gt;TypeError: Invalid argument, not a string or column: \['record\_edu\_desc'\] of type &lt;class 'list'&gt;. For column literals, use 'lit', 'array', 'struct' or 'create\_map' function.

How do I make this work?   I'm open to other ways I could do this.

&amp;#x200B;

     record_fields = [['record_edu_desc'], ['record_construction_desc'],['record_cost_grp'],['record_bsmnt_typ_grp_desc'], ['record_shape_desc'],
    ['record_sqft_dec_grp', 'record_renter_grp_c_flag'],['record_home_age'],
    ['record_home_age_grp','record_home_age_missing']]
    
    
    for field in record_fields:	
    	df_group = df.groupBy('year', 'area', 'state', 'code', field).sum('net_contributions')",1582617965,2,0,1,dataengineering
Is moving from a DE role where writing Spark pipeline jobs is the main focus to a new company where DE works on Snowflake pipeline jobs the main focus a downgrade?,"I'm interviewing for a DE role where their architecture and skill requirements is quite a bit different than my current DE role. Can someone help me determine if this is a downgrade or lateral movement?  


**This is what I'm currently working on at my current company:**

* Python/Pandas
* PySpark 
* sometimes Scala Spark
* Apache Pig/ Hive
* BigQuery (Data Warehouse)
* HDP distribution of Hadoop
* Very little data modeling or data cleansing
* Vertica (Business Intelligence DW)

  
**Potential new role is more focused on the following:**

* Python/Pandas
* Airflow (orchestration/scheduling)
* AWS Snowflake (Data Warehouse)
* A lot of Data Modeling
* A lot of Data Cleansing
* Vertica (Business Intelligence DW)

From my understanding, a lot of the big data processing and transformations happens within the Snowflake data warehouse. Is this considered a more watered down DE role?

I want my next role to be **at least a lateral move** that will sharpen my Big Data skills over the next 3-5 years and be more marketable. 

The reason I'm leaving is because my company sucks. There is no growth available in the next 1-2 years at least. And I'm mostly self-teaching right now. There's no leadership. Due to a number of layoffs - we're fighting fires instead of being proactive and building out new features.",1582603609,11,0,1,dataengineering
Groupby list,"I have a data operation and I need to groupby a list which may be by one or two features.. How can I execute this?

My first thought was to create a list of lists and pass it to the groupby operaiton, but I get an error:

&amp;#x200B;

&gt;TypeError: Invalid argument, not a string or column: \['record\_educ\_lvl\_desc'\] of type &lt;class 'list'&gt;. For column literals, use 'lit', 'array', 'struct' or 'create\_map' function.

How do I make this work?

&amp;#x200B;

     record_fields = [['record_educ_lvl_desc'], ['record_home_construction_desc'],['record_rf_cost_grp'],['record_bsmnt_typ_grp_desc'], ['record_shape_desc'],
    ['record_sqft_dec_grp', 'record_sqft_dec_grp_10_flag'],['record_home_age'],
    ['record_home_age_grp','record_home_age_missing']]
    
    
    for field in record_fields:	
    	df_group = df.groupBy('year', 'area', 'state', 'code','field).sum('net_contributions')",1582587760,0,0,1,dataengineering
"What skills/knowledge are expected from entry-level, fresh out of undergraduate data engineers??",[removed],1582582969,0,0,1,dataengineering
Azure Data Engineer Cert,[removed],1582579966,0,0,1,dataengineering
How much does an entry level (&lt; 1yr exp) DE make and where are you located?,How much does an entry level (&lt; 1yr exp) DE make and where are you located?,1582572036,22,0,1,dataengineering
"Developing Airflow Plus (modern typed, open source, testable Airflow wrapper with some magic)",,1582563747,0,0,1,dataengineering
Cleaning large datasets on your local (memory-constrained) machine?,"Hi folks,

I'm wondering how to approach the problem of cleaning/transforming a dataset on my local machine, when the dataset is too large to fit into memory.

My first thought is to stream it line by line using a Python generator and perform my cleaning steps that way. Is there any existing library or framework that is built around this concept? Or is there a better way to approach this problem?

Thanks.",1582562221,24,0,1,dataengineering
Don't let Apache Kafka steal your weekends!,,1582560021,0,0,1,dataengineering
Looking for someone with Trading Platform experience!,"Hey everyone - An upcoming podcast guest asked me to reach out to some networks about a Data Engineer position they have available and are struggling to find some people for, I thought this might be a good place to start. 

They are an online trading platform going through some high growth right now, the team itself creates quantitative products &amp; analyzes platform activity. 

They are looking for someone who has experience with Python and SQL, distributed computing like Kubernetes or Mapreduce and experience with additional technologies: Docker, PostgreSQL, Kafka, Airflow. 

If anyone knows someone that might be interested feel free to message me or reach out to me at philip@alldus.com",1582558996,1,0,1,dataengineering
Need help on our server setup!!!!,"Hi! I  badly need some advice.

Currently, we experienced delays (1 day turnaround time) in processing large   
amounts of data (200GB-300GB)   
Our company bought a server with ff. specs:  
\- 2x Intel Xeon Gold 5218  
\- 512GB RAM  
\- 8TB SSD 

And here's our current process:

1. We use Elasticsearch to store raw data  
2. Retrieve 200GB-300GB of data (from ES) to preprocess it using Python  
3. Save data to DB (PostgreSQL)  
4. Visualize data using Tableau  
(Note: we are using commodity laptops in our current setup)

Can you give me an advice on how can we maximize the specs of our server?  
What are the best setup with our scenario?",1582550529,18,0,1,dataengineering
Data Scientists vs Data Engineers: Which one is for you?,,1582520641,2,0,1,dataengineering
What are the qualities of a world class data engineer?,"There is obviously no objective measure for this, as people's perception of what is ""world class"" will differ.

However, by your own subjective estimation, what are the qualities that you believe a junior (or senior) data engineer should strive for to become a top notch data engineer?",1582517204,0,0,1,dataengineering
Thoughts on Data Catalogues like Alation?,"Has anyone ever used them before? How was it? Is it worth the money?

[https://www.alation.com/](https://www.alation.com/)",1582511278,8,0,1,dataengineering
How do you deal with DE FOMO?,[removed],1582480574,0,0,1,dataengineering
GitHub - ploomber/ploomber: A workflow management tool to accelerate DS/ML pipeline development,,1582480455,6,0,1,dataengineering
Need Feedback: Developing a design for a search tool that holds millions of records,"I have a large set of keywords, basically one word or multi-word phrases and millions of them. I want to create a efficient tool to search across them. Here are two approaches I can think of:

1. Store these in SQL DB, shard horizontally, add Full-text Index to the column
2. Store data as text files, basically a million documents, partitioned by hash function and stored in different folders and use the concept of inverted-index, built individually for each of these folders

Which approach seems more logical in terms of implementation and performance?",1582459065,19,0,1,dataengineering
Data engineer Vs senior data engineer responsibilities?,"What would you say is the difference between a data engineer and a sr data engineer? I've been in an engineering role for almost a year and talking to my employer about my next career step... He doesn't believe in promoting just for the sake of promoting, but promotions must come with increased or different responsibilities (makes perfect sense). I'm not interested in managing people and we agreed that's fine, but then I was asked to research what the main difference is for an engineer with 1-2 years experience Vs an engineer with more experience. What has changed for you personally the longer you've been in a role, if anything? All the job descriptions I look at seem to be more or less the same, regardless if it is called a senior engineer or not. TIA",1582417524,9,0,1,dataengineering
Sample coding / tutorial for DE,"Yesterday, I asked the following question: 
https://www.reddit.com/r/dataengineering/comments/f7lvdc/what_exactly_does_a_de_do/ Unfortunately, I think people misinterpreted what I was asking. 

If I wanted to see an example of web development. There are tons of resources on YouTube and elsewhere where I can see what coding in web dev loops like. Here's one for Javascript: https://www.youtube.com/watch?v=hdI2bqOjy3c&amp;t=18m21s 

**I can't find the same thing for DE.** Does anyone have an example of what coding looks like for DE?",1582411590,3,0,1,dataengineering
Streaming data changes to a Data Lake with Debezium and Delta Lake pipeline,"A client has approached me with a use case of capturing data changes in multiple instances of a Microservices Application (they run an instance per customer), to update a Data Lake for analytics.  
We took on the challenge to assemble a changed data capture pipeline with Debezium and Delta Lake combo  
High Level Strategy Overview:

* Debezium reads database logs, produces json messages that describe the changes and streams them to Kafka
* Kafka streams the messages and stores them in a S3 folder
* Using Spark with Delta Lake we transform the messages to INSERT, UPDATE and DELETE operations, and run them on the target data lake table. This is the table that holds the latest state of all source databases
* Next we can perform further aggregations on the latest table for analytics

Below is the summary post of the process along with an example project  
Would love any feedback  
1. [https://medium.com/@yinondn/streaming-data-changes-to-a-data-lake-with-debezium-and-delta-lake-pipeline-299821053dc3](https://medium.com/@yinondn/streaming-data-changes-to-a-data-lake-with-debezium-and-delta-lake-pipeline-299821053dc3)  
2. [https://github.com/tikal-fuseday/delta-architecture](https://github.com/tikal-fuseday/delta-architecture)",1582411133,5,0,1,dataengineering
Airflow set up with k8s executor,"I am using a PVC to mount dags. The configs don't have examples and I am running to a very strange issue.

    ## name of dags pvc
    dags_volume_claim = dags_pvc

    ## I don't know what below is supposed to be
    ## if PVC is mounted at /opt/app and airflow is in /opt/app/airflow 
    ## and dags should be /opt/app/airflow/dags
    ## then does this sound right?
    dags_volume_subpath = airflow/dags

Please correct me if the above is wrong.

Now the 2nd issue is that on this PVC I have all my dags, parsers and other scripts.
There is a python script on this PVC under airflow directory that is supposed to access a file inside hidden directory on root of PVC.

So for us root of PVC is /opt/app
The file is in /opt/app/.hidden/file1

When my webserver pod is coming up, it complains it cannot access 
    /opt/app/.hidden/file1


Any ideas?

Thanks!",1582393777,0,0,1,dataengineering
Unit testing pipelines?,"Has anyone does this/are doing this successfully? Are there any tools you are using? How were you able to get other teams to buy into better swe practices for data? It's ongoing problem we are facing and the only hurdle to implementation imo is the work to build it, maybe some cultural changes, and it still doesn't solve the problem of understanding if output changes are good or bad. Thoughts?",1582393030,13,0,1,dataengineering
Currently a Data Engineer need help interviewing for other positions,"So I started off as a SQL Developer, mostly working with tools such as SSIS and Informatica and using mostly SQL to do ETL. I would say my SQL skills are very good, probably at least a 9/10. Recently my company has started to use Spark and I've learned Scala but someone who didn't major in computer science, what would you recommend for interview prep? All the interviews I've landed so far ask for algorithms during the coding tests.",1582351564,7,0,1,dataengineering
Local setup or cloud is better ?,"Hey everyone,

I am a beginner to the field of data engineering. I am now comfortable with several tools such as HDFS, MapReduce, Hive and Pig. However, I have been just using AWS EMR for my practice. Are there any benefits to install the Hadoop ecosystem on my mac and configure them locally or is AWS the best way to go about ?",1582341141,12,0,1,dataengineering
Ruby pipeline tools?,"Looking at a company that's using map-reduce/java (cloudera) and Ruby for their current processing with a future shift to spark. 

Are there any Ruby based pipeline and orchestration tools? Everything I'm aware of like airflow, Luigi, etc. are python frameworks.

I suspect given the legacy Ruby code base a switch to python is a tough sell. And given the pain of bash scripts I'd rather avoid that if at all possible.

Thanks!",1582339775,2,0,1,dataengineering
What exactly does a DE do?,"I've watched many YouTube videos and while I understand the concept, I have yet to see a tutorial of the actual work that is performed. Can someone share what coding in DE looks like? Perhaps a video demonstration?",1582337275,28,0,1,dataengineering
List of non-English subreddits - Used to filter out non-English Reddit content in data projects,[removed],1582328322,0,0,1,dataengineering
Processing 1 billion records locally from Hive metastore(parquet format) takes forever 6 hours. How to speed it up?,[removed],1582324551,0,0,1,dataengineering
"I interviewed the technical founder and CEO of Vector Space AI, Kasian Franks on AI/ML, data, blockchain technology, crypto, startups, and more (podcast)","I interviewed the technical co founder and CEO of Vector Space AI, Kasian franks on developing On-Demand Correlation Matrix Datasets for Hidden Relationship Detection in Data &amp; Training in Artificial Intelligence (AI) Systems, blockchain technology, NLP/NLU, and more!

I believe the community may find the conversation to be interesting as we discuss various topics that tie into artificial intelligence, blockchain, and machine learning. Kasian Franks is a serial tech entrepreneur with decades of experience in the bio and life science industries as a computer engineer.

I hope everyone enjoys the conversation!

[Libsyn](http://directory.libsyn.com/episode/index/id/10467551) // [iTunes](https://podcasts.apple.com/us/podcast/1-kasian-franks/id1465687933?i=1000444105556) // [Google Play](https://play.google.com/music/m/Dlgsqozn2ax2uyg2zrjr7fibxqy?t=1_Kasian_Franks-All_Things_Interesting) // [Spotify](https://open.spotify.com/episode/25LFf48ku301iyibw53RBh?si=AiLU0mpqTVuK4MTU9kIOiA) // [Overcast](https://overcast.fm/+Sm_1Kd7Uk)

The episode is also available on most platforms as well.",1582318444,2,0,1,dataengineering
Is Google Cloud Data Engineering Certificate worth a try to transition into DataEngineering?,"Hello All,

Am a Master students(Software Engineering) in US with 5yrs experience in ETL development(informatica). I would like to get into Data Engineering field. I don’t have experience working in Big Data. Have done academic projects in AWS and GCP. My questions here is, does Google Cloud Data Engineer help me to transition to Data Engineering role? I am in my final semester and confused whether to prepare myself to be a Software Engineer(Data Structure and Algo) or Data Engineering(GCP Data Engineering certification)

Looking for your valuable views.",1582315759,5,0,1,dataengineering
Roles in a data engineering team?,[removed],1582315703,1,0,1,dataengineering
AWS Glue Crawler - Read single column file,"Hi there!

I would like to know if anyone had succeed in create use AWS Glue Crawler with a file with ""single column"". This file has to be split by position and has no delimiter.

We tried to create a custom classifier with no luck.

Thank you!",1582314551,1,0,1,dataengineering
Is SMACK the most popular data engineering tech stack?,,1582297364,0,0,1,dataengineering
Dimensionalizing,[removed],1582256575,0,0,1,dataengineering
What are my chances of getting a Junior Data Engineering job? Also are there lot of remote Data Engineer Jobs out there?,"I’m currently working as a Data Analytics and Database Admin for a big company in California. My daily work usually consists of doing SQL (PostgreSQL) to create views, validating stuff and using Python once in a while to create scripts. I don’t do any database setups or anything, just sql work. We also have data engineers so I don’t do any ETL kind of work. I don’t really do any analytics either although my title has it, since my boss does most of the analytics and my coworker does the reporting on SAS. 

I would like to transition into something more technical and something I can work remote in the future as I would like to travel. I do have a degree in STEM, basically physics. I’ve learned that Spark is the big tool being used for ETL and although I don’t know it, I’m planning on learning it. Would my background be enough to transition to a Junior Data Engineering role or would I need to get Spark and AWS down before applying for jobs? Also are there lot of remote work available for data engineering so I can transition to a Remote Data Engineer role within few years? 

Any advices or recommendations would be appreciated.",1582255561,10,0,1,dataengineering
Tips for landing a 2 Month internship,"Hello fellow seniors , i'm currently a second year Data Engineering student (Bachelor's Degree) , it's mandatory to pass an internship of 2-3 months during the summer 2020 , i was wondering about the requirements or some tips on what to include in my resume so i can land an internship .
A good internship will be my launching point , i'm really looking to learn from it alot since all i have seen during school is merely theoretical , my current skill chart doesn't look so promising but there is much room for progress:

2 Scala projects : 
- A Library that generates JSON data files ( Page View events) and (Recommention click events).
- A program that transfroms a database from SQL server to Mongodb server (Both hosted on Virtualbox locally ).

Hadoop : Programmed a WordCount for a text file
Spark : used pyspark for a cleaning task 
I know how to setup hadooo on virtual-machine. 

I'm looking for advice on what to do as a project next, so i can land the internship and please if anyone here is on the look for trainees i'll be more than happy to pass any kind of assignments . I feel like the Data Engineering field is made for people who already have work experience that know how data is managed traditionally and conventionally , for this reason i highly value the chance that internships give !

Thank you in advance.",1582254439,2,0,1,dataengineering
Skills relevant to data engineering,"Hi guys,

I am a grad student and have taken several courses related to data engineering like Big Data that made me learn about the Hadoop and the spark ecosystem of tools. Though we didn't have huge data sets to deal with, I have learn what each tool in the ecosystem does. But, I am confused on what are tools and concepts are used in the industry?. And what kind of projects can I put up on my resume to get into Junior data engineering jobs? I am comfortable with python (Django) and SQL as well.",1582246857,4,0,1,dataengineering
Is anyone using AWS Step Functions for data engineering workflows?,"Hey all, Step Functions seems like a good workflow tool.  I was wondering if anyone's around here is using it for driving ETL or other data engineering workflows.  If you use it, how's your experience with it so far?  If you considered it but chose not to use it, why?",1582239963,9,0,1,dataengineering
ETL with NiFi?,[removed],1582237941,0,0,1,dataengineering
Change-request mechanic for structured data,,1582231830,0,0,1,dataengineering
Do you write test cases for each of your data pipelines?,[removed],1582231617,0,0,1,dataengineering
Tips and advice on getting entry level data engineering job?,"I’ve been following this sub for around a year now and I realize I would really like to enter the data engineering world now. From what I’ve seen on this sub, it seems like most data engineers are software engineers that have focused specifically on data flows and processes. Would anyone be willing to critique me and advise me how to make the leap?

A little about me:
* 2 years exp in tech implementation in IT consulting firm, but not a software engineer
* Masters in engineering (non SWE)
* Currently working on portfolio project for sports analytics. Created a script to scrape football data, clean, and visualize. Originally just a pure python project, but now trying to make a website out of it to be able to show anyone - so now learning how to use flask, heroku, Postgres, and other tools involved in deploying a small website
* Familiar with SQL, **very** comfortable with python now. However, all my programming skills are from googling and YouTubing everything for the last 5 years, so I do have a bit of imposter syndrome here, because I’ve never written production level code. No AWS exp but currently working on solutions architect cert

I would really love to get into this field but I find myself being paralyzed because I don’t even know where to start. Any and all constructive criticism would be appreciated!",1582216069,4,0,1,dataengineering
How to combine fact tables?,"I was tasked with creating some proofs of concepts for some BI solutions at my company. I want to start with creating OLAP cubes in a data warehouse, of sorts. A project I want to focus on will use a Produced Quantity and a Returned Quantity given a date range. Both of these, I thought, would be two different fact tables; however, I want to show them both in the same table along with a Return Rate (Returned / Produced Quantities). Does anyone have any suggestions of how to go about doing this? Do I need to create one fact table that shows a Produced Quantity and Returned Quantity along with the Returned Rate, or do I need to create two separate fact tables and somehow relate them to one another and do the calculation in a BI tool (If so, how?)?",1582209046,9,0,1,dataengineering
Looking for fast and efficient start with Spark," Hi everyone,

I'm currently in transition for a more technical role at my company and one thing that seems very useful and close to my expertise seems to learn more about Apache Spark.  
I'm got a master's in mathematics, phd in social sciences and am self-taught quite proficient with all the python pandas-related stuff as well as some R, Matlab and KNIME. My company employs Spark with scala so I'm going to start more from the data-side and not the implementation (we have scala developers, so no need to try to pile onto somthing that is farther away from my expertise anyways).  
Given that background, I'm not too keen on starting any random Spark101 course as I fear that it will waste my time with a lot of stuff that I A) might have already understood or B) am not afraid of anyways because mathematical complexity is a non-issue for me.  
Do you have any (paid or free) recommendations where to kickstart hard into understanding the Spark way of doing things?

  
Thanks in advance!!!",1582148737,4,0,1,dataengineering
Is it worth using PySpark for writing 50 rows/min?,"Hey guys, I need to write an ETL for taking data from Azure (through their REST API) and putting it into a separate Postgres db. The data we would write would be around 50 rows per minute. Wondering whether I should be learning/using PySpark for this for speed, or should I just use the general python SQL API. Thanks for any help!",1582146512,8,0,1,dataengineering
How to find duplicate based upon multiple columns in a rolling window in pandas?,"Also, posted it on SO

[https://stackoverflow.com/questions/60285964/how-to-find-duplicate-based-upon-multiple-columns-in-a-rolling-window-in-pandas](https://stackoverflow.com/questions/60285964/how-to-find-duplicate-based-upon-multiple-columns-in-a-rolling-window-in-pandas)

As the data is streaming. I want to check if a duplicate record(based upon some columns) arrives withing two minutes so I discard it as and do no processing on it. print it as a duplicate.

I have tried a variety of things to no avail.

Any help would be appreciated.  
Thanks",1582141421,6,0,1,dataengineering
ETL monitoring dashboard,"Does anyone here use a dashboard to monitor ETL jobs? Either a vendor or homegrown solution?

I joined a new company in October and it's amazing to me that we do not have a solution ready to answer simple questions about our own data processes and pipelines.

Questions like:

1. How many jobs ran successfully yesterday?
2. How many rows were written?
3. How is our data growing? (size of tables)
4. What is the average load time?
5. What jobs failed and what was the reason?

We had an outage last week and my director discovered that some tables hadn't been refreshed in a few weeks and he couldn't answer questions to leadership about what was going on. We have notifications set up through Slack but not a solution that brings together the entire process.

Has anyone else gone through this exercise? Any advice or thoughts on getting this organized and off the ground. I'm going to volunteer to take this project on.",1582072331,34,0,1,dataengineering
Anybody here switch from data scientist to data engineer? I'm considering making the switch and would love to hear from others who have traveled this path!,[removed],1582069578,0,0,1,dataengineering
In memory geospatial querying...,[removed],1582062729,0,0,1,dataengineering
Data Vault integrated with a Data Lake - thoughts,"I have been exploring this new data modeling Data Vault 2 and how this can easily get along with Data Lakes. Ideally the Data Vault design might require a raw stage to maintain the natural state of the data before any ETL/ELT and keep the 100% reliability on data pulled from any source.

&amp;#x200B;

[Data Vault 2 architecture](https://preview.redd.it/tom4ottkwqh41.png?width=2054&amp;format=png&amp;auto=webp&amp;s=f115e961de82d8e9f61445135f437d5629ae9de2)

But writing this down over the Data Lake concept my first layer it's also a raw data before bring the data into a curated area to be consumed by the Data Vault/Tranditional DWH model. I feel here we could have a redundancy on design and likely the right approach to refactor this is maintain a single raw layer on the data lake and push my DV to start from the raw vault and consume the Data Lake raw stage to built Hubs , Links and Satellites from Raw Vault.

&amp;#x200B;

[Data Lake - Data Vault with only one raw stage](https://preview.redd.it/9xlj6bu3yqh41.png?width=1932&amp;format=png&amp;auto=webp&amp;s=1ef6dfc4031e411bb3c6f814ffbb14a02b1bbe5f)

Would my assumption make sense?",1582059737,6,0,1,dataengineering
An interview with Snowplow Analytics tech lead about how they manage data infrastructure for streaming events across multiple clouds,,1582054543,0,0,1,dataengineering
Checking Table Updates,Is there a best practice for finding out when tables have been updated? Is it specific to each type of database? Do you use metadata? Do you query the table for time stamps? Is this even a data engineers job or what title would this most closely align with?,1582043801,5,0,1,dataengineering
Can someone recommend a great resource for learning Data Governance primer and best practice?,"I am currently working for a tax department in a large Multinational and their data governance and controls are terrible. I can automate a few simple processes for them, but without a strong governance framework it will be like putting a band aid over a large wound.

Can anyone recommend some online resources or books on data governance?

Thanks",1582043738,5,0,1,dataengineering
Distributed Data for Microservices — Event Sourcing vs. Change Data Capture · Debezium,,1582043218,0,0,1,dataengineering
Data Lake buckets: store latest version of records only in bucket or include timestamp in name to keep history?,"I'm working for an insurance company who have policy and quote information in a Mongo DB. Both quotes and policies can be modified over time and no history of changes is kept in the datasets I'm using. When a record in Mongo is created or changed I copy it to a bucket as a json then insert it into an append only landing table in BigQuery, our data warehouse.

&amp;#x200B;

The thing is that because I use the record ID as a filename when uploading to the bucket, if a record is modified it just overwrites the file already in the bucket. This means that the landing table is building up a history of changes, but the bucket only has the latest version of the files.

&amp;#x200B;

The history of changes is useful for reporting but if I want to change the schema of the table and keep the history I have to create a query that'll transform the table from the old schema to the new. This is time consuming for me as there's a lot of nested data and the schema changes are relatively frequent. It'd be a lot easier for me to just delete the table and reprocess all the files in the bucket into it.

&amp;#x200B;

So I was considering uploading the Mongo jsons to the bucket with a name of \`&lt;ID&gt;\_&lt;timestamp&gt;.json\`.   


I'm wondering is it normal to do this? It feels odd to keep a history of changes outside the source database so I'm unsure if this is a bad pattern. Storing a history in the data warehouse, but not the data lake feels odder still though.  


If anyone had advice or knows useful resources that'd be much appreciated. I'm struggling to find anything in my googling.",1582028135,5,0,1,dataengineering
Best DE Books,"Hello,

I am a junior DE (\~1.5 years exp), and I am wondering what are the best books about Data Engineering.

&amp;#x200B;

I am currently reading **Designing Data-Intensive Applications** which I discovered from another post on this subreddit.

&amp;#x200B;

Do you recommend another one?

&amp;#x200B;

Thank you very much :)",1581945378,14,0,1,dataengineering
Can we set default schemas for AWS Redshift roles to virtually achieve different databases under ONE AWS Redshift instance?,,1581945118,0,0,1,dataengineering
What does your data eng team spend their time on?,"In our case, I find we spend a lot more time dealing with bugs and feature requests from data scientists than what I imagine the average team does. Internally we're split on this- most of us want to spend more time on tooling and cost optimization, reduce reactivity etc, but I don't see a clear way forward to do that. 

Is this normal? What are other people doing to move away from reactivity and more towards tooling?",1581919342,6,0,1,dataengineering
Spark slow read from MS SQL Server,[removed],1581877001,0,0,1,dataengineering
Which framework should I use? streaming app reading(JSON) data from stdIn,"I'm having a bit of difficulty in choosing a framework on how to go with a problem at hand.

The application should be streaming. And the data(JSONS) provided to it would be from stdin

&gt;e.g authorize &lt; operations

Furthermore, there's some functionality that requires some window operation. e.g. do something with the incoming data over 5 minutes span and don't have to use a database and rather an in-memory data structure.

I'm thinking of doing it in Scala but I can do it in either Scala/Python.

* The first thing that comes to my mind is Spark/Pandas because of Streaming, Window functionality, and data frames as the in-memory data structure.
* Also thought about scalas fs2 for streams and circle for JSON

**Any suggestions on how to go about it? What framework can be used or not at all?**

And if Spark, How to read data from StdIn?

And is it still in memory when in SparkSQL we do createorReplaceTable("""")?

Thanks",1581869563,6,0,1,dataengineering
"Differences between datawarehouse, datamart and OLAP cube",[removed],1581851347,1,0,1,dataengineering
"what differences and similarities are between ""dataflow engines"" and ""stream processing systems""?",,1581807022,1,0,1,dataengineering
How many of you struggle with drafting a DE-focused resume?,"This is primarily for those that want to jump into data engineering, but obviously feel free to chime in if you're not in that category.

I'm still pulling together my first tutorial that I believe will help serve as a starting point for those who want to jump into data engineering, so hopefully that will be released pretty soon. I recently wondered if those who want to jump into data engineering have struggles with creating a DE-oriented resume as well, especially if you're trying to transition from a non-tech field (which was my situation). I could easily pull together a comprehensive guide on things to highlight on the resume that ""translate"" into DE-related skills, and I could perhaps offer reviews/feedback of resumes as well (obviously the latter part wouldn't be free).

Thoughts? Would tips for enhancing LinkedIn profiles be helpful as well? Any other areas relating to professional development that people seem to struggle with as they try to transition into another field?",1581781759,1,0,1,dataengineering
How are jobs chained together in MapReduce?,,1581776872,0,0,1,dataengineering
Recommendations for good professional training courses?,"My company hired me as a data engineer, with a tiny bit of actual DS/ML mixed in. 
Everything I know about this field is self-taught - my real background is in mechanical engineering, but I have been working as a coder for about 2 or 3 years now. I was supposed to be working under the senior data engineer, but he recently resigned. It looks like the company is hoping that I'll step into his shoes.

My company has asked that I look into training courses, since exec has allocated a generous budget for professional development.

Does anyone have any recommendations?

For added info, I work in med-tech, so data security and governance are important. For the last few months, I've been working on developing a template for producing serverless web tools within the AWS ecosystem, for consumption within our company. These are largely to support other teams, such as Manufacturing and or Quality. In the near future, we will be building customer-facing applications, obviously dealing with patient data. So best practices around scalability, security and prevention of data leakage are important to us.",1581758281,14,0,1,dataengineering
Building Big Data Pipelines with PySpark + MongoDB + Bokeh,[removed],1581744727,0,0,1,dataengineering
Airflow for managing transient clusters on EMR,[removed],1581731584,0,0,1,dataengineering
What do you use for data analysis?,"What do you use for data analysis thats sitting on your data lake/Database/Raw files etc.., is there an efficient way to combine all streams of data into one analytical portal?",1581716252,12,0,1,dataengineering
Comprehensive Guide To Approximate Nearest Neighbors Algorithms,,1581695674,0,0,1,dataengineering
How would a business incorporate Kafka + pandas with Oracle databases?,[removed],1581691623,0,0,1,dataengineering
How can I get hands on Spark?,"Hello. I would like to learn Spark using it for some real case in my company. After searching on Google I have not very clear how could I implement it. 

The typical examples are SQL, Machine Learning, Streaming... But for ML we are good with Sagemaker+Lambda+API Gateway, the ETL is running okay with Redshift and for the streaming we have some Kinesis streams and I don't think we can use Spark Streaming for adding more, because that depends (I guess) on the backend team to implement it and it's not going to happen.

Is there any other possibility than moving some parts of the ETL to Spark? 

Thank you",1581672544,4,0,1,dataengineering
How Bosch Does Real-time Analytics on IoT Event Streams,,1581663952,1,0,1,dataengineering
How can I break into data engineering,Can anyone give me tips for breaking into data engineering. Currently working as an IT analyst I have experience with python and sql.,1581652749,11,0,1,dataengineering
Wondering what the best online free resource is to learn Data Engineering skills?,"Hey everyone! I currently work as a technology consultant (basically it’s a buzz word that means we work on consulting projects that require somewhat technical knowledge). Over the past 6 months, I’ve been working as a “Web Data Analyst” for a company that handles millions of site views per day. My role consists of a few things, but a significant portion of it is handling and maintaining ETL pipelines to report on large subsets of data. This includes creating workflows in SQL using our internal data warehouse software.  

I’ve been enjoying my work recently and from my understanding, what I do currently is similar to what a data engineer does. I understand that there are a few skills that I am lacking – for example, I do not know Hadoop. However, I do know some scripting languages such as Python. 

I was wondering which is the best online resource I can use in order to up my skillset to become a full time Data Engineer. I would preferably like my training to be free and reputable to a future employer so I can get a good job within the industry. Are they any good options on Coursera or any similar platform?

It is worth noting that I may be able to get my work to pay for a short training course. For example, if there is a 5 week Hadoop bootcamp I’m sure it’s possible for them to fund it. 

Thanks in advance!",1581637649,3,0,1,dataengineering
"Apache Airflow: Variables, Macros and Templating",,1581634157,0,0,1,dataengineering
Pigeon Hole “DE” Role. Need help on what to do!,"Hello, I was hired six months ago for a data engineering position. I made sure it wasn’t a BI role that I was previously at. The manager I interviewed with talked a lot about AWS, airflow, python, Azure cloud, redshift that they’re working with. Boy, was I stoked! coming from an SSIS, Microsoft, SQL background, finally getting my foot into DE. 

Into about a couple of months, I realized they do everything they can to get out of coding, the only client that they’re working with used python for Airflow but I won’t be working on that project. They love drag and drop and I’m very disappointed because I’ve been put on projects using SSIS, SQL server, Azure data Factory and lots of SQL and no python. 

Fast forward to now, this is where I think might be the last straw for me and will try to find another job. They are putting me 100% on a very long project to query adhoc data and export it to flat files. It’s not creating data pipelines, it’s not DE, while all my team members get to work on this huge, high visibility project using redshift and python while I get put onto this crap. I get it that I’m the new kid but damn.

I’ve communicated this very well with my manager before they’ve decided to put me on this project that I wanted to pursue DE roles, not being a sql dev or a data extract person. He is well aware of my career aspirations. 

Now, I wanted to ask for opinions on anyone who have been through this situation. How do you get out? I’m working on side python projects and implement python scripting in anything I can  relating to work. What else can I do? Start polishing my resume again? Any advice would be greatly appreciated. I’m really depressed about where this all is headed. Thanks for reading/advice in advance.",1581633932,7,0,1,dataengineering
Best free online resource to learn Data Engineering skills?,[removed],1581632710,0,0,1,dataengineering
"Automating Tax Department's Calculations (Small, static data)",[removed],1581629266,0,0,1,dataengineering
Can I get a Jr. Data Engineering Job in 3 months?,"* Graduating in May w/ B.S. Information Technology
* SQL: Can perform basic queries
* Python:  Understand basics + numpy, pandas, matplotlib. I have not done any scripting yet.
* I have basic knowledge of relational databases and their designs as well as a couple management systems.

I'm asking because I am currently studying data analysis but that is not my goal career so if at all possible I would prefer to be an employable Jr. DE than a DA by this summer.",1581613853,14,0,1,dataengineering
"Is Spark widely used in ""Windows-stack"" companies?",,1581605253,1,0,1,dataengineering
An Awesome List of Open-Source Data Engineering Projects,,1581605097,6,0,1,dataengineering
"Choosing a batch orchestration tool. Looking into airflow vs glue, open to others.","At my company, we were using databricks to productionize our pipelines and ETL jobs. After running into some pain points, we realize that this is not going to be scalable or easy to work with in the way that we like. We are doing some research regarding orchestration tools and came up with two candidates for this process - airflow and glue. I was wondering if anyone had experience with both and could tell me the pros and cons of each workflow orchestration tool to better suit my needs (or just in general for arguments sake, perhaps).",1581603879,2,0,1,dataengineering
DBND - a Python library for building and tracking data pipelines.,,1581598601,0,0,1,dataengineering
Decompose your Monolithic ML Pipeline with a Feature Store,"This blog post talks about how data engineering can manage feature pipelines feeding a feature store - and how this is essentially DevOps. While data scientists can run ML pipelines, training models - these pipelines start from the feature store. Both sets of pipelines can run at different cadences. Also,  how you can version data with Apache Hudi in your feature store.

&amp;#x200B;

  [https://www.logicalclocks.com/blog/mlops-with-a-feature-store](https://www.logicalclocks.com/blog/mlops-with-a-feature-store)",1581591381,0,0,1,dataengineering
"Understanding ADAM optimization Algorithm, The Easy Way",,1581590250,0,0,1,dataengineering
What are the good data engineering conferences to join?,[removed],1581578452,0,0,1,dataengineering
Anyone build a data warehouse of their mobile application data from their iPhone ?,[removed],1581577289,0,0,1,dataengineering
When can I start data engineering on my iPhone. ex. Manage data warehouse and query from my iPhone,[removed],1581577241,0,0,1,dataengineering
All things GCP: Machine Learning Decision pyramid - Understand which Google Cloud tools matches best for you.,,1581571103,0,0,1,dataengineering
Big Data Architectures Survey,"I'm  going to write a survey about Big Data architectures, but I don't know  if that would be useful or how difficult it would be. I mean, is there a  science behind architectural choices?

In  traditional software development we have a lot of books, papers and  protocols on software architecture, but it isn't true on Big Data  architectures. I see a lot of articles and posts about the Lambda and  Kappa architectures, but how exactly do big companies choose their Big  Data architectures?

Where can I find references and articles about that? Where do you find that?

Thanks.",1581550681,0,0,1,dataengineering
Studying Data Analysis/Engineering Concurrently,[removed],1581537853,0,0,1,dataengineering
Hiring a Data Engineering contractor,"My company and DS team has a need for a long-term data engineering contractor. The company has existing relationships with some offshore vendors (about whose employees I have received mixed reviews) and I'm wondering if there are other reputable vendors with whom any of you have either a. worked with in the past or b. currently work with. 

I am not in HR and would certainly prefer to just do work and manage my team; right now, managing my team means hiring a contractor and I would *love* some guidance on who has great engineer contractors.",1581533834,23,0,1,dataengineering
Data engineer,What are the mandatory skills to become a data engineer,1581477318,7,0,1,dataengineering
Data lake on AWS,"Hello All,

I am interested in knowing how you guys built your datalake on AWS.
What technologies did you use to ingest various sources?
Our source files are csv, JSON and Parquet and we are ingesting to raw and curated.

Thanks,
Mc",1581466112,8,0,1,dataengineering
crash course in Eclipse/Scala/Spark workflow?,"Hi. I work on a data team develops Scala in Eclipse, exports JARs to an EMR cluster, and then runs Spark jobs. 

Anyone know of a good resource to quickly learn the work flow or development cycle? Because they are different components, it's hard to find a tutorial that neatly combines them all. 

I have gotten bits and pieces from teammates and Udemy courses, learned basic Scala and get the gist of Spark. I'm not a Java programmer so I'm not used to that development life cycle. 

Thanks for any advice.",1581453083,0,0,1,dataengineering
Data Engineering Twitter Accounts?,"For those of you that are active on Twitter, what Twitter accounts do you follow that provide updates or useful insights within the data engineering world? Or, do you have a Twitter account that you use to post insight/updates relating to data engineering? I recently got more involved on Twitter and I'd really like to start engaging with other data engineering folks on there.",1581446750,14,0,1,dataengineering
How to assign withColumn based on value of multiple columns,"I have a pyspark dataframe (cannot convert to pandas as it is huge) and I need to create a new column and assign a value of ""y"" or ""n"" based on whether any of 10 columns  xval1....xval10 contain the string '48'. How would I do this?",1581444497,4,0,1,dataengineering
Interesting questions and answers on big data modeling (with Cloudera supported tools bias),,1581436701,0,0,1,dataengineering
How much data engineering can be learnt at home?,"I am a noob at data engineering, and more of a scientist. I wish to be more hands-on and get into data engineering. My question is: how much is it possible to learn from home with a Macbook Air. Asking this because data engg. is all about dealing with big volumes of data which are found at an industry scale. Things like algorithms and python coding can be learnt from home as they are only about logic. Does the same apply to big data applications?",1581427410,8,0,1,dataengineering
(AWS) Moving data from postgresql to redshift in (near) real-time?,"I can write and schedule a batch job using a few different tools to do this, but batch jobs have the issue of data becoming stale almost immediately. Any suggestions on a broader strategy to approach this? Don't need the nitty-gritty, just a bit of a north star to start planning a strategy.

Thanks!",1581359660,18,0,1,dataengineering
Love this no frills publication for Python links &amp; news,[removed],1581358440,0,0,1,dataengineering
An interview about the data vault method of data modeling and how it simplifies integrating the evolving data sources that you are dealing with in your enterprise data warehouse,,1581352945,0,0,1,dataengineering
Cherre Series A Funding Round Announcement!,[removed],1581352589,0,0,1,dataengineering
Poor man's full-text search with django and postgres,,1581282701,0,0,1,dataengineering
Poor man full-text search with django and postgres,,1581282576,0,0,1,dataengineering
Scaling Flink Timers,[removed],1581278754,0,0,1,dataengineering
Upskill as Data Engineer,"Hi folks,

I've spent quality time in data platform teams from past 3 years (5+ years of exp in total). I've worked on ingestion and ETL pipelines involving streams and batches. I've designed and developed rule engine to work on near real time events. I'm more of a developer than an analyst. I'm proficient in Java. I'm considering to upskill myself in my free time. Could you guys suggest me the things that I need to focus on in short term and Long term to stay in the game of competitive career.",1581274292,6,0,1,dataengineering
Using ODBC to connect any database directly to Jupyter notebook.,,1581262468,0,0,1,dataengineering
Not getting a good grasp of CIF architecture,"i am currently trying to make a basic OLAP for a fictional beer company, but the problem is that i have to use the Inmon Corporate Information Factory and i have looked it up but i am not really getting a good grasp or i'm not sure of what it really is.

From what i have understand, the CIF architecture in a basic form is a regular 3NF database model with data that you can load to those tables and you create the cube with it for example with SQL Server.

Thanks",1581256250,2,0,1,dataengineering
Onsite interview,I have an on-site technical interview for a Data Engineer role coming up and was looking for insight on typical types of questions and what “gotcha” questions people have encountered.,1581112310,23,0,1,dataengineering
In Need Of A Senior Data Engineer In Barcelona or Madrid!,"I am currently growing a team of Data Analysts &amp; Data Scientists and looking for a Senior Data Engineer to lead this team in a really exciting new project.

Message me for more details if you think you could be perfect for this opportunity!",1581075846,4,0,1,dataengineering
How to handle sensitive data ?,"Im reading about how to handle sensitive data as a data engineer for a position as a DE for a cyber sec company , any additional info would be great.",1581025024,14,0,1,dataengineering
How do you balance the need for validation and standardisation with the need to support diversity?,[removed],1581016017,0,0,1,dataengineering
For hire: Talend developer (Remote / Relocation / Gigs),"I don't know if I can post it here but I am looking for a job. For lack of local demands for these skills, I am considering to switch entirely to web development but that would be my last ressort. Advices are welcomed too.

I have 2 years experience in Talend Open Studio for DI and ESB, have worked on the enterprise version for 6 months. 

* Proficient in Linux
* Know enough shell &amp; scripting to be autonomous: Bash, Python, Ruby
* Web development background
* Have been exposed to: Google Cloud Platform, Jenkins, Spring, Odoo, Magento",1581000392,5,0,1,dataengineering
Where to start: Data Analyst or Data Engineer? Ultimate Goal: Data Scientist,"My background is in economics / finance and I've taken the math needed for an engineering degree (to prep for grad school). I recently learned PostgreSQL via a MOOC. I want to eventually become a Data Scientist but I'm not sure where to start. I also want to have exposure to Data Engineering tasks because I want to know what DE's go through, in case I get a job at a smaller firm. My logic is that the person who is well rounded has a better chance of being employed in a downturn. Given this, does it make sense to learn Data Engineering first and then move onto Data Analysis and then Data Scientist? Is there a better path?",1580935611,20,0,1,dataengineering
Any experienced/senior data engineers have any tips or advice for an senior undergraduate interested in data engineering roles?,"
In NYC, experience with Java and Python. I have conceptual knowledge of Hadoop and Spark. What should I know/learn for entry-level data engineer roles?",1580934480,11,0,1,dataengineering
Data Engineering Wiki,"I don't see a wiki for this sub-reddit. Or am I just missing it? If not, I think this sub-reddit could use a wiki.",1580908479,18,0,1,dataengineering
How to convert a big .json file to .csv format?,I am a complete noob(student). It's a 12GB .json file. I tried doing it in python with pandas but my system crashes. Help is much appreciated.,1580840722,20,0,1,dataengineering
An interview about the BrightHive platform for building data trusts and the complexities that are inherent in sharing data across organizations,,1580829841,0,0,1,dataengineering
Architecting IoT Data Pipeline in Azure,"Hi all,

Does anyone have experience in designing and implementing data pipeline from IoT sensors in Azure platform? 

My knowledge is bit limited on this matter so need some guidance please.

What I am thinking is something like this. Please note that this is based on my current knowledge. Feel free to criticise, challenge, give alternatives, suggestions etc.

Source: IoT sensors
Ingestion layer: Azure IoT Hub
Processing layer: Azure Streaming Analytics
Then, the output from this layer is branched off to speed layer in form of real time streaming to power BI.
The other branch is to batch layer in form of Azure Data Lake Gen2, then onto Azure SQL Database for easy querying.

Some questions I have regarding design considerations are:
1. Ingestion layer
     I believe instead of IoT Hub, another tool which can be used here is Kafka via Azure HDInsight.
     What will be the pro &amp; cons, design considerations to choose Kafka vs IoT Hub (and vice versa)?

2. Processing layer
     I mentioned Azure Streaming Analytics (ASA) above, but I believe I can replace it with SparkStreaming via Azure Databricks, although I am not sure how to present real time streaming in Power BI using SparkStreaming ( ASA is supported as streaming source in Power BI).
What will be the pro &amp; cons, design considerations to choose ASA vs SparkStreaming (and vice versa)?

3. Historical data storage
     Above I choose Azure Data Lake Gen2 as I believe it's a better choice than Azure Blob storage as the data maybe used for analytics in the future. Not sure if this is  valid or good enough reason to choose ADLSGen2 vs Blob though?

Also, from ADLSGen2, I am thinking to ETL the data to Azure SQL Database using Azure Data Factory  as most people in the company is familiar with SQL database, therefore will be easy for them to run query against the historical IoT data. 
However, this makes me think - should I then remove the ADLSGen2 altogether, so that output from ASA go straight to Azure SQL Database?
Or, should I still keep ADLSGen2 so that when in the future, we need to run some (advanced) analytics using distributed platform eg. Spark/Databricks, we can get the data from ADLSGen2 as the source and this will be more efficient and fast as it supports parallel processing, while Azure SQL Database doesn't?

If keeping ADLSGen2 layer, what will be the best format to store the data in? Json, csv, parquet, or something else?
Is it better to keep the data as raw as possible in there, i.e. no aggregation done in the previous layer (ASA or SparkStreaming) or the data should be aggregated in the processing layer eg. Count of events in 1 minute, etc?

Sorry for long post and lots of questions. I am just a newbie willing to learn!
I will appreciate your input a lot!

thanks",1580820871,2,0,1,dataengineering
"Hi, we are hiring Senior Data Engineer @ Chartbeat in Remote or New York, NY, USA",,1580810980,0,0,1,dataengineering
RocksDB Is Eating the Database World,,1580773207,1,0,1,dataengineering
Has someone implemented Data Vault 2.0 on Hadoop/Hive/Impala?,"At my company, we are researching a lot the DV 2.0 data model and making some PoCs, but there isn't a lot of experiences on the web. I'm concerned about data replication (keeping data history in the Enterprise layer, almost replicating data in our data lake). Even though is not exclusively DV related, joins are costly and time-consuming with Hive and even with Impala. We already developed pyspark applications to reduce the time of this joins, getting interesting improvements, trying to get better times for constructing the staging, enterprise and data access layers. We are already using parquet files and partitioning.

I would appreciate any experience you could share with me",1580763532,12,0,1,dataengineering
Database versioning and schema migration tools,"Hi y'all, I'm looking into how to properly version my postgres database so that I can say ""migrate existing database to this state, or this state is bad, lets rollback to a previous version"". Basically git for databases.

I'm looking at alembic right now because I am more familiar with Python. However, I see a lot of posts about Flyway.

What do you guys think? Thanks!",1580743451,3,0,1,dataengineering
How would you ETL this one? Processing Flat Files with custom delimiters,"Hey Folks.  


I've been looking into methods on how to process a series of daily data snapshots into ""upsert"" daily files.

These files were originally extracted from SQL Server using the BCP utility.  The thing that is annoying about them is they have custom field (a string pattern of |&lt;&gt;|) and custom record delimiters (string of \~&lt;&gt;\~).  If you open these files in a text editor, they are one long line, regardless of how many records they are.  There is no header details, but the schema is known.

I was thinking of processing these files into a PostgreSQL database, but I cannot use the COPY command.

The next thought was to use Python and either Pandas or PySpark and get the files into a dataframe and then into PostgreSQL.  While I can see that this would be possible, I am not 100% sure on the actual procedure.  My initial thoughts are to use `df = SQLContext.read.text('/File/path.dat', lineSep='\~&lt;&gt;\~')` and then split each record (still not 100% sure on how to achieve this yet).  From there, I would stage into the PostgreSQL database for further processing

Can anyone give any other suggestions?

Cheers",1580722999,9,0,1,dataengineering
Is there a reason almost all big data tools are written in Java?,,1580695357,8,0,1,dataengineering
Ways to efficiently store large amounts of simulation data for my PhD (**Any** feedback appreciated !!),[removed],1580676317,1,0,1,dataengineering
Building a Modern Batch Data Warehouse Without UPDATEs,"Hi there!

I wrote about adapting the Star Schema to a modern data stack and ""functional data engineering"" in the below blog post.

It's the second time I share a data engineering blog post here, and last time we had really constructive discussions, looking forward to it! 

https://towardsdatascience.com/building-a-modern-batch-data-warehouse-without-updates-7819bfa3c1ee?source=email-2fa68a257a2-1580481817235-layerCake.autoLayerCakeWriterNotification-------------------------351515d1_83b5_49b5_8676_27a5370a26d9&amp;sk=821068431a519e522ab37fe664eb14cd",1580659516,7,0,1,dataengineering
Monitoring DB trends in AWS,"I was wondering about what the best workflows/tools are for a following scenario.

Imagine you receive data from \`N\` restaurants, on a daily basis, like how many drinks, dishes of certain type, total order count etc etc, a restaurant made. All these entries go into a postgres DB, with few colums {datetime, restaurant, type\_record, count}. Number of restaurants is in the 100's, so I need something that does not need to be updated with a CONFIG file every time a restaurant is added to the system.  


Now I want to run a daily script that:

1) can query the DB, 

2)make some basic calculations

3) catch something like \`number of drinks for today for restaurant X\` is 15% higher than its daily average\` and push an alert to slack or pagerduty .  


All I can think of is to run this code on a simple lambda function. This implementation would mostly suffice but I was wondering if there are smarter/better ways to achieve this.

&amp;#x200B;

Details:

Latency of the query (steps 1 and 2) are not a problem.

The main problem is how to have such a monitoring system on the DB that is as simple as possible (easy to maintain). 

&amp;#x200B;

Any ideas?",1580645383,5,0,1,dataengineering
"Join Joyn, the next Netflix",To all data engineers: come and join an awesome company and an equally awesome Team https://jobs.lever.co/joyn/8e588223-4fc7-46da-befb-84cde214ee6e we have anything from Junior to lead and this is just an example Position,1580557381,2,0,1,dataengineering
Open Data Engineering positions,"We still have two open DE positions open for a growing team.  Team will soon be moving from an on-prem stack.

Looking for experience with AWS native, Spark/Python/Scala, Talend and/or Streamsets would be nice and building pipelines end-to-end

Must have strong SQL background and preferably legacy ETL experience.

Must be willing to relocate to Roanoke VA

Salary range is in the 85-120k range depending on experience

DM me if you’re interested in talking more.",1580514376,3,0,1,dataengineering
What should be the roadmap in 2020 to learn data engineering and make a career switch by 2021?,"Currently working as a web developer.
Need suggestions on technologies, courses, books and learning path.
Although the path could lead to Data Science and ML but right now I have no interest in pursuing those except studying mathematics for foundation.",1580447869,25,0,1,dataengineering
How to be agile when sick?,"I've been sick for the entire sprint and my burn down sucks. Everyone sees it and it makes me feel bad. Agile just feels hard sometimes, coming from a scrum master. Helppppp :(

P.S. Customers don't care if I'm sick lol",1580443137,11,0,1,dataengineering
Spoken Querying with SQL,"We have built this system that enables practitioners to dictate SQL queries with multimodal interaction (speech- and touch-based correction) on emerging environments such as smartphones/tablets. This allows users to compose ad hoc queries over arbitrary tables and even slice and dice their data on the go. Although the current project is done in an academic research environment, we want to understand how this can be useful in an industrial setting. It would greatly help our research project if you can fill out this questionnaire: [https://docs.google.com/forms/d/e/1FAIpQLSe14TTwGyaLtK0DL80h3OY20rLrQoEPpHu5hyRr9HVdxGZEuQ/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSe14TTwGyaLtK0DL80h3OY20rLrQoEPpHu5hyRr9HVdxGZEuQ/viewform?usp=sf_link). We can send you the link to our system if you would like to try it out.",1580432527,1,0,1,dataengineering
Planning to take Udacity Data Engineer Nanodegree after reading reviews,,1580413720,13,0,1,dataengineering
Data Vault Modeling Primer for Heterogeneous Data,"Pretty neat reference document for Data Vault modeling something which guys snowflake advocate. 

[https://elib.uni-stuttgart.de/bitstream/11682/10311/1/Integration\_of\_Heterogeneous\_Data\_in\_the\_Data\_Vault\_Model.pdf](https://elib.uni-stuttgart.de/bitstream/11682/10311/1/Integration_of_Heterogeneous_Data_in_the_Data_Vault_Model.pdf)

Has anyone been using Data Vault Modeling at work ?",1580407646,3,0,1,dataengineering
My company are hiring a Lead Data Engineer - AI driven Start up Proptech company in NYC. 180-220k salary plus equity.,"Hi Guys 

I am working for an AI driven PropTech company in New York City and we are hiring a Lead Data Engineer to take ownership of the Data Team. 

My manager is looking to speak to Senior Data Engineers based in New York with 5+ years Python programming experience and experience with Airflow and AWS redshift! 

Happy to share more details if you want to email me at [colm@alldus.com](mailto:colm@alldus.com) with CV. 

&amp;#x200B;

All the best",1580403859,0,0,1,dataengineering
Implicit vs Explicit data sources,"Would an example of an implicit data source be reporting data like Google Ads (e.g. search term reports that are read only) and explicit be categorizing a domain as news, entertainment, etc?

Implicit is read-only whereas explicit you have to write.

The definitions for explicit are not as clear so I wanted to get your thoughts on if I'm thinking about it correctly.",1580401507,0,0,1,dataengineering
Use Azure Blob-storage as a simple document-store.,"Hi, I had a thread earlier about writing data to blob-storage without having to worry about duplicate records. I have now made some python code that allows for using blob-storage as a simple NoSQL database. Data can be stored in json or csv and read as regular blob-data. Documents can be up to 100MB in size.  
Hope some of you get to use it. (And please contribute, code should be ported to newest version of azure python sdk)

 [https://github.com/maka89/Document-Blob](https://github.com/maka89/Document-Blob)",1580386428,0,0,1,dataengineering
Honest reviews I found about DataCamp,,1580384251,2,0,1,dataengineering
What are the advantages of dbt against running SQL queries?,"Hello. I have read some posts about dbt but it is not very clear to me why would I use dbt instead of running SQL queries directly (using Airflow or other tool). As far as I understood, the main advantage is that you can code things that in SQL are not possible, like loops. But beyond that, is there anything else?

Right now, for me it seems that it is not worth the time you have to spend to learn the tool comparing with the advantages. 

Could you give me some insights on this? Thanks",1580380006,10,0,1,dataengineering
Rule Proposal: No Job Listings,"There was a recent post that is just a straight job listing, and as far as I can tell there’s no rule against it. I generally like the career transition advice posts. I’d even be cool with an “I’m a headhunter and these are the skills we’re looking for in data engineers” type post. But headhunters posting straight job listings could really dilute the value I (and I’d imagine others) get from this sub. Thoughts?",1580344933,15,0,1,dataengineering
Fully Remote - Scala Data Engineer - PropTech - Salary: $170-240K DOE,"Helping a PropTech client look for a fully remote (US) Scala Data Engineer!

**Why work here?**

* \-Fully Remote -
* Salary: $170-240K DOE
* They put cutting edge tech at the forefront of their business developing advanced algorithms!
* They recently received $60M in funding to scale and grow their platform as well as investing in R&amp;D to scale new products.
* Unparalleled work culture to achieve the most collaborative/innovative environment possible!

**What They're looking for?** Our client are looking for a Scala based data engineer who is proficient in using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.

Contact: [philip@alldus.com](mailto:philip@alldus.com)",1580332720,4,0,1,dataengineering
GCP Cloud Composer and AI Platform Notebooks,[removed],1580327981,0,0,1,dataengineering
Want to switch career from Security Analyst to data engineer,"Hi Guys,

&amp;#x200B;

I just need advice from experienced folks here, im currently a Soc analyst and handling vulnerability assessment project (gathering raw vulnerabilities report on qualys and transferring the useful data to MS Excel -&gt; and making a dashboard report on Power BI.) I realized that im enjoying it rather than the security side stuff. However i do have a little background with SQL and python. Do you think i can transition to Data engineer with just self study? 

&amp;#x200B;

Thanks in advanced",1580295779,15,0,1,dataengineering
What are good questions to ask your DE interviewer?,,1580247426,13,0,2,dataengineering
How do you organize/document the re-architecture of an existing data pipeline?,"Hello DE Community,

I started a new role in October 2019 for a healthtech startup on their analytics team and all of my work the first 3 months has been data engineering which is new for me. We're a small team of 6 and for the most part all generalists. 

Our team is in the process of building a new data warehouse in Redshift and my job to-date has been building jobs to move the source data from our application databases (Postgres) and push it into Redshift. I just got assigned a new project which is to recreate part of our current data pipeline that send data to our BI tool.

The work right now is just documentation and analysis… mapping dependancies, mapping which .sql scripts produce specific outputs that get picked up etc. How do you all go about documenting and organizing a project like this? I'm essentially pulling apart the pieces that make up the job now and putting them back together using the data that we migrated from our application databases.

For context if it matters, we're an AWS shop and use Matillion as our ETL tool.",1580233975,10,0,3,dataengineering
How do you organise your Airflow dags/scripts?,"I'm new to Airflow and want to know some best practises for organising scripts and dags. For example, in a somewhat complicated and multi-step DAG, I can imaging having this all in a single file would get unmanageable. How do you handle situations like this? Factor them out into a common ""scripts"" directory or something?

Some ""real-world"" example DAGs would be much appreciated too.",1580222353,13,0,1,dataengineering
onTimer bug in Flink ProcessorFunctions on Kinesis Analytics?,"Hey folks,  


Some strange behaviour I've encountered that I'm wondering if anybody has run into.   


We are using the ProcessorFunction ([https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process\_function.html ](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html)) functionality of Flink in order to do some stateful processing of events. In particular, we're leveraging the timer callback [https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process\_function.html#timers ](https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/operators/process_function.html#timers). For some reason, these timers perform perfectly well locally but the identical code loaded into Kinesis Analytics won't fire the ""onTimer"" function. I've logged everything up until then and it works perfectly well. It's just that the onTimer functions won't seem to fire. Is there some limitation of Kinesis Analytics that might prevent this? Any recommendations you might have?",1580199101,0,0,1,dataengineering
Snowflake auto commit and Duplicate,[removed],1580194578,0,0,1,dataengineering
How to store Tensorflow or Keras model as a JSON?,"How can I convert an hdf5 weights file to a json to store it in a database? 

How can I convert it back to use it again?

Key points:

Using ArangoDB

Willing to consider hdfs + spark

Need to store weights and architecture

I am using tensorflow in python

HDF5 is terrible

&amp;#x200B;

Thanks!",1580164298,0,0,1,dataengineering
Trying to architect a new dataset be like...,,1580159120,6,0,1,dataengineering
Automation and framework development in data engineering,[removed],1580158610,0,0,1,dataengineering
Rapid Processing of Large JSON and CSV Files on the Command-Line,,1580155375,0,0,1,dataengineering
Checkout my new Post!,"Hey Guys! I have recently taken to writing to share my findings  while  working with Glue on AWS. Hoping to get some feedback and possibly claps if you liked it. 

Your insights will absolutely help me in tweaking my writing style.

[Leveraging Glue as a Central Meta Store](https://medium.com/@christo.lagali/leveraging-glue-to-act-as-a-central-metadata-store-402c753b14e?source=friends_link&amp;sk=91d17a6a15e4609bea50e25904afd535)",1580148703,0,0,1,dataengineering
Writing a self-contained ETL pipeline with python,,1580148104,4,0,1,dataengineering
Airflow DAGs inside of Docker image?,"Iv noticed most people who use Airflow and Docker do not copy their dags folder into their Docker image and instead mount it externally or through github.

Is this best practice and what are the pros and cons?",1580141510,8,0,1,dataengineering
An interview about how the Great Expectations framework helps you add meaningful tests and validation to your data pipeline to drive down technical debt,,1580134962,0,0,1,dataengineering
Do you know Lightbend Cloudflow ?,"Hello,

Has anyone tried Lightbend's Cloudflow ? It looks quite polished, I'm curious of it. Does it integrates well with Spark Structured Streaming and the Dataflow model ?",1580131749,0,0,1,dataengineering
Is the future being a mix of Data Engineer and Data Scientist?,"Hello. I am currently working in a position which involves both. I research new ML models, I code tools to automate things, I add new variables to the BI tool, I create new reports, I deploy ML models into production... And I make an extensive use of AWS.

I am amazed on how easy are certain things with AWS and how much work it saves comparing to only a few years ago. Even though, there are some areas in which it could be better, but other companies as Snowflakes arises to cover that niches. In Snowflake pretty much everything is fully self administrated and you don't have to do anything, and AWS has services like Personalize or Forecast, which enables a non technical person to create decent models; or things like Sagemaker, which makes ML developing and deployment very easy. 

As time passes, everything is getting easier and easier and requires less work in both sides, data engineering and data science so I am wondering that maybe in the future those specialicites doesn't exists and the only thing will be Full Stack Data Scientist or Data Scientist Engineer or whatever. 

What do you think?",1580125837,13,0,1,dataengineering
Apache Kafka message encoding and schema management,,1580121940,0,0,1,dataengineering
Multi Matrix Deep Learning with GPUs,,1580118590,0,0,1,dataengineering
Kicking the tires on BigQuery – Google’s Serverless Enterprise Data Warehouse,"Part 1 - [http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-1/](http://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-1/)

Part 2 - [bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-2](https://bicortex.com/kicking-the-tires-on-bigquery-googles-serverless-enterprise-data-warehouse-part-2)",1580118003,0,0,1,dataengineering
How to learn data science “best practices” if you’re the only data scientist on the team?,"I know this is /r/dataengineering but I think y’all can give me valuable insight as well. 

 I’m a grad student in my final year. 

I just accepted a spring internship at a well-known tech company that  doesn’t have a data scientist in the particular group I’ll be working in. If I do well, the plan is to be brought on full time post graduation later this summer. 

I know a lot about stats, ML, A/B testing etc. However, I’m less familiar with putting things in production or writing “production level code”. 

Are there any books/learning resources I should look into before I start? 

At the moment, I’m considering [Clean Code](https://www.amazon.com/dp/0132350882/ref=cm_sw_r_cp_awdb_t1_wyIlEb93NCPQF), [Designing Data-Intensive Applications](http://shop.oreilly.com/product/0636920032175.do), and [Geurilla Analytics](https://guerrilla-analytics.net/). Which (if any) if these should I read?

Any other recommendations/words of advice are much appreciated!",1580085529,7,0,1,dataengineering
Daily Engineering Daily Resources,"How are you keeping up to date with all the new technologies in the data engineering topic? So many tools, so many possibilities. What resources you read on daily basis to get to know the most about DE? What are most-worthy blogs, sites which is worth to subscribe?

I would like to create a complete feedly list with these resources. Every tips are appreciated.",1580062360,3,0,1,dataengineering
Google just published 25 million free datasets,,1580048079,4,0,1,dataengineering
A Data Engineer's Naive Foray Into Data Science,,1579993913,0,0,1,dataengineering
Best way to design Bucketing for Hive table (Spark on Hive),"What is the best way to design hive table for best performance for below scenario? 

Scenario

1. Columns : user\_id,load\_dt/timestamp,content\_id,content\_url,referral\_url,session\_id
2. Each day wiki user activity logs are received with Kafka
3. Approx 100 M events are received in 5 min window
4.  these logs are processed by Spark
5.  number of users/number of events are not fix for interval 
6. Facts needs to be calculated : 
   1. number of user  visited page by each day
   2. number of users by country (join with another table = user profile info)
   3. how many users came through google url (referral url column)
   4. how many users revisited the page (find out based on diffferent session id)

&amp;#x200B;

My thoughts on design

Activity table

1) partition table by load\_dt

2) create 10 bucket with cluster by  session\_id 

&amp;#x200B;

User table

1) partition by country

2) index by user id

&amp;#x200B;

Any feedback on this design ?",1579978164,4,0,1,dataengineering
open source data catalog tools,"Can you please share your experiences with open source data catalogs like ckan, dkan etc? I would like to use one of them in my org.",1579960396,5,0,1,dataengineering
What is a decent salary with a couple years of experience?,[removed],1579920195,0,0,1,dataengineering
Anyone use Greenplum database?,"Has anyone used Greenplum? If you did, what client did you use with it (pgAdmin, something else)? Any tips or thoughts about it?",1579885030,0,0,1,dataengineering
Where do data engineers sit on your org chart?,[removed],1579867458,0,0,1,dataengineering
Speeding up SQL Server Data Warehouse Architecture With Automation Procedures – 10 Problem-Solution Scenarios To Jump-Start Your Development,,1579864177,0,0,1,dataengineering
Data Engineering or Software Engineering?,"Just about to graduate college and am considering what I want to try to make my life’s work. In your opinion, is data engineering or software engineering the better field to enter?

I suppose things I specifically want to know about how it compares in each field are the following:
1. Competition, or how many compete for a data engineering job v a software engineering job
2. Salary
3. Job satisfaction
4. Future industry stability, or how well the industry is expected to do 20 years down the line",1579844646,0,0,1,dataengineering
ELI5 - Delineation between ETL and ELT,"Pros and Cons?

Use cases?

Theoretically speaking, wouldn't the lead time between \[ETL\] vs \[ELT\] be the same?",1579823328,0,0,1,dataengineering
Better term for non-backfillable?,"Is there a better term for non-backfillable data?

For example, an API will return data for dates in the past on some metrics.  If your job fails to run for requesting data on a date in the past, you can run the job again for the same date and get the data.

For other metrics it will only return lifetime values at the time of call.  Thus, if your job failed to pull lifetime values at the correct time, that data would be non-backfillable or &lt;insert better term&gt;.",1579811252,4,0,1,dataengineering
Question for anyone who has or is currently mentoring someone?,"What do you look for in a mentor / mentee relationship?   
How could someone provide value to a mentor?  


I'm asking because I would really like the ability to work with someone and have them offer me guidance.",1579798376,3,0,1,dataengineering
Best DE certification 2020,"Hello Guys,

I am working as a data engineer for 1 and a half year and I want to get a certification.

Initially, I wanted to get certified in CCA 175 Hadoop and Spark developer, but I do not find good online material for preparation.

&amp;#x200B;

Do you recommend me another certification?

&amp;#x200B;

Thanks for the help !",1579791439,9,0,1,dataengineering
Kube Explained: Part 2 - Containers,,1579782766,0,0,1,dataengineering
"Monitoring Sonos Devices with ksqlDB, InfluxDB, and Grafana",,1579777554,1,0,1,dataengineering
"How to answer the interview question ""describe a pipeline that you've built"" ?",[removed],1579724966,0,0,1,dataengineering
Check out my new Post!,"Hey Guys! I have recently taken to writing to share my findings  while working with Airflow on AWS. Hoping to get some feedback and possibly claps if you liked it.

[Implement Parallelism in Airflow](https://medium.com/@christo.lagali/implement-parallelism-in-airflow-375610219ba?source=friends_link&amp;sk=6ae3e98d4a43f71495d86cc1073e18e4)",1579711555,0,0,1,dataengineering
Airflow vs Appworx,"We're currently looking at running airflow for process automation and managing data flows.  We have a poc together to show management, but they're concerned about feature parity with our current monolithic automation tool called ""Appworx"".  Has anyone worked with both and can speak the comparison of the two?  Google Fu brings up nothing, probably because Appworx is a legacy application that's not too tier anymore as far as I can tell.",1579703134,2,0,1,dataengineering
Snowflake users - how have you overcome not being able to use stored procs (unless you know JS)?,My company uses Snwoflake and I personally utilize DBeaver to query... but I sorely miss stored procs...,1579694859,36,0,1,dataengineering
Recommendation for team collab tools,"Hi everyone! I just became responsible on an engineering team and currently in the process of transforming the workflow, any recommendation on what should I be implementing for efficiency and collaboration? 

I'm planning on using JupyterHub, deploy flask apps, etc..",1579647518,5,0,1,dataengineering
How to use the BigQuery Plugin in Grafana ?,[Using BigQuery plugin in Grafana](https://stackoverflow.com/questions/59840012/using-bigquery-plugin-in-grafana),1579619489,0,0,1,dataengineering
Need help!,,1579609769,0,0,1,dataengineering
Data engineering jokes,"Slow work day here.   


Anyone got any data engineering jokes?",1579604635,18,0,1,dataengineering
SQLAlchemy won't work under Airflow with Python 3.5.2 on MacOS,,1579598210,0,0,1,dataengineering
Creating a Python Poetry Package for PySpark on Kubernetes,,1579580301,2,0,1,dataengineering
Druid vs Alluxio for kubernetes???? Anyone have any experience?,"I want to put an in-memory layer between HDFS and kubernetes pods. I'm looking at Alluxio and Druid but I can't find any documentation on people comparing these two.

From the looks of things, Druid looks far more mature but as I've never used either, I can't say for certain.",1579559492,0,0,1,dataengineering
My data collection/processing methods are always very MacGeyver-esque. Is this a problem? Is it normal?,"An example to illustrate my point:

DE is extracting client data from pdfs at the request of another department in their company. While each of these files is visually identical in format, they differ enough under the hood that programmatically parsing the files and extracting all of the information isn't an option. 

A pdf-parser library lets DE consistently extract *some* of the information they need, but not all of it. An OCR library lets DE extract *some* of the information, but, again, not all of it. Between the two, all of the information is available, so DE cleans up the results from each library and merges them. Very Frankenstein.

Is this standard for such a problem? Am I doing something horrifying to more tenured DEs? I'm at a tiny company and the only person who really serves to check-and-balance my decisions is the long-time DBA, so there isn't much standing in the way of me and my own stupidity.",1579555645,9,0,1,dataengineering
An interview about how Mayvenn replatformed their production dataflows using Ascend and improved their ability to deliver meaningful analytics to their business,,1579536102,0,0,1,dataengineering
SAP cloud Data Warehouse,"Hi Engineers, 

The company I work for is at a T junction moment, a few different possibilities that I've been helping guide them through.   
A real curve ball has entered the field though SAP cloud data warehouse. 

I have no experience of it, I know no-one who uses it or has even mentioned, obviously that's alarm bells straight away.   
Anyone have any experience and can offer a review?",1579528826,6,0,1,dataengineering
Thinking about going full throttle in DE aiming to be a freelancer,"Hi all! 

I’m thinking in going deeper into data engineering (I have basic working experience) aiming to achieve some independent work (freelance) in this area in the future.

Do you recommend this over a more traditional path, like web developer? Any thoughts?",1579496228,13,0,1,dataengineering
Kafka Hello World,,1579489968,0,0,1,dataengineering
[Airflow / Snowflake / S3] - is there a better way?,[removed],1579476806,0,0,1,dataengineering
Why do we need a pipeline?,"I was learning about Kafka and ETL pipelines which made me ask myself a fundamental question: why do we need a pipeline in the first place? Why can’t we just do all the ETL at one place. 

Here are the points I came up with:

1. Difference in rate of production vs consumption. 
Maybe you have an incoming stream of data and you don have the resources to process the data as fast as it’s coming in. In that case, use some message-oriented middleware and build a pipeline. 

2. High coupling
Without a pipeline, the sources would know about the destinations. They both would be highly coupled. If your sources and destinations are many-to-many, then your communication code could easily get messy. Solution? Use a pipeline. 

3. Scalability and Reliability
This is as a result of high coupling. Without a pipeline, we wouldn’t be able to scale producers and consumers separately. Also, both the producer and consumer will go down together in case of server failure. 



Pls correct if i’m wrong with anything. 
Also, what else? What did I miss?",1579426406,4,0,1,dataengineering
What's a good solution to save JSON response so that it can be queried later? -looking for professional advice.,"Hello fellow engineers,

I am working on a data pipeline which gets data every 5 secs from an API using Nifi, does some processing. I want to save the data with time stamp somewhere so that this can be used later.

I get 12 responses a minute which is 17,280 records. I need to query them by minute in future.

What's the most efficient way to store them such that retrieval is done with minimal latency?

Thanks..",1579421628,9,0,1,dataengineering
"How to automate the retrieving, extracting, storing of excel data in most fault tolerant way?","Hey everyone, I need some advice about a project I'm starting for a client.

Basically I need to automate the retrieving, scraping and storing of tens

of excel workbooks which are located on the internet, using python. They are all mostly different

with datapoints I need at different places in the sheets. The process should be repeated monthly

for each workbook but on different dates etc.

&amp;#x200B;

The thing that's bothering me the most is how to automate/schedule all of this and make it as fault

tolerant as possible (and easily recoverable by less technical people). I was looking into tools

such as Airflow and Luigi but I'm not sure if they're an overkill for me and do they fit my needs at all.

Anyone knows are there any other scheduler/automation tools out there for such purposes?  


Any help very appreciated!",1579390994,7,0,1,dataengineering
How did you start?,"I""m just starting my career as a python developer and I'm aiming the position of data engineer.  
It has been less than a year since I first wrote a 'hello world' and I want to know if I could just jump into a DE position and what background should I have",1579390399,6,0,1,dataengineering
Updating json-records in blob-storage,"Hi all. I have a problem I would like some input on. 

I have an incoming stream of events in the form of json records coming from azure event hub. I want to save these. However, these events have a key, so that I want to overwrite an existing record if there is one already with the same key.   


I should probably use a NoSQL database like Azure Tables, cosmos db, but the first is not very compatable with Databricks, and the second is a bit overkill. 

So any tips on how to save these records to blob storage with resonable performance?   


(I need it to be compatible with databricks so I can direct query the data in Power BI using Databricks)",1579363687,1,0,1,dataengineering
Fast ETL in Python. Ideas?,"I’m working on an etl Pipeline that feeds a bunch on ML models.

At the moment we extract data from a few sql dbs, do some feature extraction and dump everything in another sql dB, where we then read the relevant columns for the model we’re running.

Everything is in Python, mostly pandas. We have a custom optimised sql read (pyodc) but a lot of the merges, groupbys (rolling) could be parallelised. 

Everything is spin up on docker, running on blade server. I’d like to make better use of the 40 cores at my disposal. Especially if we scale this to cloud instances with even more cores and memory.

We’ve tried pandarallel, but it is pretty flaky. I’m experimenting with dask, but it requires quite a lot of changes. Ray / modin don’t speed up merge or groupby according to the docs.

Any experiences with similar problems?

This runs every night, and takes a few hours. The aim is to run it faster.",1579361150,44,0,1,dataengineering
How do you handle CDC / Audit (created/modified metadata).,"Hi All,

Have what I imagine is a common problem.

We're currently redoing some data infrastructure - python shop, use airflow, mostly batch jobs into   
postgres RDBMS (aws rds). 

How do you all handle database changes? I feel like the traditional approach is to have a date\_created, created\_by, date\_modified, modified\_by for every table?

This is pretty painful both from an application code POV and also a table creation POV. Not to mention that you miss information on changes between create/last modification. Surely there's a better way?

&amp;#x200B;

The data size is in GB, is it practical to read the WAL and store it to S3? Basically I'd like to be able to do a full audit on any users/apps/data.

&amp;#x200B;

Thanks",1579344006,5,0,1,dataengineering
Looker's LookML,How does LookML work? Is there a library or tool available that generates SQL the way LookML does?,1579303356,1,0,1,dataengineering
What info should be within a schema in a relational database?,[removed],1579263882,0,0,1,dataengineering
Opinions of Apache Airflow,"I've been looking at Airflow (creating a PoC) to manage some of my work recently, having previously been using an in house tool, and while it clearly has better functionality I am struggling to get along with it.

What are people's opinions of Airflow's user experience? I have read so much about using it that I feel like I'm missing something. Does anyone have any recommendations for reading that has helped them on their journey with Airflow?",1579262599,49,0,1,dataengineering
[Recommendation] ETL Tool,"Dear Data Engineers,
The new company I joined a few weeks ago is evaluating ETL Tools to work with their Snowflake Databases. So far, Talend and SnapLogic are leading the ranking. However, I am not sure whether Talend is a good match. I have so far only heard people complaining about it. Arguments were primarily unintuitive and legacy eclipse based user interface, generally old tool (almost 15 years old), basically a blackbox where you cannot really see the underlying code and a lot of bugs which were hard to reproduce and took a lot of time to fix even with the help of the Talend support.

Has anyone of you had similar experiences regarding Talend? Also do you have any considerations regarding SnapLogic? I personally think that Matillion looks quite promising. I have a Spark background myself and I would prefer using it, but a tool is required which is also suitable for non-tech users.",1579256714,28,0,1,dataengineering
What to expect in Data Modeling interview (Big Data technology)?,What steps do you follow to attack data modeling problem ? Do you start with ER diagram or directly start by creating table. I have never gave data modeling interview before so not sure what to expect. I also can't find sample Data Modeling interviews on youtube too.,1579232472,4,0,1,dataengineering
Recommendations for Pipeline/Transform Tools for Startup,[removed],1579199587,0,0,1,dataengineering
Business Usecase,[removed],1579196123,0,0,1,dataengineering
Any Data Engineers here in education?,"K-12 in the U.S. more specifically, but curious to hear from others as well. I ask as I currently have the title of Data Analyst but I've had some brief discussions with boss on transitioning role. I already spend a lot of time on database development, ETL, and building of data applications. I also have been implementing Airflow to centralize automations, interact with api's, etc.. So yeah wondering if that's enough to push for 'Data Engineer' title.",1579193170,10,0,1,dataengineering
30 days of free access to IBM Data Science and AI Programs,,1579166222,8,0,1,dataengineering
How to Learn Data Engineering if You're a Data Scientist that specializes in statistical modelling,I'm a Data Scientist with a background in pure mathematics. I have no formal training in Computer Science.,1579160708,7,0,1,dataengineering
Advice for starting on an Agile DE team,"I recently landed a DE role with a team at a very large company in my area. In the interview, the manager mentioned that the team operates using Agile.

I've worked in analytics and data engineering type roles before, but this will be my first time working in Agile and actually my first time working in a Scrum environment in general.

I'm really excited for this role and want to be as prepared as possible. Any advice from others who have worked on Agile teams? Also accepting links to good learning resources about this topic. Thanks!",1579144039,5,0,1,dataengineering
"We have the results for ""What do you call a group of Data Scientists""!","CLUSTER is your winner!

I really wanted kaggle gaggle to win but the people have spoken

Check out the rest of the results here : [https://greatexpectations.io/blog/datasci-counter-poll/](https://greatexpectations.io/blog/datasci-counter-poll/)",1579140456,1,0,1,dataengineering
Any advice for Netflix onsite data engineer interview?,"I will be going thorough 5 round.coding,modeling, design and 2 behavioral. Do I need leetcode practice for coding round?",1579110867,11,0,1,dataengineering
SQL Server DBA Being recruited for Postgres and Denodo Role,[removed],1579053698,0,0,1,dataengineering
Final round Interview for a data engineering role with potential team mate. Tips and suggestions are appreciated,"Hello everyone,

I am having a final round technical/culture round with a data engineer. This company builds SaaS platforms for e-commerce. I am desperately looking to move from my current job and would like to stand out among two others who are going to give interviews tomorrow. 

What should I remember and work on?

How would you evaluate a potential team mate when you are interviewing someone?

What are the traits you look for?",1579041414,4,0,1,dataengineering
New to Airflow - Need help with Bashoperator,[removed],1579032422,2,0,1,dataengineering
An interview about YugabyteDB and how it was architected to power the new generation of planet scale applications,,1579020986,0,0,1,dataengineering
"Last semester, should I take Data Mining or Machine Learning?","So I am actually a Senior in Mechanical Engineering with a Minor in Computer Science and will be graduating this May.  While I study ME, I have focused mostly on CS the past 2 years and have accepted a solid full time job offer as a data engineer at the startup I currently work at.  I only have to take 3 classes to graduate and am working part time. However, part of me wants to take Data Mining or Machine Learning. At my school these are pretty rigorous courses and I am wondering if you guys think it would actually be VERY beneficial to take one of these courses.  If I do I will be quite slammed this semester as my last three courses and part time data job will occupy roughly 60 hours a week.

What are your guys' thoughts? Would these courses greatly benefit me? Should I take neither and focus on personal projects that I have put on hold and maintain a good work/school/life balance my last semester?",1579019009,10,0,1,dataengineering
Has anyone read the Facebook storage research paper?,"https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf

Has anyone read this paper? I had a few questions about this",1579015285,2,0,1,dataengineering
Most in demand data analyst tech skills,"I scraped three job listing sites to find which data analyst tech skills are most in demand. I compared the results to data scientist and data engineer job listings. Here are the top ten results for ""data analyst"":

https://preview.redd.it/1lz5njid5ra41.png?width=700&amp;format=png&amp;auto=webp&amp;s=204dbd09a592b2efb03a187de50e53e70c856745

See the full article here: 

[https://towardsdatascience.com/most-in-demand-tech-skills-for-data-analysts-26d4ea4450f8?source=friends\_link&amp;sk=33f4dbcef7bbda51493608fe53c47ec1](https://towardsdatascience.com/most-in-demand-tech-skills-for-data-analysts-26d4ea4450f8?source=friends_link&amp;sk=33f4dbcef7bbda51493608fe53c47ec1)

Constructive feedback appreciated!",1579012102,9,0,1,dataengineering
Postgres Upsert - fragmentation issues,"Hi everyone..

Wondering if anyone's got experience with using Postgres UPSERTs for ETLs and if so, have you ever experienced issues with fragmentation and bloat on the tables?

We have hourly batch ETLs upserting into our tables (tables \~ 10s of Millions, upserts \~ 10s of thousands) and we have auto vacuums on AWS, however too often we're having to manually run full vacuums to get the space back and prevent processes from hanging. It feels like we are constantly fighting a battle against it and if left alone for a few days, things would deteriorate to a stand-still.

Has anybody else experienced this issue? Does Upsert fundamentally have a negative impact on fragmentation and if so, what are other people using?

I've done a bit of reading on the issue but nothing conclusive, for example --&gt; [https://www.targeted.org/articles/databases/fragmentation.html](https://www.targeted.org/articles/databases/fragmentation.html)

&amp;#x200B;

Thanks as always!",1578994981,1,0,1,dataengineering
Is Apache Spark suitable for parallel running of a binary?,"I have a Java or C++ binary that I need to run 100 times in a parallel fashion. The binary contains about 10,000 lines of code doing complex simulations with both numerical and non-numerical computation. 

I know I can use various schedulers (OpenPBS) deployed on a cluster to achieve this. I haven't explored Apache Spark properly yet. Is my requirement within the set of Spark capabilities?",1578985090,6,0,1,dataengineering
Do we need InfoBurst with BusinessObjects and Tableau?,"I am an architect at a corporation that currently uses Business Objects and Tableau. We have used BO since \~2013 and picked up Tableau later in \~2017.

There's been a investigation into InfoBurst, a tool that does scheduling, bursting, publishing and delivery of reports. My understanding is that BO and/or Tableau do this to a degree.

The thought is that our group wants to leverage InfoBurst as a means to integrate between the two (BO and Tableau).

The current support team challenges in supporting BO and Tableau are noted:

1. have to use BO admin tool to setup/manage WebI and Crystal reports

2. have to create subscriptions, extract refreshes and create manual powershell scripts for certain extracts in Tableau

I presume these tools are capable enough to where I guess I'm not convinced that either tool was set up with best practices when each were stood up, in light of the challenges we seem to be facing. I'm probably wrong, though...

Albeit, all I know is that BO is for data abstraction (and allowing for business rules) through universes and providing reporting, and Tableau is for reporting and visualization.

I'm questioning if we even need BO in the mid to long term, IOW can Tableau interface directly with data source(s) and allow for business rules/logic to be embedded? IOW, can one (Tableau) do the job of the other (BO?)

If so, would there then even be a need for InfoBurst? If not, I believe Orchestrator does posh, could that be leveraged? Surely InfoBurst is not the only way to leverage powershell?

Further, I'm wondering with our current stack:

Can Tableau and/or BO ***not*** dynamically deliver and refresh extracts to Tableau server/desktop, sharepoint, network share or email? Does it ***not*** have the capability of event-based scheduling based on db triggers, ETL schedules, etc.?

Is there ***no*** means to migrate BO universes (i.e., functionality) to Tableau? Is there ***no*** PDF ***nor*** PNG format supported for Tableau dashboard images? ***No*** means to publish to Sharepoint?

Does Tableau ***not*** provide auditing/tracking, alerting based on job execution status? Does Tableau ***not*** support multi- ***nor*** single-pass bursting?

Ultimately, trying to understand if InfoBurst is really a need or if it's a matter of the tools we have access to currently not configured properly.",1578964512,0,0,1,dataengineering
Data Managed Services,"I work for a manufacturing company that has been using sensor data with SCADA systems since long before “IoT” became a thing, but now we are finally trying to modernize our data architecture in the cloud to mature our analytics strategy. We don’t have the engineering expertise to stand up new pipelines quickly or maintain them. We’re considering outsourcing our data pipelines from SCADA -&gt; EDW/Lake to a managed service rather than build a team.    
   
Anyone have experience or recommendations with data managed services?",1578950048,3,0,1,dataengineering
Streams and Tables in Apache Kafka: A Primer,,1578939071,0,0,1,dataengineering
Schema Evolution in Data Lakes,,1578922028,0,0,1,dataengineering
"What is Hadoop ? Overview of Hadoop Ecosystem ,Architecture and all the components in simple terms !",,1578909054,0,0,1,dataengineering
Advice on the transition from Microsoft to AWS,"I work for a company that has historically been a Microsoft shop.  We use SQL server for our data warehouse and vendor application databases.  Our ETL processes (infesting external data and generating extracts for other vendors/apps) are all built using SSIS.  SSRS is our main form of end user reporting (200+ custom reports).  

There has been a push for us to find current MS DW processes that can be migrated to AWS native solutions over the next 12 months.  The end goal being to have the entire DW living in AWS (or as much as possible).  I am currently looking for an AWS friendly platform for our ETL processes.  To me, the first logical choice is AWS Glue.  However, I am having a hard time finding documentation on how to go about implementing ETL in Glue beyond very simple copy examples.  Our DW is a Kimball model with a central normalized data model and data marts that serve different reporting requirements.  Our ETL transformations are mostly looking up business keys in the normalized database to find surrogate keys and then joining relevant entities.  We have tsql stored procedures that SSIS will run against the normalized DB and write the results to denormalized fact/dim data marts for reporting.

I am looking for guidance and advice from anyone with experience with migrating a DW from Microsoft to AWS or experience with the capabilities of AWS Glue/Spark.  Thank you in advance.",1578884987,5,0,1,dataengineering
databricks-connect without using cluster,[removed],1578871412,0,0,1,dataengineering
What's the line Data engineering and business intelligence ?,,1578793131,0,0,1,dataengineering
What's the best way to learn as much as I can as fast as possible regarding Data Ingestion?,"Preferably involving technologies like Sqoop, Hive, Hadoop, Kafka, NiFi, Syncsort",1578791684,5,0,1,dataengineering
"Is an ""Intro to Data Engineering"" Workshop Feasible?","I've always had a passion for teaching, but not enough to become a teacher myself (I love programming way too much and its associated salary). Since there isn't really a traditional way of getting into data engineering, I was thinking of creating and hosting a workshop for people who either (1) know they want to get into data engineering but need a starting point or (2) have the skills to get into data engineering but are interested it what it looks like at a basic level. My goal is for every participant to have a project that they can include within their portfolio that shows that they understand the basics of data engineering (aka I actually want it to be a valuable investment of time).

In my mind, I'd have the participants create a Python script that pulls data from a public API, do some transformation on the data with Pandas, and insert the data into a database (likely SQL). And then have them complete some questions about the data that require knowledge of SQL. I'd also provide instructions ahead of time and make sure that all participants have the necessary environment set up on their laptop before attending the workshop. I would also likely give a dive into additional data engineering concepts that they should explore as a next step, such as Apache Spark, Apache Airflow, and NoSQL databases.

Would something like this be feasible? And/or am I crazy?",1578777949,41,0,1,dataengineering
Help narrowing down and fleshing our senior synthesis and project ideas for a soon-to-be graduate,"Hey guys, first post here but I frequently lurk here and on the datascience sub as well. During my undergrad I've had 3 internships and have had tons of exposure to data engineering, data science and analytics, and data modeling. Because of this, I want my senior project to incorporate at lease some of these techniques into it. Specifically, my project must pertain to my majors (applied mathematics and computer science), but I have a lot of headroom when it comes to how I choose to present my topic and relate it to these areas. I'm still in the brainstorming phase of what I want to do and I'd like help narrowing down my ideas and possibly fleshing out a few. Here's what I have so far:

1. E-Commerce platform that tracks all activity and records it in a DB. 
   1. From here I'd like to use the captured data to drive an analytics program or platform that can easily create dashboards and models. 
   2. I'm not really sure how to begin implementing this though, and I want to try and use methodologies and languages that are actually in demand (i.e. used in production and industry). 
2. IoT device that captures data of some sort of information or data and relays it back to a web server where the user can interact with it.
   1.  I've been exploring TensorFlow, RaspberryPi, NodeMCU, and a few others to see what I can build with these (they seem very versatile). 
   2. For this I'm not sure what specific area I should go for in IoT. Smart cities really intrigue me, but the solutions been discussed for smart cities appear to be on a much larger-scale that what I can feasibly achieve as a student. 
   3. Devices that monitor or track health data also intrigue me. Potential areas I have looked at are blood sugar levels, heart rates, and weight, exercise, and sleep patterns.
3. Anything related to ""buzzwords"". I can use this project to learn about buzzwords that I'm relatively inexperienced with, such as: 
   1. block chain
   2. cryptography
   3. centralized crypto-currencies
   4. AI (specifically autonomous vehicles, image/object recognition)
4. Finally, if it helps at all, here are the languages, tools, and frameworks I am familiar with
   1. Python
      1. Pandas, Numpy, PsycoPG2, Matplotlib, Seaborn, Sklearn, Apache-Airflow
   2. SQL
      1. PostgreSQL, Google Big Query, MySQL, MS SQL Server, GraphQL
   3. R (still pretty novice, but I understand the basics)
      1. RShiny, Caret, Tidyverse
   4. Web (still pretty novice, but I understand the basics)
      1. HTML, CSS, JavaScript, BootStrap
   5. Visualization
      1. RShiny, Tableau, SpotFire, Excel, Matplotlib, Seaborn
   6. Others
      1. C#, C++, Java",1578766164,0,0,1,dataengineering
Career Progression ?,"What do you typically see in your data engineering career progression pathway? If any? So far at my last two gigs I was the first of my kind.  So there has always been ambiguity on where/how to promote. 

From what I’ve seen around the industry it can be something like:
Jr. Data Engineer -&gt; Data Engineer -&gt; Senior Data Engineer -&gt; Data Architect ? 

What do you think ? What have you experienced ?",1578762831,6,0,1,dataengineering
A Guide To Modern Batch Data Warehousing — Extraction,"Hey! I wrote an opinionated guide to modern batch data warehousing (published to Towards Data Science), based on the research and work I have done when I participated on a migration from a ""classic"" to a ""modern"" data environment

It's heavily inspired by Maxime Beauchemin's Functional Data Engineering principles, and the design philosophy of Apache Airflow (a framework he created)

I hope you like it!",1578762208,20,0,1,dataengineering
It would be interesting to see a new take on data enrichment. Anyone have any thoughts ?,Something that solves more problems and a new way.,1578722037,1,0,1,dataengineering
Frustrating Problem With Continuous Sliding Window On Streaming Data,"Hi folks. I'll do my best to explain my problem. Hopefully somebody can give me hand.

So, I'll set the stage. We process perhaps 45k events a day every time a customer buys something. Suppose the event looks something like this:

    {
      customer_id: &lt;string&gt;,
      timestamp: &lt;timestamp&gt;
    }

We'd like to store the count of transactions (meaning count of events) in the last 3 days, for a particular customer in some data store. Preferably Dynamo. 

I'm using a Kinesis stream in AWS to produce the events, and I've explored consuming by either AWS Kinesis Analytics SQL or Flink. 

Here's the problem I'm running into:

I can only ever trigger the window query on the stream when a new event comes in. This becomes an issue as soon as the window moves past an event. I can't re-trigger the calculation so the count is off. Without any sort of TTL the count will be incorrect until a new event for that customer comes in.  


https://preview.redd.it/m6s4sx37s2a41.png?width=893&amp;format=png&amp;auto=webp&amp;s=248f739e7d0cc6bf9158e807492c4a367f1f4f0d

Here's a poor illustration to try to explain what I mean. Imagine each star is a transaction by customer foo. Suppose the current date is 2020-01-03. With Flink, as the event comes in on 03, I have code that will query the stream for the last 3 days and count the number of transactions for foo. I get 3 and update my cache. Perfect! But then as the window advances, the count remains 3. There's no other event to trigger the count on 2020-01-04 or 05, etc, so the count remains 3 until a new event comes in. 

I would like this count to somehow update every time a new event by customer foo leaves or enters the window. 

A couple of thoughts I have:  
1. I need some way to re-trigger the query and cache as an event leaves the window  
2. I need some way to periodically re-query and group all items in the window on some set interval, perhaps once a minute.   


I have ideas about how to do this that feel hacky. For example, perhaps each time I consume an event I push a ""delayed event"" into an SQS queue that becomes visible at current time + window length. Then, every time I eat one of these events I re-query the window for that customer. This isn't that ideal. I'm wondering if there's some canonical way to handle this problem. I'm open to using any software and exploring any options that you think support this in a very nice non-hacky way.  


Thank you!",1578716740,21,0,1,dataengineering
Setting Up Multi Node Apache Cluster,,1578689067,0,0,1,dataengineering
"I want to move from a Front-end developer to a data engineer, any path to follow?",,1578682945,17,0,1,dataengineering
Data Engineering vs. Database engineering role,"Hello data engineers,

I am about to take my first job as a data person and could really use some advice. I have two offers. First job offer: **data engineer**, no cloud resources, all on prem, Spark, Python, Bash. Second offer: **database engineer** (more of a database administrator role), also no cloud, mostly on prem, heavy SQL and Bash, no data engineering per se, but they are looking to create a data warehouse so possible growth in that direction, also a bit more laid back atmosphere and schedule compared to the first offer but not so much data engineering.

The questions I have are: 

* Is database engineer/administrator a promising career path? How does it compare to data engineer? 
* Which position sounds like a better start?
* What are your thoughts about no cloud usage? I have some experience setting up and using AWS resources on my own, but it won't be a part of either of the two jobs.
* Any other thoughts/suggesting would be appreciated!

Thanks!",1578631739,23,0,1,dataengineering
Deploying a simple pandas script to fetch and write csv,I am completing a project where I have datasets and ml model in a python package . I started with a sample of dataset to make a prototype now I have to modify it to take in as many csv without including it in the package. To solve this I am thinking to write a fetch dataset script to fetch data and use it to train but the problem is I don’t have clear idea of where to put these raw csv and have pandas clean and merge and write a clean csv so that the script with ml model can fetch the data when training. Where to store Data and deploy the Data cleaning and meeting script?,1578624003,9,0,1,dataengineering
Data Engineering and ITIL practices and procedures,"Curious what has been others experience when it comes to dealing with the Administrative/ITIL tasks usually associated with the IT department. I'm talking about ServiceNow, Change Control Boards, and IT project managment. Any advice or experiences are welcome.",1578613689,1,0,1,dataengineering
How has California Consumer Privacy Act (CCPA) affected your data processing?,"From my personal experience, I am having interesting dilemma, especially since my team have myriad data stores and pipelines. The details on ""do not sell"" aspect also leaves some grey area.

For example, we need to somehow redact the PII data while keeping the raw data in house. It is not easy if the data is later needed needed to pivot to acquire non-PII data.

What are you guys/gals' experience?",1578600164,2,0,1,dataengineering
Microsoft Data engineer certification,[removed],1578592779,0,0,1,dataengineering
GameAnalytics + Imply Meet-up in London - January 15th,"Hey Engineers! If you’re based in London, come join us at our Apache Druid meetup with Imply next Wednesday.  

We’ll be sharing a few key lessons we learned from migrating our backend systems to Apache Druid. And, there will be 🍕 and 🥤.

You can find all the details here: https://www.meetup.com/Apache-Druid-London/events/267380924",1578583374,0,0,1,dataengineering
Automation testing of data pipeline,"Hello DataGeeks,

I have been asked to do automation testing of below data pipeline. can somebody suggest/direct me how i can achieve end to end automation testing of this pipeline. 

*I'm looking for framework, tools which i can used.*

**Oracle DW -&gt; Apache Nifi -&gt; AWS S3 (landing area) -&gt; \[Using AWS Lambda step function &amp; cloud watch\] -&gt;  To store data into AWS Redshift -&gt; Power BI**",1578569436,4,0,1,dataengineering
Pipeline to the Cloud – Streaming On-Premises Data for Cloud Analytics,,1578561752,0,0,1,dataengineering
The Power Of Streaming ETL — Flatten JSON With ksqlDB,,1578513796,0,0,1,dataengineering
Best practice for loading large CSVs into database?,[removed],1578510462,9,0,1,dataengineering
Analysis of which data engineer tech skills are most in demand,"I scraped three job listing sites to find which data engineer tech skills are most in demand. I wrote about the results in [Towards Data Science](https://towardsdatascience.com/most-in-demand-tech-skills-for-data-engineers-58f4c1ca25ab?source=friends_link&amp;sk=718920a6a6907224a009600c1720b020). Hope you find it helpful!

Constructive feedback appreciated!",1578507992,9,0,1,dataengineering
Should I leave my job because we use a GUI-based ETL tool?,"Im about to be the ""data guy"" on my team but executives are incredibly insistent on using a GUI-based ETL tool (Knime, really similar to Alteryx). Should I leave my company for a company that utilizes more python, given that a majority of the industry is going in that direction. Or more so, am I missing out if Im forced to used an GUI based tool opposed to programming?",1578504317,20,0,1,dataengineering
Ant suggestion to my Data Architecture?,"First of all, I love this subreddit. Being a data engineer myself, I find every post interesting, and the expertise of some people answering question is outstanding.

So, I am a (Lead) Data Engineer in a small company. I also work as a data guy for a side-project and have my personal (side-)side-project myself.

Long story short, I am building something very similar to [newsapi.org](https://newsapi.org) \-- a news API that basically allows you to query the news. Why am I doing a clone of it? Well:

1. Because I want to
2. It is quite of a challenge
3. I think I know how to build it
4. I want to make the price for my product 5 times cheaper comparing newsapi

So, in this post I will try to explain my outline of the architecture and I hope that someone will be able to advise me something (because I am not that experienced in the tech stack that I am going to use for this product).

DATA FLOW OUTLINE (no precise tech stuff here)

Most of the news publishers have RSS/Atom feeds (something similar to an API if you do not know what it is) to give a programatical access to the latest published news.

By checking each RSS once in a while you can see new records (here's [the one of the NY Times](https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml)). Therefore, by collecting all the endpoints for all the news providers' feeds that I need, I will be able to collect my data. 

Each article itself (to simplify) is title, published date, author, description, etc.

Let's say I have the place where I store all the articles. After that, I would have to query that with my API. 

I plan to store only up to 7 days of recent data in the Elasticsearch.

&amp;#x200B;

CHOSEN TECH STACK

Now, I will describe how I see it being implemented. 

1. Airflow to schedule the process of obtaining new data. 
2. Postgres to store ""operational data"" -- info about feeds, endpoints, last queried time, how many new articles, etc
3. AWS Lambda (serverless function) to read the RSS feed of each news publisher (feedparser package)
4. Elasticsearch to store the article data for the API
5. AWS Lambda + Flask  to deploy API 
6. RapidAPI (an API marketplace) to deliver my solution to the end users
7. EXTRA. S3 + AWS Athena to store raw files and query them if needed

Airflow will schedule a batch (let's say of 100) of feeds to read. It will send 100 asynchronous calls of Lambda function. Each Lambda function will return all the articles that are now in the feed. 

Then, all of those will be deduplicated with those that my solution have seen already. This deduplication will be run against the data in Postgres (not in the Elasticsearch). 

After that, all new records will be inserted into my Elasticsearch cluster. In addition, a CSV file will be moved into the S3 bucket (plus I will configure Athena to query those CSV files if needed). 

My serverless API will make hits to the Elasticsearch. 

There is definitely many things I have looped but I hope you got the idea.

**Any suggestions/questions are more than welcome.** 

I write pretty much everything about this project on my Medium account. For example, here is a [small one about RSS feeds and how to read](https://towardsdatascience.com/collecting-news-articles-through-rss-atom-feeds-using-python-7d9a65b06f70) them with Python posted on Towards Data Science.

Also, if you like you could support me by subscribing to the [Product Hunt prelaunch page](https://www.producthunt.com/upcoming/newscatcher).

If you would like to participate or become a beta tester, you are welcome!

Happy New Year everyone and I wish you all the Best!",1578485339,0,0,1,dataengineering
What are some current open problems in data engineering?,[removed],1578485174,3,0,1,dataengineering
How to prepare for a DE interview?,"Here's what my schedule is for a general DE interview, not specifically for any company

1. Leetcode easy/medium Algorithm questions
2. Leetcode all level SQL questions
3. Reading research papers on DB architecture, spark, data streaming etc., anything related to DE
4. Occasionally doing practical of DE, like playing with dataset on different tools
5. Writing blog around DE practical stuff once in a while 

Is there something I'm missing or should focus on more? Please guide",1578471469,21,0,1,dataengineering
Learnings in 2019 and learning wishlist for 2020,"Fellow Data Engineers,

Now that 2019 is over and a new decade is beginning, what trends, technologies and tools did you observe and use in 2019 that have come out of the closet and have gained/gaining mainstream adoption?

What big things do you anticipate will make the news in 2020?

I personally worked on Spark, AWS, Snowflake etc last year. I want to learn a few new things and have started learning about dockers a bit. Would like to know about what you all have worked on and learnt in 2019 or willing to learn this year.",1578419764,8,0,1,dataengineering
What do you call a group of Data Scientists?,"A murder of crows

A caravan of camels

A business of ferrets

A(n) \_\_\_\_\_\_\_\_ of data scientists?

Vote here to decide! [http://allourideas.org/counter\_for\_data\_scientists](http://allourideas.org/counter_for_data_scientists)

Vote multiple times, it is more fun that way. I'm personally campaigning for *n.*

Credit to this tweet for the discourse: [https://twitter.com/chrisalbon/status/1214384871491035136](https://twitter.com/chrisalbon/status/1214384871491035136)",1578411844,7,0,1,dataengineering
The next 2 years for data engineers,"I've recently absorbed a DBA function into my data engineering team.   
I accepted this because I see a movement from a traditional DBA role to a more data engineering role and I've read a number of blog posts that backed me up on this. That with less managed servers and a move to the cloud, DBAs are moving to more data analyst/data engineer etc responsibilities. 

My question to the hive, where do you see Data Engineering roles in two years?

I personally think there are more specific roles within data engineering to specialise in. Information managers/ data modellers etc.",1578406967,57,0,1,dataengineering
An interview about how the Debezium framework simplifies implementing change data capture for all of your database engines,,1578319082,0,0,1,dataengineering
What are you go to technical sources to keep up to date?,Do you have any journals or particular websites you use to keep up to date?,1578297929,1,0,1,dataengineering
"How to generate HTML documentation for an existing database (Oracle, MS SQL Server, MySQL, PostgreSQL, Azure Database, Amazon Redshift, SQLite, Firebird)",,1578230874,0,0,1,dataengineering
Udacity Data Engineer Nanodegree Review 2020,,1578213807,0,0,1,dataengineering
Largest set of data you ever worked on ?,"Folks, I am  trying to understand the sizing aspect of data based on what everyone has encountered here in real life scenario , expecting valid answers , some background on challenges with type and frequency of data will also be insightful.",1578190261,23,0,1,dataengineering
Good examples on how to build Data Pipelines?,"Hello, I am an aspiring data engineer on my company (we only recently are focusing on our data value) and I'm a little bit lost. 

I am pretty familiar with the essencial tools such as Python, SQL, but I wanted some introductory material on the processes of building a data pipeline. Not necesseraly the frameworks but examples and some explaining on each step. 

Do you guys have any material like that? Thanks in advance!",1578177440,13,0,1,dataengineering
Data engineering and python?,"I’m a software engineer. I’ve spent 2 years working for an infrastructure team(ie we were building a framework that other web backend developers could use while writing service code). Overall, i enjoyed building/maintaining the infrastructure (for example using redis, python-rq, activemq). 

I took a deep dive into MachineLearning recently. After a month, I realised that i’d rather not spend my time dwelling in statistics and math to understand the algorithm in and out. I just don’t think that’s my niche. That’s when i encountered “data engineering”.

I like the idea of building architecture and pipelines to enable users(other developers/data scientists/analysts etc) to interact with data. I may be getting ahead of myself, but this feels right to me. 

Oh, also, I’m sort of a python enthusiast, so i’m curious as to what role python plays in this field. Does it have a future? Can i be a python data engineer?

Thanks!",1578161081,23,0,19,dataengineering
How can I learn enough Software Engineering?,,1578160883,0,0,1,dataengineering
Data Engineering career advice,"Hi all,

A little bit of context, I've been at my current job (Data Engineer) for around 2 and a half years and am looking to move on as I feel like I'm stagnating there and want to get exposure to some more exciting/up-to-date technologies.

The problem I am having is that other Data Engineer roles want somebody with however many years experience in many things that I have not used before e.g. Spark, Hadoop, Airflow, Pandas, commercial Python experience, etc.

My current role involves heavy use of ETL tools (Talend) and SQL Server for data warehouse development.

I have had some limited exposure to AWS (S3, Redshift, Athena).

I did previously learn Python in my own time and am currently doing another course on Udemy but, without using it at my current role, I'm struggling to retain what I'm learning.

Is there anything else I could do to better position myself in looking for a new DE role? Should I consider dropping down to a junior position in order to get the sort of exposure I would like?

Any suggestions would be much appreciated!",1578073635,30,0,20,dataengineering
Top Events for Data Professionals in 2020,,1578051214,0,0,1,dataengineering
What are some good conferences you recommend for data engineering?,"I'm in an entry level data engineer role, primarily using Hadoop, spark scala, and oozie tools. Looking to stay relevant and learn some new things. What do you recommend?",1578026795,38,0,20,dataengineering
How do you create self healing data pipelines?,"I was asked this question in an interview. At my current work place, we see platform errors and we rollback after certain time and try three times before calling it a failure. However, I believe there are better ways to deal with this and I wonder how they look like..",1577983933,3,0,1,dataengineering
An Insight Into How Google Services Manage Petabyte-Exabyte Scale Data,,1577981325,0,0,1,dataengineering
"Seasoned DEs, what would you say are the most important technical skills to go beyond the basics and what are some good resources/books/roadmap to take one to the next level?",,1577979064,3,0,1,dataengineering
What are some good portfolio project suggestions for aspiring Data Engineers?,,1577976945,12,0,1,dataengineering
Is there any data cleanup program with the GUI/ease of use of power BI but uses a more robust language? (e.g. python),"Hello all,

Disclaimer: I'm an Excel guy, not a data engineer, so this is a shot in the dark. We're a small, not-so-techy company where most of our pipelines need to be ingested from CSVs and SQL databases.

I've recently learned about Power BI and like many ""excel guys"", it's replaced clunky macros for me.  I love how easy it is to use, how it ""records"" your actions like a pipeline, and how I can switch back and forth to editing it using the advanced editor.  

However, what I don't like (at least from what I know) about power query is that:

- It's basically an end user service tool and I have to do everything on my computer.
- M language is super weird, doesn't even look like VB or excel much.  
- It can't really export to anything, I can't even figure out how to use it as a transformation tool and push the output up to an SQL server.

I'm on the lookout for some more robust tools, and while I'm trying to learn python (medium proficiency in pandas), I don't think I'm super ready to move to an all-code stack.  I REALLY love how intuitive the Power Query interface is, except for the aforementioned annoyances.  Is there a program which you guys could point me to that still maintains some semblance of easy GUI?

- Combine, sanitize, merge and transform files, mostly from csvs, SQL, web, or excel files
- can be scheduled
- outputs can be into CSV or pushed into a simple DB
- outputs or errors can be logged
- etc.

Thank you all!",1577954273,6,0,1,dataengineering
Fast IPv4 to Host Lookups with ClickHouse and PostgreSQL,,1577946171,0,0,1,dataengineering
why do you think most of the companies are moving away from Hadoop?,"I read articles which say Hadoop is dying and all but I am not sure why few companies are moving towards non hadoop solutions. On the top of my head I can say that it eliminates the pain of maintenance.

What are the other reasons?",1577932540,19,0,1,dataengineering
Is Data Engineering a very heavy configuration role?,I'm not sure how to best word it but does DE involve more time spent setting things up as opposed to writing logic?,1577902702,9,0,1,dataengineering
Data scientists who switched to data engineering: What compelled you to make the switch? Do you like your current work better?,,1577834870,0,0,1,dataengineering
Data Science Roadmap: Become Fluent in 8 Stages,,1577803060,0,0,1,dataengineering
An interview with a DataDog engineer about how they build reliable and highly available systems for processing timeseries data in real time and at massive scale,,1577744962,0,0,1,dataengineering
Data pipeline discussion thread (Tools used at your workplace).,"What all tools do you use to design data pipelines? (Hadoop/non hadoop solutions).

1. I work on building enterprise data lakes and we use kafka,sqoop for ingestions, Nifi for transformations, Hive for storage and CA workload automation client for scheduling.
2. What is your tool kit in the order of Ingestion, transformation/processing, storage, visualization?

I hope this helps other newbies.",1577738757,23,0,1,dataengineering
What is the best infrastructure with this type of hardware?,,1577736562,0,0,1,dataengineering
Data Engineer job with Data Warehousing for Business Intelligence,[removed],1577655359,0,0,1,dataengineering
Data Science Roadmap: Become Fluent in 8 Stages,,1577629743,0,0,1,dataengineering
Looking for sample projects/problems for a data engineer role,"Looking for sample projects/problems for a data engineer role (Python + Postgres). 

For example the challenge could be about extraction, transformation and load of data and running sql queries on top to answer business questions. 

: )",1577525074,15,0,1,dataengineering
What to focus in Python for Data Engineer Interviews at FAANG,"Hello All, i am currently working as a Data Engineer(previously ETL Developer)  working in a small company and i am prepping to get a break in  big companies like FAANG. I have 6 YOE in Data Warehousing. My skill set is (sql, ETL Informatica, Basic Python+Pandas,GIT, GCP and Airflow). I understand that most interviews are based on sql+python+data modeling. For Python, I am practicing on Leet Code(easy and medium), I mostly focus on string manipulation. What other areas in python should i focus to get through the interview ? I don't have experience working on huge volume of data, is that going to be an obstacle ?",1577480052,14,0,1,dataengineering
Data Engineer job with Data Warehousing for Business Intelligence,[removed],1577447781,0,0,1,dataengineering
Basics of big data,,1577422368,0,0,1,dataengineering
Swagger Documentation tool for Data Dictionaries?,[removed],1577391738,0,0,1,dataengineering
Data Science Pulse,"I am planning to launch this website to make a difference in Data Science field which is crowded by ""how-to"" blogs. My main problem with the ""how-to"" blogs is that they just assume that everyone knows what Data Science is. And people just want to implement Data Science in their business. I come to understand that this is not the case always.

My objective with this website is to clear up, understand and uncover the underlying patterns within Data Science field so that people can come up with objective reasons on WHY they need Data Science.

I have created a site and a landing page with the philosophy behind the idea. Please see below for the link.

 [https://datasciencepulse.com/](https://datasciencepulse.com/) 

I am looking for a healthy discussion regarding this philosophy within the field.

I know this is Data Engineering space but nonetheless I am of opinion that little bit Data Science doesn't hurt as well :)",1577261548,9,0,1,dataengineering
How important are Competitive Programming skills when you are applying for position like that of Data Engineer?," Assuming I'm trying to apply for position similar to DE at companies like Google, Facebook, Amazon, How important is the competitive programming skillset?

I'm well aware of different Algorithmic stuff like Time/Space complexity and all different algorithms, but I'm not really into cracking competitive questions. I know this is very important for those Big 3, but given the role I'm interested in, will this be a obstacle for me?

I prefer reading research papers like those of Dremel, implementing different architectures with some DS stuff etc. Should I go completely deep into this or I'll have to do competitive programming to have any chance at cracking these companies?

Really confused here, what to focus on really.",1577204732,2,0,1,dataengineering
An episode about building Materialize for interactive analytics on continuously updated streams of data,,1577145651,0,0,1,dataengineering
Lambda for ETLs,"Hi ladies and gents, happy holidays and all that stuff!

Does anyone have any experience of having used lambdas for their ETLs? We have some python jobs which tigger from a cron on EC2 and it feels like we have outgrown the set up and are running into issues, mainly with insufficient logging, deployment risks, downstream dependency visibility.

Although not having used it in production before my preference is to use airflow with perhaps Data-dog, but as most of the product stack is on AWS our engineering team want to move the data engineering stuff closer to them and have strongly suggested using Lambda.

An example use case would be google big query web events, 2/3m records per day in a single extraction. Also every 15 min extractions from a mongodb instance. 
Not everything is sourced from within S3, however our DW itself is a Postgres instance in Amazon RDS.

If anyone’s tried this and hit any snags, or found it fits the bill please let me know I’d appreciate it! 

Thanks",1577128581,6,0,1,dataengineering
Experience with ETL tools?,"I am doing research on ETL tools and want to understand better when each tool is best. For instance, in what situations is Prefect better than Airflow or Ascend.io better than Argo? I am also interested in Dagster. Do any of you use these tools in your companies and would be willing to speak or chat with me? Thanks for considering.",1577115357,23,0,1,dataengineering
DevOpsRemote.work: Find DevOps Remote Jobs,,1577111961,0,0,1,dataengineering
does anyone have tips on becoming a data engineer?,[removed],1577086696,2,0,1,dataengineering
Analyse Facebook Ads using Google BigQuery Automation,,1577082709,0,0,1,dataengineering
Data Engineering Blog Ideation,"Hi all,

I am Ajay. I have extensive experience in data engineering and distributed systems engineering in cloud. I am planning to start a blog on the same with in-depth articles with theory and practical implementations.

Following are my initial topic list which I have to write about. 

1. Fault Tolerance Explained Mega Series
    - Introduction to Fault Tolerance 
    - Fault Tolerance in Distributed Systems     
    - Buiding Fault Tolerant Architectures in AWS 
2. Apache Spark Explained Mega Series     
    - Overview of Apache Spark     
    - RDD Explained     
    - Spark SQL Explained      
    - A Unified Engine  


These two topic sets would comprise nearly 15 to 20 articles explaining each aspect in as much depth as possible without getting too much technical.

My question is simple. Is there a demand for such blogs and such in-depth articles considering it would take considerable amount of time and effort to write each of them?

Will you be interested in such a series of data engineering topics?

Please let me know if you guys really like a good in-depth blog about data engineering.

Thanks and Regards
Ajay",1577040130,19,0,1,dataengineering
How's 'Data Engineering with GCP' Coursera certification?,"I snubbed Udacity nano degree certification due this detailed review thread.
https://amp.reddit.com/r/dataengineering/comments/c11ut4/warning_stay_far_far_away_from_udacitys_data/

Next alternative for a beginners is 
https://www.coursera.org/specializations/gcp-data-engineering

Before starting on it, want to get answers to following questions : 
Does this certification give a beginner good Data engineering foundation skills and introduce to d2d work challenges ?
Is it valued in industry ?",1577020615,7,0,1,dataengineering
Building Data Pre-processing + ML Pipeline for senior year project and needed some advice.,[removed],1577012936,0,0,1,dataengineering
Udacity Data Streaming vs Data Engineer Nanodegrees?,"I’m currently a data analyst (mostly excel/tableau/SQL/some R) after doing a boot camp and am interested in data engineering. I am considering the Udacity Nanodegrees but am not sure which one would be best to move into data engineering. 

Thoughts?",1576989447,1,0,1,dataengineering
Career suggestions needed,"I am currently working as a Data modeler overseeing data modeling, Metadata management and database design. What positions should I pursue in the data engineering space? What role can I fit into?
Please suggest.",1576892430,1,0,1,dataengineering
Apache Airflow Summit 2020 in London &amp; North America | Attendees Survey,"We (Apache Airflow PMC &amp; Committers) are planning to organize 2 **Apache Airflow** summits in 2020, one in North America, one in Europe.

These summits will be community events, and an opportunity to bring together users and contributors of Apache Airflow, and collaborate on the development of the project.

We would like to tailor these summits based on what the community wants and expects from it.

So we created a survey as a means to collect this data. If you have 5 minutes, please fill this survey: [https://forms.gle/qDi52z9TY9pT9Lsm6](https://forms.gle/qDi52z9TY9pT9Lsm6)

This is your chance to voice your opinion :) 

This will help us make some key decisions on how we organize it.

#",1576881376,25,0,1,dataengineering
Kubernetes for data-related microservices,"I wanted to know how/if people use Kubernetes in their organizations, particularly as it pertains to data related applications (models builders/predictors, ETL, microservices). A little background, my user base is entirely internal and no more than 100 users. I am trying to determine if Kubernetes is even worth the effort and energy for a one person data engineering team with a small customer base. I currently use a combination of ECS and manually deployed on prem docker containers stung together with traefik (its kind of janky but it does work).

My big question is, when does the conversion to a Kubernetes platform make sense?",1576851396,5,0,1,dataengineering
ETL: SSIS/Stored Procedures vs Python,"Hi all,

What are the advantages of using Python for ETL compared to using a combination of SSIS/Stored Procedures? I'm asking this because I am learning Python for ETL and data analysis and I am interested in your arguments.",1576843957,23,0,1,dataengineering
Google Cloud Platform on Coursera worth it?,"I was wondering if anyone had taken the GPC specialization (with certificate) on Coursera and was willing to share their opinion? It's $44 a month just to follow and you'll have to buy credits if you don't have the free trial anymore.

How long did it take you to complete and did you think it was worth the investment (time and money)?",1576841608,6,0,1,dataengineering
Pyspark rdd to SQLite,"Hello all, How do I insert  rdd into SQLite database without collecting it as rows and inserting row by row ? Thank you",1576804466,3,0,1,dataengineering
Building a Twitter Sentiment-Analysis App Using Streamlit,,1576796050,0,0,1,dataengineering
How much is your current salary as a data engineer and where are you located?,,1576793756,116,0,1,dataengineering
ET(L) with Python,"I was wondering if anyone has had much luck doing the L piece of the ETL triangle with Python. I handle millions or hundreds of millions of rows at a time and Python in my experience loads VERY slow. I specifically use MSSQL as my database. I've read a fair bit on the subject and I've tried several ORMS and drivers within Python. I've chunked out the data and other things and achieved some success but for me to really justify using Python over C# I'd need much better results. So I guess my real question, has anyone gotten decent load speeds for 100 million rows using Python?",1576776982,24,0,1,dataengineering
Query AWS Redshift like a Boss!,,1576759480,0,0,1,dataengineering
What IDE for ETL with Python?,"Hi all,

What IDE do you use for your ETL with Python? I have heard you could use jupyter notebook for ETL or VSCode/Pycharm? What are the pro's and con's for your choice?",1576702409,42,0,1,dataengineering
Header-detail partition scheme in Hive,"Hello,

I need some guidance on how to model the partitions of headers and details of invoices using Hive.

Header tables I plan to store partitioned by year, month and day (e.g., year=2019/month=12/day=2)

Detail table:

Around 200 million records per day) Have invoice number (unique) to relate to headers, but no dates included in the detail per se

I am thinking about 2 options for detail table:

*Partitioning also by year, month, day. This needs a join to be performed in the data ingestion process, before storing. In this scenario, joins between header and details could be penalized because the join is based on the invoice number
*Partitioning by a fixed amount of characters of the invoice number (unique). (e.g., for invoice number 123456789 a partition would be ""12345""). I think this approach could be better because invoice numbers are sequential and thus ""tightly"" mapped with the dates (partition scheme of header table)
*I also need to manage corner (dirty) cases, like invoice details with no valid invoice id: I'm thinking about a dedicated partition for these cases

Does somebody have some suggestions or experience with a related scenario?

Thanks a lot",1576698185,2,0,1,dataengineering
"Has anybody deployed a Streamlit ""production"" application for your project",Was hoping to use Streamlit for our project and was wondering if anyone is using it for any of their prod use cases or potentially considering using it?,1576697961,4,0,1,dataengineering
ElasticSearch + kibana as a BI stack?,"I work at a company where we're looking to switch from mongodb (which was very good for prototyping purposes) to a more adapted data store.

A few key points:
- we need a graphical exploration tool (metabase / redash / superset)
- the queries will generally run on data in the 10 million rows range - I guess you could extend it to 100 million for a bit of runway
- even though we used mongodb, most of the data was validated against a jsonschema so it's fairly well structured 

Someone suggested using ElasticSearch + kibana. 

Although I can intuitively say that it doesn't fit our use case since: 
- we have structured data (and using an indexing engine just doesn't feel right)
- SQL and tools in that ecosystem seem to just be the norm.

But it still looks like it could work. 

So I'm wondering: 
1. why would it be a bad idea?
2. why don't people ever compare ES + kibana to the likes of metabase and redash?",1576691274,2,0,1,dataengineering
Huge join or little statement full of nulls plus a lot of updates for creating fact tables?,"Hello. Imagine I want to create a fact table for orders that includes the dimensions/metrics product, domain, user\_id, product\_id,revenue and profit. Nowadays, my company has an ETL which is like the following:

&amp;#x200B;

^(INSERT INTO dwh.orders)",1576681042,2,0,1,dataengineering
How do you tackle redundancy in data collection?,"Many times in my jobs I did the same thing:

* get data from API using python
* parse the data and save it directly to some SQL/NoSQL DB
* query data and use it for analysis

For redundancy reason I want to run collection multiple times on different servers (sometimes data is time sensitive as well - not available at a later date). And as such I need to handle duplicates in data, since multiple servers can write to DB at the same time. 

1. I wonder what is the best solution/architecture for the case (minus just using unique indices or other mechanism of the DB)? 
2. Additionally, could you advise on the architecture if I would start a new small project (e.g. couple of data streams, do some transformation and later show it on a dashboard) that can be scalable as the project goes one without breaking a bank?",1576658656,2,0,1,dataengineering
Anyone have any good reference or thoughts around adding public data to enhance your source data ?,Let’s say I have a source system that has data on homes. I then use the home address to get long / lat data to tie to large public data around demographics. That’s an example. Anyone else do this and have success and care to share how they used public Data to compliment source application data that lives in their warehouse to aid insights ?,1576653964,7,0,1,dataengineering
How to Analyse Facebook Ads using Google BigQuery Automation,,1576647356,0,0,1,dataengineering
An interviewer asked me how do I de-duplicate records in Hive.,"I told them that hql is similar to sql and you can write distinct, unique keywords. They mocked me and then at the end I asked them how they do it in their company..they said they use snowflake instead of Hive and it comes with distinct and unique keywords.

It killed me.",1576624165,10,0,1,dataengineering
Honest reviews of Snowflake?,"It seems like there is a great furor about snowflake, but I see very little potential benefit to it apart from price and it being fully managed (which is barely a plus) and there are some serious downsides regarding optimization options.

Can someone give me an honest review of snowflake, including the negative aspects?",1576622716,33,0,1,dataengineering
Save attachments from emails,"Hi guys, I know that maybe this topic shouldn't be here, but I thought that maybe someone here has face the same problem: to save automatically attachment from emails. There are different solutions like: macros, python or... Any suggestion? 

Thanks!",1576621698,1,0,1,dataengineering
Can we talk about Dremio,"I have been fighting with backing up and restoring a Dremio server for nearly a month now. Went to the community page and asked a question; got auto-moderated for some reason.  I have just found this application to be now where near worth the hassle from a data engineering perspective. It feels like the documentation is sorely lacking and their ""support"" is really just one person kind of answering questions.   


I am curious if anyone has experienced the same. Has it worked for you? If not, what did you move to?   


Feel free to down vote me for just venting but where else are we supposed to discuss vendor selection if not here?",1576612677,3,0,1,dataengineering
Best Practice,What is best practice for storing data source connection details in python scripts for data extraction?,1576605362,7,0,1,dataengineering
Data team in a medium sized organization,"Hello guys, I'm new on this sub, which look awesome! I'm coming with a problem :)

We have a brand new data team in my company and we don't know where to put the responsibility cursor (and what would be the best in term of scalability).

Let's say we have a marketplace application with 5 IT teams: ordering, product management, invoicing, logistic &amp; data. The data team is mainly needed to analyze user workflow on the product, working with segment &amp; amplitude - for questions like: ""did the user used this new feature?"", but also analyzing broader metrics like number of orders, total amount of products etc..., working with bigquery &amp; looker - for questions like: ""what is our GMV this month?"". 

Today, we don't know where to draw the line:
* Should the data team be a stakeholder of the other teams? Meaning should they express a need for a specific data and it should pass through grooming in the correct team (invoicing for credit note, ...). Every communication being explicitly put into contracts (API versioning, JSON schema for event listening) with a deprecation warning when something changes.
* Should we have a ""data guy"" in each team having the knowledge of what the data needs, how it works. Basically knowing when we should warn the data team of a change and specify when each team should add a tracking on a new feature.

I would gladly hear how it works in your organizations if you happen to have similar issues :)",1576577491,11,0,1,dataengineering
"An interview about how the Marquez platform for metadata management powers data lineage tracking, data discovery, and health reporting at WeWork",,1576521444,0,0,1,dataengineering
Data Engineer job with AWS Big Data Certification,"Hi,

Can be possible to land a Data Engineer job by passing AWS  Solutions Architect and Big Data Certification plus basic Python and SQL?

&amp;#x200B;

Note: Have working experience as systems admin/eng.",1576518386,8,0,1,dataengineering
Security Documentation for Data Products,"I work as a data engineer -- creating ETL pipelines for marketing data (e.g. Google Ads, Bing Ads, etc). Are there best practices for documenting the security of our systems that would make it easier for our clients to understand them? For instance, a client says, ""what happens in the case of a security breach?"" There are many very pointed questions that I can answer one-off but I'm curious if there's an industry standard for documenting security.

We use Google Cloud Platform for all of our ETL and follow their best practices. Are there external-facing documents that we would put together for clients? If so, what are they called? Any templates I should follow?",1576516667,2,0,1,dataengineering
Anyone use their skills here to create a side hussle ?,"I’m a jr data engineer and would love to get a side hussle going with the skills I use at work ( extract data from apis , store in data lake and create denorm tables in data warehouse )",1576471477,10,0,1,dataengineering
An Insight Into How YouTube Stores &amp; Manages Petabytes Of Data Every Single Day,,1576428862,2,0,1,dataengineering
How to realistically prepare for a data engineer interview?,"What do you guys do to prepare for an interview? Practice writing SQL? Spark? Do System Design? Coding challenges? Put together a pipeline? Read documentations? What can you do?

Any resourceses that you recommend?",1576363861,7,0,1,dataengineering
Looking for guidance on pulling data from an API,"I am connecting to an API that holds a national listing of healthcare providers. The documentation is found here: [https://npiregistry.cms.hhs.gov/registry/help-api](https://npiregistry.cms.hhs.gov/registry/help-api) 

I am able to connect and pull in ***some*** data so that's a start, but say I wanted to pull in everything for the state of Wyoming, how could I do so? The API documentation says: 

"" An API query will return a maximum of 200 results per request. The Skip field in the API will let you skip up to 1000 records. By using these two fields with your search criteria, you can get up to a maximum of 1,200 records over six requests. ""

I'm using Python and my current API query looks something like this:

import requests

response = requests.get('[https://npiregistry.cms.hhs.gov/api/city=&amp;state=WY&amp;postal\_code=&amp;country\_code=US&amp;limit=50&amp;skip=&amp;pretty=true&amp;version=2.1](https://npiregistry.cms.hhs.gov/api/?city=&amp;state=&amp;postal_code=30301&amp;country_code=US&amp;limit=50&amp;skip=&amp;pretty=true&amp;version=2.1)')

&amp;#x200B;

But it requires a zip code to run. To get everything in Wyoming should I just make a list of all Wyoming zip codes and loop through it? How do I get around the limits they place on requests?",1576358503,3,0,1,dataengineering
Data versioning in your data science projects,"Hey people! How do you version your data used for your Data Science projects? Let's say you had Model M0.1 released June 2019 using customers.csv and sales.csv ; You had a 3-month cadence for your release.  
Similarly there was another release made in 09-2019., only this time both csvs have been updated.

Now you are trying to rerun your predictions for the old model but data has been updated. How do you version your data for this? Do you keep copies of data each time a release was made? Or use some tools to better organize this? Any thought? Suggestions? Complaints.",1576275196,22,0,1,dataengineering
Great Talk about the State of Data Pipeline Testing Today.,[removed],1576256229,0,0,1,dataengineering
AzureDevOps,"Hey guys,

Month 2 in my data engineering role, trying to get my head around git and devops, has anyone had experience in deploying databricks into uat / prod from Dev? What was your experience if so?",1576254650,0,0,1,dataengineering
SQL — From Intermediate to Superhero,,1576223105,0,0,1,dataengineering
Side project,Any advises/ tips to start a data engineered related side project that can build up my profile?,1576222864,5,0,1,dataengineering
Processing large .gz files efficiently,"Hi all

As a small side project at work I've found a fun challenge that I thought I might share with you for input. We receive a usb-drive monthly with 2x2tb .gz files with financial data. Each of the two files holds a text file of around 25 tb when unpacked with each row consisting of some delimited base information (date, time, instrument, exchange etc.) and then some price data in a variable number of columns.

The goal for this data is twofold:

* Generate some easily queryable aggregate data (say one row per instrument/minute) - this dataset tends to be around 1/5000 size of the original.
* Store the full dataset in a more optimal way enabling new aggregates, recalculation with new logic, analysis on parts of the raw data etc.

I played around with some sample data a bit and on my local machine and I can load the data into my local Sql Server at around 250gb per hour (after it has been unzipped) but with only transformations being splitting into columns and converting data types from string to date, time, char etc. I'm sure if I did this on a server I could up that speed a few times since I'm limited by ram/cpu and not by disk speeds - however when doing the full size data set the initial processing would need to be on slower disks to unzip and split the files since I don't think we could easily get hold of 50tb NVMe disks. So it seems whatever approach I take on prem working with this sort of data will be unwieldy.

So what are good options that we might also be able to pay? Currently I'm thinking of doing something like:

* Upload compressed data into Azure Blob store (or data lake gen2) where the price for storage is a measly $1.5 per 100tb per month or something (+read/write costs).
* Use data factory (polybase enabled) to either uncompress, then move or to directly move the data into Azure Sql Datawarehouse ([here is an old example from 2016 where they load 1tb of data into Azure sql Datawarehouse in 15 minutes](https://docs.microsoft.com/en-gb/azure/data-factory/v1/data-factory-load-sql-data-warehouse)). (So at current prices around $30 per tb for preprocessing - possibly more before aggregates done etc.).
* Use the Datawarehouse to aggregate the data as needed - then export and either compress or put in an Azure Sql DB and take a backup - push this file back on premise for my analytical data stores. This export via either ADF or Databricks?
* Export raw data to data lake gen(2) but in one file per instrument per day. That way we have it accessible and can access parts of it without having to load the whole dataset. These smaller files should probably again be compressed - maybe parquet files?

As you might see I'm new to working with fairly big data and luckily this is another teams headache - but I'd like to practice a bit and I enjoy the challenge. So what are you pro's take on this? How would you approach it?

**TL:DR:** 25tb individual txt files into aggregates and raw data stored more optimally. How?",1576219944,24,0,1,dataengineering
Can we really do it all with Python?,"I'm a data scientist at a small company without a proper engineer. I've learned a whole new respect for y'all as I've inherited a lot of duties I never expected to deal with. I am trying to build the infrastructure we need, and I'm truly overwhelmed by all the tools, libraries and ecosystems out there. Help!!!

End-game everything needs to go to our Snowflake data warehouse. That seems easy enough with a little Python:   
[https://docs.snowflake.net/manuals/user-guide/python-connector.html](https://docs.snowflake.net/manuals/user-guide/python-connector.html)  


We need to ingest from a Postgres database, some JSON, and some CSVs. Our biggest table is about 70 million rows. Big, but nothing too crazy yet. I already wrote some R scripts that were able to do all this, but it took about 20 minutes to write \~500k records with \~50 fields to the warehouse. That was off my aging MacBook Air. It felt ""too slow"" for production so I started Googling away which only confused my more. 

We use tools like Segment and Funnel to ingest data from 3rd-party sources, but they lack connectors for everything, so some custom API scripts are inevitable. Again, my thoughts turn to Python. 

And I need something to run all these scripts for me. Sounds like Supervisor is out, and Airflow is in. Ok. I can adapt. 

So it seems to me this can all be handled with Python, from whatever crazy data source they throw at me, all the way up to Snowflake. But then I stop and ask: am I being too simple minded here? Are there tools that I should know about before I go reinventing wheels again?   


How do y'all handle it? If I'm being stupid, please for the love of God, tell me so.",1576216072,7,0,1,dataengineering
Data Engineers in a Data Science team,"Any Data engineers in a Data Science team?? How does your project look like?

What kind of tools and technologies do you use? 

How do you deploy your model?

How does your team version control?",1576205433,4,0,1,dataengineering
CDC with Big Data Tools,"Has anyone implemented CDC with (Nifi + Kafka)?
By the way, what alternatives are there when we want to ingest files ( with CDC) or data from relational databases?

Thanks",1576187153,0,0,1,dataengineering
what are the most important data engineering tools and technologies which you use at work?,"example:  I use kafka, Nifi, Hive, Sqoop a lot and I am looking to learn few more technologies and wondering where to start?",1576172354,23,0,1,dataengineering
Preprocessing infected cell images,"Hi, I'm having a lot of problems to preprocess cell images in order to make them different from being infected by virus or not. The thing is both class images are so similar, so I've thought on edit those images to clarify differences, but every tutorial (I don't know how to use a single library for images preprocessing, such as opencv) always difference on foreground and background or detect specific objects or colors. Here, my image is plain and the object is just a dot with no specific form or color (nor the rest of the image).

&amp;#x200B;

Do any of you know about a tutorial, a post or something similar that would be useful?",1576157069,0,0,1,dataengineering
No CS degree - disadvantaged for applying?,"I have a degree in business and am thinking of doing a masters in business analytics (data engineering concentration), but many of the data engineer positions on LinkedIn call for a computer science or related degree. Would the MSBA count?

Been coding in SQL for 5 years for various business groups and thinking of going more technical.",1576145626,24,0,1,dataengineering
Reference Data Management and Mapping - Open Source?,[removed],1576119035,0,0,1,dataengineering
I have $500 to Spend on DE Stuff Before Next Year. What do I Buy?,"Title

My job gives $500 each year to spend on personal development. So I can spend it on anything work related. What do I buy?",1576103274,4,0,1,dataengineering
Common Issues faced in Spark,,1576095594,0,0,1,dataengineering
How to master Java transformations in AWS Glue,,1576093584,0,0,1,dataengineering
Building Interactive Dashboards with Dash Bootstrap Components,,1576083116,0,0,1,dataengineering
Do you ever feel like quitting because fuck this is hard?,"I haven’t been doing this long. Still super junior but Jesus this is testing me some days. 

I’ve never been the type of person to want to pack up and just leave but some days I feel like the dumbest person in the room and I want to just go. 

I had a very distinct day last week where I finally felt everything rush over me and I felt buried in stress. That was the first day that I just wanted to cry and hand my badge into my manager. 

I’m not a bad employee. I’m not stupid. I know all of these things. I just wonder if I’m smart enough for this shit. This is fucking hard as hell and I’m struggling. 

I could use some words of wisdom or encouragement please.",1576079308,55,0,1,dataengineering
Where should I start looking for a remote data engineering job?,"I am based in Canada and does anyone had a good success here in getting a remote job which is based out of USA/CANADA?

I had few interviews with US companies but I didn't had enough luck.",1576019003,2,0,1,dataengineering
Any suggestions on ETL/Data Quality frameworks for managing 100+ pipelines?,"Hi all,

I work at a python/scala big data company where we use Airflow, AWS, EMR, Redshift, Snowflake, Graph databases like Neptune, transfer terabytes of data back and forth between clusters and S3, that sort of stuff. 

We're looking for a tool to observe data pipeline health checks for 100+ pipelines in the company, spanning multiple tools like the ones listed above. I want to see if stuff is on fire, missing files, changed schemas from our vendors (surprise!), so whenever stuff breaks our developers would be notified about it asap. 

Anyone using a technology that might fit this bill, that you would recommend? 

I was looking at streamsets, apache nifi, or writing some custom code to hook into our airflow jobs. I'm not sure what is the path of least resistance here... of course I would prefer to have open source software, but money isn't an issue. We would also like to be flexible with the tool, so not entirely graphical to the point of being restricted by the software.",1576004257,13,0,1,dataengineering
Would I save money migrating from Redshift to Snowflake?,"Hello. I have read about the autoscaling of Snowflake and it's something that could help my company to reduce the costs. Currently, we have 1 year reserved cluster with 10 nodes. The average and maximum CPU consumption within 3 days (almost every day the profile is very very similar) are the following:

https://preview.redd.it/r1nb3fcopu341.png?width=1681&amp;format=png&amp;auto=webp&amp;s=ca73fffde66b8d8f2c7dbb1338bc046e84f00f53

https://preview.redd.it/3e13i3copu341.png?width=1681&amp;format=png&amp;auto=webp&amp;s=6defe688e8cc4c13bc61e54efd62578ade80afc6

With that kind of plot, would be worth to migrate for saving money? I am doubting a lot because despite there are a lot of moments when the consumption is very low, there are others which is 100%. The storage is about 75% and increases very slowly, so that would be an easy calculation to do.

Currently we are not experiencing any kind of issues with Redshift, so it would be just for cost optimization. 

Thank you very much!",1576003536,3,0,1,dataengineering
Minimal WebUI for Prefect workflow management system.,I just found a really cool project on GitHub for [Prefect](https://www.prefect.io/). I love prefect but would love to monitor tasks like in Airflow. [https://github.com/roveo/praetor](https://github.com/roveo/praetor),1575985095,2,0,1,dataengineering
Datacamp Data Engineering Course worth it?,"Hi all,

I started working as a data engineer, from a data analyst recently. I'm reasonably proficient in SQL and R but have little knowledge of python and data architecture so I've been looking at courses. I noticed the datacamp one is onsale but cannot find a single review of it, has anyone completed the course? if not is there any other courses you guys recommend me take, the ones ive found are ridiculously expensive.",1575980998,0,0,1,dataengineering
"Pipedream, a free integration platform built for developers",,1575957217,1,0,1,dataengineering
"I got a new job as a DE , my first job coming from an analyst role. I really like what I am doing and learning but I feel like I’m moving at a slower pace than my manager and team lead would like and it really sucks I don’t know if I’ll get fired or If I am just being hard on myself","I’m not sure if I should look for jr roles or idk , I just wish I could move at a slower pace and feel more comfortable asking questions. I know this is more of an open ended comment so I’m sorry to vent but if anyone has any advice lmk. Even a grasp of what the first 6 months of a DE and reasonable expectations or any uplifting stories of success :)",1575951804,27,0,1,dataengineering
Which Framework to use to train model and predict online?,"I want to give the users the possibility to request predictions online and I'm evaluating with framework I should leverage for this.

The challenge is: **not only it should generate predictions on demand, but it should also train the model itself.**

For example, I have data for each city in a Country. Obviously I don't want to schedule a job that stores daily/weekly predictions for each city (which probably will never get called). Instead, I want to provide the code for a model and then train it on historical data and make the prediction based on the city selected by the user.

&amp;#x200B;

I was thinking to use Kubeflow, but I have never used it before and I'm not sure this is the right use-case. 

Do you have any frameworks/ approaches to suggest I can look into? 

&amp;#x200B;

Thank you!!",1575942801,2,0,1,dataengineering
200 OK! Error Handling in GraphQL,,1575937315,0,0,1,dataengineering
"""Data feed streamlining"" learning resources","Hey all

I have a data science internship interview coming up next week. I asked the recruiter what the role would entail, and he mostly mentioned a bunch of stats/ml stuff. Two things that stood out to me however were **(a) data feed streamlining** and **(b) database structure.** 

My background is very much in the ml/stats stuff, so I'm not really familiar with data streaming, etc. Does anyone have any idea what kinda questions I should expect, or how I should prepare? 

Thanks!",1575926860,2,0,1,dataengineering
An interview about how SnowflakeDB was built to provide a performant and flexible data platform for the cloud era,,1575907242,0,0,1,dataengineering
How do people typically document their data pipelines within an organization?,I'm curious how people document the overall data pipelines within an organization (if you even do / wish you did).,1575864584,4,0,1,dataengineering
Questions on Data Engineering docs,How do people typically document their data pipelines within an organization?,1575863736,9,0,1,dataengineering
"What is the difference between Kedro, Prefect and the newly released Metaflow?",[removed],1575849282,1,0,1,dataengineering
"what is a good resource (book, class, etc) to learn about distributed systems?","as a junior-level data engineer, my knowledge of distributed systems is very poor.  i have a tough time debugging issues when our hadoop or spark clusters go down or have issues scaling. i don't really know how to create a cluster, or take down nodes, allocate the right amount of resources for a spark job, or investigate issues.  


i'd say my python and sql are excellent and i can get around on unix.  


unfortunately, my team is unwilling to teach me. they'd rather just do it themselves.. which kind of sucks.. but i've decided to take it into my own hands and try to figure this out all myself. what is a good place to start?",1575837413,10,0,1,dataengineering
AWS pre:Invent and re:Invent takeaways,"What were your notable, data-related takeaways from re:Invent and the days leading up to it?",1575827018,5,0,1,dataengineering
"Master Data Management, how to match and merge records to unify your data",,1575818511,0,0,1,dataengineering
Is Udacity still offering monthly subscription on their data engineering course?,The current price for the full course (5 months) is $1800 which is insane I think. So I was hoping to finish it off in 2 months  and pay only for 2 months. But I am not able to find a monthly subscription. Did Udacity stop their monthly subscription feature?,1575799245,4,0,1,dataengineering
What software or program do you guys use to develop graphics of data flows?,That’s the question! I was at a conference the other day and saw some really nice data flow graphics. What’s the best way to do it?,1575796993,2,0,1,dataengineering
What different strategies do everyone here implement to build your CI/ CD pipelines to test your ETL scripts,,1575791996,9,0,1,dataengineering
What was your first step towards being a Database Engineer?,[removed],1575782974,0,0,1,dataengineering
"Since Spotify Wrapped came out, wanted to share an article they posted about their event delivery system. Really interesting stuff, apparently they ingest over 350TB of raw data every day",,1575757516,1,0,1,dataengineering
Data Channel IT Services, Data Channel Marketing reporting tools are used for Integrate your all marketing data under one platform. Data Channel provides Data aggregation tools which helps you to integrate your all Api under one roof. For more-  [https://datachannel-65.webself.net/](https://datachannel-65.webself.net/),1575716235,0,0,1,dataengineering
Want to switch career from system engineer to data engineer,"Hi Team,

Need advice about switching career from system engineer to data engineer.  I have a computer science degree and knowledge with Linux, basic AWS, Python, and SQL. 

How possible is to switch or do I need get experience as Software Engineer or developer first? 

Thank you in advanced !",1575667321,12,0,1,dataengineering
Advice on applying for data engineering jobs?,"I'm a data scientist that does a lot of data engineering work, and found myself more interested in the data engineering side.




How do you prepare for data engineering interviews?  Software engineers have leet code.  Data scientists also have leetcode and prob/stats/ml questions.",1575641263,16,0,1,dataengineering
"Is there any market or demand for datasets , like peer to peer. Any biz model to do this",So if I am a data engineer and I scrape some days or hit an api and then create a data set can I sell it or monetize that process ? Outside of being a data engineer for a corporate entity.,1575609438,5,0,1,dataengineering
Blog Article: The Most Underrated Python Packages,"[https://towardsdatascience.com/the-most-underrated-python-packages-e22bf6049b5e](https://towardsdatascience.com/the-most-underrated-python-packages-e22bf6049b5e)

Love me a good curated list of Python Libraries... [https://github.com/great-expectations/great\_expectations](https://github.com/great-expectations/great_expectations) Has to be my favorite  ;)",1575600309,3,0,1,dataengineering
Profiling the Airflow Scheduler,"Ash is working on making Airflow better for everyone, directly on the open-source project, reviewing and merging PRs, preparing releases, and lately working on [roadmap for Airflow 2.0](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+2.0)  \- one of the big goals is to “improve Scheduler performance and reliability”.

While he's been hacking on it, we convinced him to take a moment to post a progress report:  
[https://www.astronomer.io/blog/profiling-the-airflow-scheduler/](https://www.astronomer.io/blog/profiling-the-airflow-scheduler/)",1575583209,0,0,1,dataengineering
Profiling the Airflow Scheduler,,1575582778,0,0,1,dataengineering
Lost and Need Help with Data Engineering 101,"A bit of an introduction,

I am a solutions analyst in a marketing company with a degree in Business. I transitioned into the world of data through a retraining program and changed into a hybrid role where I do a bit of everything.

I want to start a data engineering project for me to ease my work and by extension the work of the team by creating a data stack that enables us to ingest data from a variety of platforms like fb ads, double click manager and some crm platforms, transform them to fit our analaytics structure and automate reports and dashboards.

Currently we have nothing in place apart from tools that we pay to use and export data to create our manual reports.

I am intermediate in Python and learning Javascript and my SQL is not the best and I feel this project is a way to derive value for myself and the company grow.

That being said, I don't have the right knowledge to start. I read things about airflow, azure data factory, using cloud functions and its all increasingly a blur to me.

I'm looking at the meltano stack right now but I am wondering if you guys have a better option. Currently I am throwing things but my architecture is shit to say the least (I feel)

I am using StitchData to load my data into BigQuery, then load it up using Google Data astudio for viz work, and now I feel that's less than ideal as I don't have transformation to the extent I need.

Am I at the right place ? Am I sticking myself into the wrong hole ? Is this too ambitious ?

Looking forward to the opinions of my betters here. Cheers !",1575562888,44,0,1,dataengineering
Career Advice: What should I do to advance my career towards DE and DS ?,"I've been working as QA analyst having automation expertise. For past couple of years, I've moved away from automation to ETL testing and database testing involving large scale enterprise datasets. Right now, I'm in talks with one of product company who develops Data Modeler products to join their team. Is it a good move as I'm not quite connecting Data Modelling with DE/DS ? Please advise.",1575536101,5,0,1,dataengineering
How Cloud-Based Solutions Are Transforming the Online Gaming Industry,,1575523708,0,0,1,dataengineering
Is there a way to protect my R code that runs on a AWS account owned by a client?,"I just joined a company that, for some weird unimaginable reason, needs to build an ETL pipeline inside an AWS account owned by a client. 
There's one part of the ETL pipeline that runs a code written in R. The problem is, this R code is very important part of our business, and our intelectual property. Our clients can't see this code. 
Is there any way to run this in their AWS environment without them having access to our code? R is not compilable, so we can't just deploy an executable file there. And we HAVE to run this in their environment, I suggested creating an API to run this in our AWS environment, but this is not an option.",1575508869,2,0,1,dataengineering
Must read blog posts recommendations?,"I know about a handful of engineering blogs that are super high quality and informative (Netflix, Airbnb). I'm mostly interested in your favourite individual blog post(s) by engineering teams related to data engineering.

Also while I'm looking for recommendations, are there any notable people / lists on Twitter in DE worth following?",1575490377,0,0,1,dataengineering
Looking for career advice,[removed],1575473263,0,0,1,dataengineering
Tutorials vs stack overflow for learning large concepts you have no familiarity with?,"I'm currently working on a personal project and have come to the point where I'd like to deploy my airflow ETL pipeline and orchestrate the docker containers using kubernetes.

I have little to no experience with kubernetes and deployment to production environments.

My two choices of how to figure it out are:

1. Google/stack overflow and hack my way to a working solution

2. Take a 20 hour udemy course that will walk me through the process in detail, but probably shelf my project for 2 weeks.

Given that I'm not really on a deadline to complete this, but I also don't want to spend more time than necessary to learn deploy/kubernetes, which approach do you think is best?",1575469757,8,0,1,dataengineering
Implement Avg Time difference between events for our event analytics pipeline,"Hi,

I am new to this Data Engineering landscape and trying to create a real-time event analytics pipeline for our company. Our clients generate location pings at regular intervals of 5 seconds with their states. We want to calculate the Avg time an order stays in one particular state.  


In our current pipeline, we use Amazon Kinesis to ingest the messages and Kinesis Analytics stream processor to do the streaming joins and calculate aggregates over 1-minute intervals. These aggregates are stored in a PostgreSQL database which can be queried over from the dashboard which is going to be provided to the operations team. 

We are looking to empower the Avg Time difference queries to see how long the product stays in one state.",1575467629,1,0,1,dataengineering
"At bigger companies, which aspects of data engineering are the most employable and worth the most value?",[removed],1575446183,0,0,1,dataengineering
"Metaflow, Netflix's Python framework for data science, is now open source",,1575446015,3,0,1,dataengineering
Business Benefits of Cross Domain Tracking Using Google Analytics,,1575443369,0,0,1,dataengineering
Databricks Opinions?,[removed],1575413981,0,0,1,dataengineering
Understanding what is Data Leakage in Machine Learning and how it can be detected,,1575402909,0,0,1,dataengineering
Why Airflow,,1575398903,0,0,1,dataengineering
AWS MSK &amp; Lenses.io - Kafka in Days not Months. New Support for JMX Open Monitoring with Prometheus,,1575389264,0,0,1,dataengineering
Commercial ETL tools VS open-source,"Hello,

I'd like to get the input of this great community on their experience with commercial ETL tools. I'm in the process of evaluating Cloverdx (previously cloverETL) for my employer, and while I find it great, I cannot justify the price. Everything it can do can be done using python+Airflow+OS scheduling or some cloud service. Perhaps the visual designer makes it more user friendly for the non-software employees, but that doesn't justify the hefty price tag IMHO. 

Can anyone share their opinions/experiences on switching from a script-based data pipeline to a commercial ETL tool?

Thanks",1575386567,10,0,1,dataengineering
An interview about building a successful data team and managing their career growth to power a successful financial business,,1575385084,0,0,1,dataengineering
Change management tools?,[removed],1575358081,0,0,1,dataengineering
How To Set Up Snowflake On Demand (On AWS),,1575335425,0,0,1,dataengineering
Opinions on a project for a CS degree.,"Good evening,

As part of my degree I need to submit a project which needs to have a substantial coding component. I initially had a pretty clear idea of what I wanted to do, but as I started reading different articles and papers I started having second thoughts about my project idea.

I have always been interested in Data Warehouses, and I thought of creating a ""simple"" Data Warehouse to illustrate the different elements and technical aspects that need to be taken into consideration in building it, and then using some scripts, automate some reporting different departments of the fictious company may need.

Most of the articles I have read so far point out to the idea that whilst Data Warehouse are more needed than ever, the traditional architectures are not fit for purpose in the current environment ([https://www.jamesserra.com/archive/2017/12/is-the-traditional-data-warehouse-dead/](https://www.jamesserra.com/archive/2017/12/is-the-traditional-data-warehouse-dead/)), the main reason being the diversity of data sources, and the increasing importance of real time data.

My idea is gradually evolving to the integration of both relational data and streams using KAFKA, specially after having read Martin Kleppmann's articles, but I am not sure on how to keep the scope of this project manageable, or whether there are more suitable technologies than KAFKA that I should consider.

My tutor for the project can't help me much at the moment since he hasn't worked with Apache before, and for this reason I would appreciate if somebody could give me some advice on whether the project is interesting, and some advice on how to approach it.

Many thanks in advance.",1575313582,4,0,1,dataengineering
DB User Access Control,"What are the tools or best practices out there on managing permissions on databases?

I'm looking for something like apache ranger, but for agnostic jdbc connections.

I want to have a flexible and transparent system to control user's permissions in databases at column level. 

I hope this makes sense.",1575306086,4,0,1,dataengineering
How the Auth0 Data Team Uses R and Python,,1575284887,0,0,1,dataengineering
What tool am I looking for?,"I want to send data to Kafka then run some business logic on the data I am receiving from my consumer and spit that data out as fast as possible to a web frontend. Here is my idea so far but I am missing some pieces I feel like.

IoT Device -&gt; Kafka -&gt; Consumer (not sure what I should use for a consumer technology) -&gt; Cassandra -&gt; Flask/Spring boot API that CRUDS Cassandra

Any advice on the overall data flow would be great, or tools I should be using here to help. Especially in that consumer piece, I am not sure what to use.",1575251407,10,0,1,dataengineering
Nifty Pandas Trick: Your dataset has many columns and you want to ensure the correct data types,https://twitter.com/justmarkham/status/1192794326763474944,1575231478,3,0,1,dataengineering
Looking for some assistance on a project as I've drawn a blank,"I work in a highly regulated industry where each country has its own set of technical and legal regulations. There are both differences and similarities between each countries regulations and I want to develop a system where a user is able to easily identify where there are matches and where requirements are bespoke to each country. 

What I have achieved so far is to break down each set of regulations into categories and laid them out in a spreadsheet in a simple |Country|UniqueID|Regulation| - in another spreadsheet I have mapped each requirement based only on its UniqueID in a table format. So where they match they share the same line, where they don't there is no corresponding match on that line. Something like the following. 

|Denmark|France|Holland|Colombia|Egypt|

|DK001| ------ | ------ | CO011 | EG023 |

The dashes denote an empty cell, so no match for that requirement for that country. 

The end result that I am trying to achieve is where a user can input one or more countries, select the category and the output would display where there are matches and where they are bespoke requirements based on the users choices. It wouldn't need to be fancy, but would need to detail the actual requirement, not the Unique ID. And this is where I'm drawing a blank. I can't quite work out what would be the best way of powering this. I can't even think of the correct search term to use to find resources in google. 

If anybody has an idea on what would be the best way to go about this, or even point me in the right direction in terms of best resources, I would be very grateful.",1575199863,5,0,1,dataengineering
Management of Data Lake Raw Zone,"How are you handling ingestion to the raw zone of your data lake? Specifically for object based storage like S3. Are you copying a new snapshot to a date-named directory each time there is a load? Do you only bring over the deltas? I’m new to data lakes and wondering how best to update the raw zone if we are loading data, say, daily.",1574961033,9,0,1,dataengineering
Any good alternative to SAS DQMATCH?,"Hi, does anyone here know any good alternative to sas dqmatch? It’s basically a fuzzy match between strings. 

I know of fuzzy wuzzy package for python but this would mean I would still have to build a synonym table as well as the unique identifier along with a reasonable process flow that covers all steps involved. 

Was just hoping if there’s any existing packages already for R or Python that covers this. 

Thanks",1574917708,2,0,1,dataengineering
Simple question. How can I join two data sources in sql if one has a field/column the other lacks?,"So we get some data off couple of big name APIs and have to merge the data sets. For the most part this is simple, it comes down to a SQL UNION. We've been creating '0' placeholder columns for fields that don't exist in one dataset but do in the other. This is problematic though because in this case '0' implies a value of zero, not NULL. But we can't have NULL because the datatypes for the matching columns must be the same (numeric in this case) in order for the UNION to work. 

Does anyone know of an easy way to deal with this issue if you had to for several dozen columns?",1574896605,4,0,1,dataengineering
Concepts of Data Preprocessing,"Data is truly considered a resource in today’s world. As per the World Economic Forum, by 2025 we will be generating about 463 exabytes of data globally per day! But is all this data fit enough to be used by machine learning algorithms? How do we decide that?

Read this article to find out :  [https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825](https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825)",1574841254,0,0,1,dataengineering
Can someone explain MSK?,[removed],1574805694,0,0,1,dataengineering
Hi I am Jr DevOps Engineer in a Data Driven Company. How can I improve the company infrastructure (AWS) and provide automation.,"Here is our tech stack :

  
\- Kafka, Spark, Cassandra, Redshift, Jenkins, Kubernetes, Terraform,Atlantis AWS - GLUE/S3/EMR/DATAPIPELINE/LAMBDA/EC2/ATHENA, Wavefront and Cloudwatch for monitoring even AWS datalake. So my question since I work with several Data engineer, how can jr DevOps help them, what can we automate from a resource and infrastructre level, heck can we help them with even security ?  What level of automation or help does  a Data engineer needs from a DevOps. I really want to improve my skillset and I feel like this is one insight by asking here could help me. Any suggestion or advice is appreciated.",1574805659,3,0,1,dataengineering
An interview about how Sentry used Clickhouse to build an event data warehouse and pay down their architecture debt,,1574799929,0,0,1,dataengineering
Scaling Apache Airflow for Machine Learning Workflows,"Airflow is great for managing ETL pipelines! But what about machine learning workflows?

I was running a few hundred DAGs for all our data engineering with the Celery Executor, but as soon that we started doing machine learning, we realized we needed a solution to launch ML tasks remotely... and get ML version control.

I have seen companies using the *KubernetesPodOperator.* Last week, Dailymotion shared how they [manage to schedule Machine Learning pipelines with Airflow and Kubernetes](https://medium.com/dailymotion/bring-machine-learning-models-faster-to-production-with-airflow-and-kubernetes-e9d47ca3bee5). 

I was working in a much smaller team and we didn't have the resources to maintain a Kubernetes cluster...

So we decided to write an Airflow plugin for Valohai, a machine learning platform built on open standards. I just shared our experience [scaling Apache Airflow for Machine Learning workflows](https://towardsdatascience.com/scaling-apache-airflow-for-machine-learning-workflows-f2446257e495) using the *ValohaiSubmitExecutionOperator.*

Which executor or operators you use to scale Airflow for ML?",1574794866,4,0,1,dataengineering
What are the responsibilities of a jr data engineer job and are these jobs common ?,,1574791656,4,0,1,dataengineering
Repo organization?,"I'm looking for a reading list (articles, books, blogs, etc.) for the latest greatest theory and practice for repo organization for a large data warehouse with diverse ETL processes (written in multiple languages and created with multiple applications). I feel like there must be a textbook answer out there somewhere for what my organization is struggling with: organizing a very diverse repo. 

Any tips?",1574790759,0,0,1,dataengineering
Data science tripped up by data engineering?,"Quick question (or potential rabbit hole). How many data scientists here are regularly tripped up by data engineering and infrastructure issues? I regularly read about 80/20 issue related to cleaning data vs. analyzing it, but want to see if this holds true for the experts in this group. Do you end up wasting lots of time cleaning data and working on ETL, data warehouse and data lake issues?",1574779196,4,0,1,dataengineering
Python books or courses for Data Engineering,"Hi,

I am looking for recommendations on any books or courses which teach data engineering using python. Appreciate any links /resources.

TIA!!",1574744078,19,0,1,dataengineering
The Catch-22 Problem All Aspiring Data Engineers Face - Tips ?,"Hello,

I am an aspiring data engineer. The catch-22 problem that people in my position face is as follows: data engineers, by definition, develop infrastructure and solutions for processing large amounts of data (more or less, bear with me here). This is very hard to do using personal projects (since you simply do not have access to, or resources to support, a large amount of data), but how else would you get hired if you have no experience? It seems like a very difficult field to break into. 

I personally got very lucky and landed an internship that allowed be to develop lots of distributed ETL pipelines using cool tools for terabytes of data. BUT, if it were not for this luck I would be at a loss for how to approach entering this interesting field. 

I suppose my question is, how would a regular college student attempt to break into this field? How would you best demonstrate your expertise using certain big-data technologies without access to data thats big enough to necessitate those tools in the first place? Sorry if this comes off as a rant.",1574741273,0,0,1,dataengineering
Does it get better? Former business/data analyst with less technical bacground,[removed],1574712216,0,0,1,dataengineering
Interview Questions,"I had a few interview questions I've run into either in interviews or the application process and I wanted to get some feedback on them.

SQL

* One job description said, ""You must really know SQL and not just know SQL."" and then said, ""Yes there is a difference"".
* Another said rate your SQL skills from 0-10 0 being knows nothing and 10 being expert.

Question: What's the difference between someone who knows SQL and someone who really knows SQL. What are some SQL qualities that someone has that's a 10 vs someone who is an 8 or 7?

ETL

* How would you load 1,000,000+ large (gigabytes) log files into your database?

Question: I joined an established team as a Data Engineer and we working primarily with SSIS I haven't run into a problem like this yet. What's an appropriate answer to this?

Networking

* And understanding of web-based networking concepts, specifically an advanced understanding of HTTP.

Question: What networking knowledge is a Data Engineer expected to have?",1574711354,10,0,1,dataengineering
Airflow survey,"Apache Airflow developers team is running the following survey. Feel free to contribute if you are using airflow.

https://docs.google.com/forms/d/e/1FAIpQLSf20UIdK4GbTMXT_hsPn2iPvH_sNG6QmH8NXGnmJlt7WY_xkg",1574709961,0,0,1,dataengineering
Azure DataFactory copy job copies extension in the name of the destination file,"In Azure Datafactory I copy zipped files from one server and unzip them inside another server.

The source file has the following name and extension ""[sourcefile.zip](https://sourcefile.zip)"" with '.zip' not being in the name and actually the extension (checked it with the option hide extensions in file explorer). When the copy job is finished The folder gets decompressed but keeps the '.zip' in the name but is not a zipped folder.

&amp;#x200B;

Inside my job file I do a copy job from the dataset of the source server and get the ""@item"" that is given by the foreach loop and concatenate the "".zip"" in the wildcardfileName to capture the actual zip file. because the foreach does not give me the full name of the file and if I don't mention the "".zip"" inside the wildcard I get the error \`Could not find file\`

&amp;#x200B;

           {
                                ""name"": ""CopyDataToFileServer"",
                                ""type"": ""Copy"",
                                ""dependsOn"": [
                                    {
                                        ""activity"": ""CopyDataToIRserver"",
                                        ""dependencyConditions"": [
                                            ""Succeeded""
                                        ]
                                    }
                                ],
                                ""policy"": {
                                    ""timeout"": ""7.00:00:00"",
                                    ""retry"": 0,
                                    ""retryIntervalInSeconds"": 30,
                                    ""secureOutput"": false,
                                    ""secureInput"": false
                                },
                                ""userProperties"": [],
                                ""typeProperties"": {
                                    ""source"": {
                                        ""type"": ""BinarySource"",
                                        ""storeSettings"": {
                                            ""type"": ""FileServerReadSettings"",
                                            ""recursive"": true,
                                            ""wildcardFileName"": {
                                                ""value"": ""@{concat(item(),'.zip')}"",
                                                ""type"": ""Expression""
                                            }
                                        }
                                    },
                                    ""sink"": {
                                        ""type"": ""BinarySink"",
                                        ""storeSettings"": {
                                            ""type"": ""FileServerWriteSettings"",
                                            ""copyBehavior"": ""PreserveHierarchy""
                                        }
                                    },
                                    ""enableStaging"": false
                                },
                                ""inputs"": [
                                    {
                                        ""referenceName"": ""IntegrationRuntimeStorageZip"",
                                        ""type"": ""DatasetReference""
                                    }
                                ],
                                ""outputs"": [
                                    {
                                        ""referenceName"": ""FileServer"",
                                        ""type"": ""DatasetReference""
                                    }
                                ]
                            },",1574694092,0,0,1,dataengineering
How can i learn Data Modeling/Dimensional modeling,"Hey guys,

I have recently taken over a data warehouse project and i feel completely overwhelmed with trying to add new sources of data. I have existing dimensions Im wanting to add but i have no idea how to design dimension tables. Is there any good resources i should look into? I've been reading Kimballs data warehousing book in my spare time to try get up to speed. 


Work has also said they would be willing to pay for some training, does anyone have any suggestions? I was looking into some Microsoft certs like 70-767.

Thanks in advance!!",1574591874,8,0,1,dataengineering
Apply for DE jobs in Bay Area,"I have held SQL Developer positions for last 10 years and now would like to switch to DE role or Big Data Engineer role, right now I am focusing on python, algorithms, data structure, JavaScript and node.js and will eventually work in aws developer certification.

I apply to these jobs from indeed.com but I have no luck, I am living in Fresno, should I put a Bay Area address on my resume so employers think I am local.
It is scary to move to Bay Area without a job, I already have a good job here but I feel stuck in a role that’s not going anywhere and I feel I am not technically growing. I am preparing for triple bytes interview by practicing algorithms and DS.

Any tips on projects or job hunting tips will be helpful.
Thank you.",1574456035,6,0,1,dataengineering
"Need to use Docker container, unsure where to start","I'm doing a technical interview, and at the end it says ""It is also required that your code can be executed in a Docker container (use Docker Compose if you require additional infrastructure)"". I don't really know anything about docker, if someone could give me a run down of what it means, as well as some resources to learn how to use it would be greatly appreciated. THanks1!",1574449366,6,0,1,dataengineering
Streaming data from SQL Server to Kafka to Snowflake ❄️ with Kafka Connect,,1574445646,0,0,1,dataengineering
What do you think of this project idea to build DE skills?,"I'm a DE with almost a year of experience (I also have another 2 years of experience as a software developer).

At work, I mainly use airflow in docker containers for ETL work and build dashboards in Looker.

My reason for wanted to take on a side project is that while I'm familiar with docker, airflow and Looker, i don't feel that I have a truly deep understanding of them because I did not build those tools from the ground up. I'm able to add features, but the lead engineer is usually the one that has to do major work and is consulted on architecture and the really important stuff. On top of that, I want to gain experience in technologies that we don't implement at work (like spark, kubernetes, ML).

In order to build these skills iv come up with a DE focused side project that will touch on most of these technologies. 

The Project:

Monitor and predict power outages using US energy information administration hourly production and consumption api.

The plan is to use dockerized airflow, spark and Amazon RDS to build an ETL pipeline. I will also use spark ML to predict power outages. I will then build an analytics dashboard in tableau to display KPIs and outage predictions and alerts. I'd also like to use kubernetes to manage the docker containers.



My Question:

With the real goal of this project being to gain experience with DE technologies, is this a solid plan? (Given that I just thought of it yesterday) 

I know some of the tools used (like kubernetes) my be overkill for a small side project, but im more focused on learning.

Also, how expensive do you think this could get if I'm running ETL jobs hourly and using cloud storage? Is the budget going to be impractical for a side project?",1574433302,17,0,1,dataengineering
What is a Data Catalog and Why Should You Even Care?,,1574417705,2,0,1,dataengineering
"Any good way to think/learn about data with respect to size and fitting it to business needs , like how gbs-terabytes drive thought ?",Not sure if that fully makes sense. To help clarify a simple example of when an analyst can use their computer for data processing vs cloud resources.,1574404337,2,0,1,dataengineering
Kafka Tutorial - How to Visualize Data in Apache Kafka Using D3.js,,1574354151,0,0,1,dataengineering
ETL vs. ELT: A Live Data Integration Showdown,[removed],1574332381,0,0,1,dataengineering
What data catalogs have you used?,"It seems like data catalog is coming out of the woodwork nowadays with pure vendors like Alation, open source, and even Tableau getting into the mix.

What data catalogs have you had hands on with? What made them great or bad? What do features do you look for?",1574309773,20,0,1,dataengineering
"Validate data as you receive it so you know your data pipeline will succeed, no matter what changes are made upstream","I was tired of having to keep re-writing the same validation code for every pipeline I wrote so I wrote this tool to help:

 [https://show.schemaed.com/](https://show.schemaed.com/) 

The idea is that you define a schema which contains, data types, non-null, unique constraints as well as custom constraints which you can write in python so when you process a file you know you are guaranteed that the input file matches your original assumptions.

When data is invalid (according to our schema) each failed row gets a description of exactly why it failed so you don't have to start testing every row and column to find out where incorrect values are

Would love some feedback :)",1574264022,16,0,1,dataengineering
We should really have an interview questions/experiences thread,"Hi, I posted one of my interview questions yesterday and I read few exceptional comments which are possibly better answers than what I did in my interview. I think we should have a thread on this so that we can learn from others experiences. Any thoughts?",1574259649,3,0,1,dataengineering
Anyone a DE in EU and have any opinion on it as a career there ?,,1574233095,10,0,1,dataengineering
Are there roles that want someone who is great at data science and data engineering?,[removed],1574227898,0,0,1,dataengineering
Here is a question which I tanked answering in a recent interview.,"How to you design an optimal build failure/data flow failure notification model without disturbing too many people if you have a pipeline failure off hours.

&amp;#x200B;

I tried saying, I will create thresholds on priority, define several exit codes for my jobs and create a notification model based on that.

&amp;#x200B;

They didn't like this solution.",1574200139,14,0,5,dataengineering
First Onsite Data Engineering Interview Advice,"I've never done an onsite/white board interview before, and I feel very unsure of how these problems will go.

I'm having a really hard time finding example practice problems for Data Engineering interviews. I was told it will be a SQL/Python based on a white board, with some sort of problem, where I will have to understand the business/product sense, then create data models, and write up an ETL solution. 

As of right now, I have a solid amount of Python and SQL under my belt from courses/internships. I've done similar work to ETL in past experiences, but I don't know how detailed I will go with this on the whiteboard... am I going to literally create a python script, set up database connection credentials and a cursor in python code, add SQL queries to be read by the cursor, maybe memorize some Python Pandas syntax for the Transform, and do all of this on the whiteboard? Are there certain things I can omit?

Also, does anyone have any valuable resources for actual practice problems that tackle the business/product sense, data modeling, and ETL all at once? Or.. am I better off creating my own mock database system, think of my own problems, and then write up solutions?

Thank you!",1574195196,1,0,1,dataengineering
Tips on becoming a data engineer,"Hi, 

I am going to graduate in May 2020 with a B.S. in Computer Engineering. I am interested in data engineering as a field. Some of my relevant coursework is:

Data Structures, Algorithms, Distributed Systems, Database Organization (Relational DB Design), Operating systems

I have one internship at a startup as a general engineer, some of my roles included: 

● Tested middleware updates on IOT device in order to optimize software workflow  
● Wrote batch scripts for client to maximize local database update efficiency  
● Wrote Linux bash scripts for Build Root Flavor embedded device to fetch memory usage  
● Maintained local SQLite database for client, and wrote scripts to automate updates  
● Drafted workflow for Azure microservice using CosmosDB, ServiceBus Queue, and IOTHub  
bindings  
● Built and designed front end desktop applications using Python Tkinter GUI library

After this semester is over, I am planning on building a fully functional, end to end web scraper as my side project. 

As experienced Data Engineers, does my resume look good for a data engineer? What experience, skills, certs, could be helpful? I am searching for a job in Chicago. 

&amp;#x200B;

Thanks!!!!",1574193407,4,0,3,dataengineering
what are some easiest scheduling tools similar to Airflow? I know that there is Luigi..any other open source option?,,1574182576,11,0,1,dataengineering
Checkout our new Kafka CLI,"Maybe you've had the same struggles with managing kafka as we did. We just released the first ""beta"" of an open-source kafka tool we have been developing internally for quite some time: [https://github.com/real-digital/esque](https://github.com/real-digital/esque)",1574176302,0,0,1,dataengineering
From Data Oops to DataOps: 5 Things You Need to Know - Atlan | Humans of Data,,1574154078,0,0,1,dataengineering
Help review Consultant to Data Engineering CV/Resume?,"CV link: [https://docdro.id/DF74ms6](https://docdro.id/DF74ms6)  


I've currently got 3.5 years' experience working for two big brand technology consulting firms as a data consultant with a specialisation in data engineering. My major selling point previously was that I can do the consulting side but I am also a tech guy. 

However, I want to move away from consulting to a pure big data engineering position and I'd like some advice/input on how I could improve my CV and experience.   


Thank you in advance!",1574152014,1,0,1,dataengineering
Hypothetical scenario: Which job would be the better stepping stone towards becoming a Data Engineer? Data Analyst or Database Administrator?,,1574137497,20,0,1,dataengineering
An interview about data virtualization and data engineering automation with AtScale and the value of abstractions for your data platform architecture,,1574115908,0,0,1,dataengineering
Open source library to perform entity embeddings on categorical variables using Convolutional Neural Networks [+ unit tests and Continuous Integration],"In the past 2 years I have been working as a Machine Learning developer, mostly with tabular data, and I've developed a tool to perform entity embeddings on categorical variables using CNN with Keras. I tried pretty much to make it easy to use and flexible to most of the existent scenarios (regression, binary and multi-class classification), but if you find any other need or issue to be fixed, do not hesitate to ask.

I tried to add some cool stuff on the project, such as **unit tests**, **code coverage** with Codacy, **continuous integration** with Travis CI and **auto deployment** to PyPi and **auto-generated documentation** with Sphinx and ReadTheDocs, so if any of you is interested in how to setup your project to have these features, feel free to use it as a base project.

Looking forward to any reviews about the source code. Any tip to improve the readability or even performance, its really welcome and well appreciated.

**Github:** [**https://github.com/bresan/entity\_embeddings\_categorical**](https://github.com/bresan/entity_embeddings_categorical)

PyPi: [https://pypi.org/project/entity-embeddings-categorical/](https://pypi.org/project/entity-embeddings-categorical/)

Code coverage (nowadays reaching 97%): [https://coveralls.io/github/bresan/entity\_embeddings\_categorical?branch=master](https://coveralls.io/github/bresan/entity_embeddings_categorical?branch=master)

Thanks and I hope it can help somebody out there :-)",1574109801,2,0,1,dataengineering
AWS EC2 and EMR Start and stop when the jobs are not running,[removed],1574093401,0,0,1,dataengineering
What is the right way to transfer FTP files to Google Cloud Storage?,"I have been using rclone to transfer FTP files to google buckets but recently I have been having trouble with the files not appearing in the buckets while rclone ls shows they are there gsutil ls shows they aren't :/  


So I was wondering what is the easiest way to do this simple task.",1574078211,1,0,1,dataengineering
String parsing using PySpark,"I have a sample dateset:

    |cookTime|datePublished|         description|               image|         ingredients|                name|prepTime|recipeYield|                 url|
    +--------+-------------+--------------------+--------------------+--------------------+--------------------+--------+-----------+--------------------+
    |   PT25M|   2012-08-06|Who doesn't love ...|http://static.the...|1 stick Butter
    1 ...|         Patty Melts|   PT10M|          4|http://thepioneer...|
    |    PT3H|   2012-01-30|My oh my, was thi...|http://static.the...|2 Tablespoons Can...|Spicy Stewed Beef...|   PT20M|         12|http://thepioneer...|
    |   PT25M|   2012-02-08|This was really, ...|http://static.the...|6 whole Pork Chop...|Pork Chops with G...|    PT5M|          6|http://thepioneer...|

From the cookTime and prepTime I'm trying to find a reliable way to extract time string from values such as ""PT25M"" or ""PT3H"" and basically convert all to minutes and calculate total time in mins. Is there a way to do so using PySPark re-ex functions, or should I resort to using Pandas/NumPy instead? In either case, what would be the ideal way to parse string values and extract and convert numerical values using prefix ""PT%"". 

Thanks for looking and for any help!",1574076647,1,0,1,dataengineering
Break into data engineering w/o technical background,"I live in Eurooe, graduating in science of communication and trying to become a data engineer in the meantime. I'm good at SQL and I started 2 months ago to learn Python with some books (Think Python and Automate the Boring Stuff) . Would a couple of github ETL projects be enough to break into the field?",1573956802,8,0,1,dataengineering
Career paths for experienced data engineers?,"I am a medium to senior data engineer and being the ""end of year"" and ""review"" season, I am setting out to think about what goals would be good to achieve in the coming months.

The main reason I am asking this is because after 6 years specifically in the data industry, I feel like I will continue to loop around many different tech areas which is resulting in a bit of ""tool fatigue"". 

I want to be strategic with how to develop my career, and frankly I am a bit lost. 
One option is to continue mastering new tech, trying to become more and more ""senior"" to climb the corporate ladder. But I am quite young (late 20s) and I fear that most corps don't look to me for senior and architecture roles just yet. I've seen several older developers who might not be as technical as I am, get into posts which are higher than me simply because they are older and have more generalist experience. I am not that comfortable playing a ""waiting game"".

The other options are to pivot my career into product management or DevOps, which scare me a bit since it will be taking things into a different direction.

My current status:
1. Designed ETL in three different tools and experienced all major ""traditional"" RDBMS and well versed in DWH stuff.

2. Comfortable writing Python, Java and some Scala.

3. AWS certified with a little GCP/Azure Experience.

4. Reasonably comfortable with Docker, Kubernetes, Jenkins and Kafka.

5. Have used the major Apache Projects (Airflow, Hadoop, Nifi, Spark, Flink)

6. Spent quite some time working enterprise dash boarding tools (ex. Qlikview)

7. Decent knowledge of alternative data stores (MongoDB, DynamoDB, Elastic, Redis, Amazon Redshift)

8. I have a maters degree in ML.

**So my question is, what are your professional goals this coming year? What does a career path for an experienced data engineer look like? **

I am mostly interested in the goals of people with 5+ years experience (medium to senior level) but it would be also cool to get the insight from newer people.",1573949825,0,0,1,dataengineering
Insights about Insight Data Engineering Program,Has anyone attended the Data Engineering program by Insight in the recent past? I'd like to know your experience as most of what I can find online are specific to the Data Science program. I'd also like to know about their scholarship program and the legal clause about not interviewing with any other company upon acceptance of the offer,1573946077,17,0,1,dataengineering
Kafka Connect + dbt Sanity Check,"Hi Everyone. I am looking to overhaul our current stack as follows. 

Data ingestion with Kafka (MSK on AWS) and Kafka Connect. We’ll write Avro to S3 and use commercial JDBC drivers with the JDBCSource connector. 

From there, we’ll use Glue Catalog and add external tables point to the S3 location for storage, plus the Kafka schema registry for the Avro schema. 

We’ll then use dbt and SparkSQL (Spark on EMR) to model the data on S3. 

And finally we’ll expose the data with Athena (Tableau) and Presto on EMR (Metabase) for business intelligence. 

I’m wondering if anyone has any thoughts on Kafka Connect generally, dbt generally, the explicit omission of a data warehouse like Redshift in favour of Presto/Athena and the whole end-to-end solution. 

What is likely to go wrong? What might be a better approach?",1573938729,14,0,1,dataengineering
PySpark - Recommendation needed for functions,"I'm completely new to Spark and was just starting to learn a few things and was wondering how to reliably parse and match keywords in a column. I'm exploring a sample JSON data-set for example with following df schema:

    root
     |-- cookTime: string (nullable = true)
     |-- datePublished: string (nullable = true)
     |-- description: string (nullable = true)
     |-- image: string (nullable = true)
     |-- ingredients: string (nullable = true)
     |-- name: string (nullable = true)
     |-- prepTime: string (nullable = true)
     |-- recipeYield: string (nullable = true)
     |-- url: string (nullable = true)

Now, I'm trying to add a column contains\_chicken based upon if ingredients (separated by "","" and line breaks) list the word ""chicken"" in them, and write to a csv. Here is my sample code:

    from pyspark.sql import SparkSession
    import pyspark.sql.functions as psf
    
    spark = SparkSession.builder.getOrCreate()
    
    sc = spark.sparkContext
    
    path = ""data/recipes.json""
    recDF = spark.read.json(path)
    
    recDF.withColumn(""contains_chicken"", psf.when(psf.col(""ingredients"").rlike(""chicken""), ""true"").otherwise(""false"")).toPandas().to_csv('export.csv')

Upon inspection, it seems like the csv has false negatives for most and ends up only marking 2 values as true, and one of them is incorrect. Am I using the correct function to do this? Should I do some data cleansing on the data-set first? What approach should I take for testing this?

Any direction or response is appreciated and thanks for looking!",1573935977,2,0,1,dataengineering
The Easy Way to Extend Pandas API,,1573887974,0,0,1,dataengineering
Generated SQL,"We’ve been playing with Stitch with a SQL Sever source and Snowflake destination. 

Is there a post or any info on how it automagically generates the create table statements and insert statements and assigns data types?",1573865996,2,0,1,dataengineering
"Are virtual private databases a thing currently? What are some of the good ones other than Oracle? If they are not so trendy right now, how do we fulfill the needs of security w/o them?",,1573848256,9,0,1,dataengineering
How do you move from the ingest layer to serverless layer?,I was going through the data engineering cookbook's interview questions and found this question. I have some rough ideas but wanted an articulate answer. I read some posts (top google search) but couldn't get much out of it.,1573831566,2,0,1,dataengineering
Best practices data warehousing,"Hello. I am more or less in charge of the data warehouse of my company so I was wondering if there is any good resource about how to create and manage data warehouses. 

Kind of questions that I want to get answered:

* How narrow should be the scope of a schema? Is it better to have a schema called ""users"" with tables about newsletters, interactions and recommendations with some kind of prefix for the tables? Is it better to have three different schemas?
* How many staging areas are needed? We have three schemas: stg, meta and dwh. Is it necessary to have meta also?
* What are the best practices for naming? I have read some of them but not all the cases were covered
* Is it a good idea to have a schema called mix or something like that for tables that are maybe too specific for fit into another schema?
* How important is the column type (i.e varchar length?)
* Is it a good practice to create key restrictions?

I will probably have more but I don't recall right now. I am using Redshift by the way. What do you recommend? 

Thank you very much in advance",1573814212,16,0,1,dataengineering
"Data Profiling: Process, Best Practices and Tools (Overview)",[removed],1573812900,0,0,1,dataengineering
"First data engineer role, what’s the best data engineering with python course I could do over the next few weeks","Hi all, in a few weeks I’m moving from a stats role using R and SQL to a data engineering role. I can probably use whichever software is best suited and I’ve heard that python is better for data engineering.

Are there any free online courses or blogs that I can follow through over the next few weeks to get a head start on developing my python skills? I’ve had a search myself, just looking for specific recommendations. 

Thanks!",1573807976,21,0,1,dataengineering
With the advent of telepathy nearing how do data engineers support these solutions. Can a data engineering (reasonably) provide value with existing skills for current demand to support this potential future demand ?,,1573802894,2,0,1,dataengineering
Tutorial on how to use Airflow without pain,,1573743062,0,0,1,dataengineering
Community for building data products,"Hey folks, first and foremost there is a fantastic data engineering discord channel that is linked at the top of this subreddit. For the practice of data engineering, career guidance, help, and a host of other things that is the perfect community to join.

However, I'd like to invite you to [this Slack channel](https://join.slack.com/t/growthtesttube/shared_invite/enQtNjc2OTcxMDQ1MjQ5LTIwNWMzZGUzYWFkZTQyNDY1M2VmN2ZlMDQxMTdjY2VmNzUwNTFlYTg5MGY3ZGI2ZWMzZWE4MjdlMDNjN2M4NWY) where we work on launching data products. I used ""products"" and not ""projects"" because our goal is to (eventually) generate revenue for the small squads involved in each. Data products can take many different forms: ebooks, courses, services (e.g. lyrebird ai), resources (e.g. Ahrefs), and physical products. We're interested in all of them and want to hear your ideas.

Though the focus is on launching data products you don't have to be a veteran or professional engineer to join. There will be spots in each group for junior folks/those looking to get some experience to join in and get your hands dirty with some data work. There's also [more information on our site](https://www.growthtesttube.com/#/). 

We've really just launched this community and so are seeing if folks are interested in joining. The next incubator project we have planned starts on January 1 and will consist of a team of no more than 12. That doesn't mean we can't run others simultaneously, especially if there's interest. Anyway, stop by if that sounds interesting and introduce yourself. Thanks!",1573726201,8,0,1,dataengineering
Non-engineering resources for Data Engineering,"I'm at the final stage of an interview process for a data engineering role at a startup in London. I have a fair amount of experience with most of the engineering side of data engineering (warehousing, SQL, Spark, have the GCP data eng cert, system design) but as this startup has no data analysts or data scientists yet, I will be advising teams on dimensions, metrics, KPIs, as well as visualising the data. You could say that the role would be spanning the full spectrum of data.

This sub has a load of information on the engineering side but are there any recommended courses for picking up the non-engineering side of data? I've seen that there is a [data warehousing course on Coursera](https://www.coursera.org/specializations/data-warehousing) but I was curious if anyone has any further ideas? I have an annual subscription to Dataquest so I can pick up statistics and data viz via the data analyst course.

Things I reckon I should be looking at (feel free to suggest things I could be missing):

Data modeling (UML diagrams, many to many relationships)
KPIs (customer lifetime value, cost of acquisition etc.)

Or is there a sub with a sidebar which covers the majority of this content?",1573681216,3,0,1,dataengineering
How did you deploy Airflow into production? (AWS),"Hi All,

Have committed to rolling out Airflow and now looking at the best way to deploy it on AWS specifically.

Did you do a plain EC2? or run in Docker with ECS? If so, did you split up each service to a different container?

More interested in the pros/cons of both approaches so I can make an informed decision.

&amp;#x200B;

If it's relevant;

Every DAG/task will be an AWSBatchOperator to run our containerised scripts

Volumes will be low, in the 20's per day for now

Logs will go to S3

Will use an RDS instance for the DB

Want to view the webserver/ GUI locally

&amp;#x200B;

Cheers",1573681195,16,0,1,dataengineering
An interview about data protection regulations and how they can influence the design of your data platform,,1573613940,0,0,1,dataengineering
A little data scientist needs your help,"Hello data engineering fam! I just graduated college as a machine learning engineer and I’m struggling with finding a job as a data scientist because of my lack of knowledge in data engineering (I live in France and this is what the market needs apparently). All my skills are around using python for machine learning/deep learning and SQL for data collection. Now I’m lost in what to learn first in term of data engineering (technologies for data acquisition, data storage, data deployment, and data viz etc).  Can you give me a small (or big) list of a must know technologies for a junior profil like me and some certification that I should get like coursera or udemy ones. Thanks a lot in advance &lt;3",1573589512,4,0,1,dataengineering
Training plan for data engineering on the job,"I have just joined my company’s analytics team as a data engineer. As of now, it looks like my role will be a hybrid of a database focus and a infrastructure focus, and I will be expected to be skilled in Python, AWS, Snowflake, SQL and a couple of ETL tools. I may also be required to build data pipelines for specific analytics projects. The team is new, so things will evolve, esp over the next 1 year.

**MY QUESTION(s)**

I am creating an on-the-job training plan for myself, for the next 2-3 months, and some feedback/suggestions on these questions would be super helpful:

* **Subject areas**: I think I need to focus on *Python*, *SQL*, *AWS* and *learning to build data pipelines*. Are there any other subject areas I have missed?
* **Resources**: I was planning to go through Dataquest’s Data Engineering track. I don’t expect it to teach me everything, but does it cover all the relevant subject areas?
* **AWS**: What learning paths (and classroom training) within AWS will be appropriate for me here?

**CONTEXT**

Background: Just so you know where I am coming from, I was hired into this role immediately after an internship, where my project involved building a prototype to measure, record and visualize a few data quality metrics. I used Python, SQL, Tableau and AWS (mostly Lambda, with SNS and CloudWatch). This seems to me like the opposite of the norm here, where people are trying to break into data engineering from other roles.

My current skills: I currently have some knowledge of Python and SQL from grad school, but I can improve upon them further. I also got some experience using Snowflake during my internship, and needed to read up a bit on Lambda, SNS, SQS, CloudWatch for what I was doing.",1573588907,8,0,1,dataengineering
Unable to activate Airflow Web authentication:.,[removed],1573579596,0,0,1,dataengineering
How do I vizualise the flow of control in some sql queries?,"I wanted to improve my SQL and for that, I need to understand the flow of control in SQL. I understand the basics but here is one such instance where I am unable to comprehend as to how a column from one table can be used in the where clause of another table in a different query(subquery)?

[This](https://www.hackerrank.com/challenges/the-report/problem) is the problem statement. The query is:

```SELECT IF (Grade &gt;= 8, Name, NULL), Grade, Marks FROM (SELECT Name, (SELECT Grade FROM Grades WHERE (Min\_Mark &lt;= Marks) AND (Marks &lt;= Max\_Mark)) AS Grade, Marks FROM Students) As MyStudents ORDER BY Grade DESC, Name;\`\`\`

My doubt is that in the subquery: (SELECT Grade FROM Grades WHERE (Min_Mark &lt;= Marks) AND (Marks &lt;= Max_Mark)), how come 'Marks' can be used?",1573576016,3,0,1,dataengineering
What ETL technologies does the Chinese government use?,"China has some of the most intense data processing challenges. I’m assuming they don’t use common, open-source ETL tools like Spark, Kafka, Airflow.",1573560877,7,0,1,dataengineering
How do I optimize this code on Pyspark?,[removed],1573541261,1,0,1,dataengineering
"Re: column databases, thoughts on PostgreSQL w/ store_fdw extension vs Greenplum?",,1573240771,7,0,1,dataengineering
How to use Lenses as a Secure Data Layer to Produce Data into Apache Kafka,,1573228264,0,0,1,dataengineering
Advice for total beginner: should i take the bait?,[removed],1573220617,3,0,1,dataengineering
Looking for DE Research Papers,Are there any research papers around Data Engineering? Any links?,1573200395,8,0,1,dataengineering
How is Databricks Spark different than Spark?,[removed],1573185039,1,0,1,dataengineering
UX Research,"Hey, friends! I'm looking for some people with experience handling workflow automation and orchestration to participate in a 1-hour video call. You'll get $50 to talk about your data challenges and provide feedback on a new product I'm working on. If you're interested, take this 1-minute survey to see if you qualify, and make sure you leave your email so I can reach out to you to schedule the session.  [https://www.surveymonkey.com/r/S8GT5R8](https://www.surveymonkey.com/r/S8GT5R8)",1573167431,0,0,1,dataengineering
AWS Glue/Lake Formation or Airflow?,"I have a greenfield data pipeline to build on AWS, and I've narrowed down orchestration to these two options. Lake Formation looks like a good fit for quickly exporting our data on Aurora, but I'm afraid we'll end up fighting against the tool when trying to integrate with external sources like Google Analytics.",1573156459,15,0,1,dataengineering
Why is CData not more popular?,"Fivetran and Stitch are quite popular for ""modern"" ETL (or rather EL). But looking at CData I wonder why it's never mentioned. In terms of connectors (or ""drivers"" in their parlance) they look at least as complete if not more extensive and they have the option for on-premise which neither Stitch nor Fivetran offer. Is there a reason for that? Have you even heard of them?",1573068612,10,0,6,dataengineering
Is there something like Fivetran/Stitch but On-Premises?,,1573063820,9,0,3,dataengineering
Migrating to Snowflake,"Hi Guys! Long time lurker, first time questionner. Love this sub! Just want to ask for advice what could be the best option for acquiring data from a RDS to Snowflake? Thanks!",1573016634,14,0,1,dataengineering
Not Understanding The Logic,"This is for Teradata + Job Scheduler

&amp;#x200B;

&amp;#x200B;

In a job scheduler, two steps have been setup for refreshing database.

On the first step, it is called as 'data set ready' script.

The script is on bteq:

SELECT  
Cast(  
CASE  
WHEN 'CONDITION' IN ('','X') THEN '--'  
ELSE ''  
END  
||'CT '  
||Trim(Substring(Databasename From 1 FOR 5))  
||'Tablename\_'  
||Trim(Substring(Databasename From 11 FOR 200))  
||'.'  
||Trim(TableName)  
||' as '||Trim(Databasename)  
||'.'  
||Trim(TableName)  
||' with no data and stats;' AS VARCHAR(1000)) (Title '')  
,'0A'XC(Title '')  
,'.IF ERRORLEVEL != 0 THEN .QUIT 1;' (Title '')  
FROM  
database.tablename  
WHERE Databasename NOT IN ( 'databasename')  
AND 'CONDITION' NOT IN ('','X')  
ORDER BY  
Databasename  
,TableName  
;

SELECT   
Cast(   
CASE   
WHEN 'CONDITION' IN ('','X') THEN '--'   
ELSE ''   
END  
||'INSERTO INTO'  
||Trim(Substring(Databasename From 1 FOR 5))  
||'Tablename\_'  
||Trim(Substring(Databasename From 11 FOR 200))  
||'.'  
||Trim(TableName)  
||' as '||Trim(Databasename)  
||'.'  
||Trim(TableName)  
||' with no data and stats;' AS VARCHAR(1000)) (Title '')  
,'0A'XC(Title '')  
,'.IF ERRORLEVEL != 0 THEN .QUIT 1;' (Title '')   
FROM   
database.tablename  
WHERE Databasename NOT IN ( 'databasename')  
AND 'CONDITION' NOT IN ('','X')  
ORDER BY   
Databasename  
,TableName  
;

Moving to the second step of the schedule, the job name is called that actual name of the database as the step above with the following bteq command:

/\* ARC SCRIPT \*/  
SELECT 'logon host/username,pw;' (Title '') ;

SELECT 'ARCHIVE DATA TABLES' (Title '') ;

/\*GENERATE ARCMAIN SCRIPT \*/  
SELECT '(' || Trim (DATABASENAME ) || '.' || Trim (TableName ) || '),' (Title '')  
FROM DBC . TABLESV  
WHERE 1 = 1  
AND DATABASENAME = 'This.database'  
ORDER BY 1 ASC;

SELECT 'RELEASE LOCK,' (Title '') ;  
SELECT 'FILE=SIM;' (Title '') ;  
SELECT 'LOGOFF;' (Title '') ;  
.EXPORT RESET;  
/\* COPY SCRIPT \*/

SELECT 'logon host/username,pw;' (Title '') ;

SELECT '.SET QUERY\_BAND = ''BLOCKCOMPRESSION=YES;'';' (Title '') ;

SELECT 'COPY DATA TABLES' (Title '') ;

SELECT '(Database' || '.' ||Trim(TableName) || ')'  
|| ' (FROM(' || Trim( DATABASENAME) || '.' ||Trim(TableName) || ')),' (Title '')  
FROM DBC.TABLESV  
WHERE 1=1  
AND DATABASENAME =''  
AND TABLEKIND ='T'  
ORDER BY 1;

SELECT 'RELEASE LOCK,' (Title '') ;  
SELECT 'FILE=SIM;' (Title '') ;  
SELECT 'LOGOFF;' (Title '') ;  
.EXPORT RESET;  
.LOGOFF;

I am lost in between the two steps. 

What is their relation? What is the whole purpose of the first step? 

Wouldn't the second step (this single step) be enough to backup and restore the database?",1572996932,0,0,1,dataengineering
Is this an example of an ETL pipeline?,"I'm a data scientist who is currently learning some data engineering processes for my role.

Currently, I am running a Jupyter Notebook with Python for implementing time series models in Amazon SageMaker. My current set up:

1. CSV files uploaded to AWS S3 bucket

2. CSVs then pulled into Amazon SageMaker through the Jupyter notebook using boto3, botocore and sagemaker libraries

3. Data manipulation then takes place with pandas in the notebook itself for the analysis to subsequently be carried out

Does this process represent a data pipeline, and an ETL pipeline per se? The data is being transferred from one AWS storage space to another with a view to transforming and then analysing the data but I'm wondering whether this strictly fits the definition of ETL.

Would be grateful for any advice.",1572991384,10,0,7,dataengineering
Data Analyst wanting to make the move into SWE/Data Engineering. How do I do it?,"I'm currently a data analyst, and I've been working as one to varying degrees for the past 10 years.  My current role has been the most technical, where I use some SAS, SQL, and Tableau.  A lot of what I do is pulling data from our servers, modeling it with SAS &amp; SQL, and then building dashboards.

Anyway, if you look at my post history, I've been considering moving from this data analyst role to something more technical, e.g. SWE or Data Engineering.  Data Engineering seems to make the most sense since I'm somewhat acquainted with data.  I'm enrolled in Georgia Tech's Masters in Analytics program, but will most likely be dropping out because I don't want to move further into Analytics and Data Science.  

How do I make myself qualified for a role in the data engineering field?  I've done some self-teaching in Python, took a break for a little too long, forgot mostly everything, but will be revisiting that soon.  I'm also going to be taking a few community college courses in data structures, algorithms, etc., but I don't if that'll be that helpful for data engineering.  I'm just trying to get a little more exposure to SWE.  What are some other things I should do to make myself a more qualified applicant?",1572974165,27,0,26,dataengineering
Seeking Toronto data engineers,"Not sure if this type of post is allowed - my apologies if not.

I work for a high growth VC funded tech startup in downtown Toronto where my team is currently focused on building out our data infrastructure . Specifically, we’re building a reporting database on top of our production database. We’re looking to hire a data engineer to act a part time consultant / mentor to help guide us through this process. 

The time commitment would be once or twice every two weeks, likely outside of regular work to accommodate for your full time job. A lot of the work would be consultative high level guidance, but would also require lower level expertise. We are planning on working with Postgres and Python.

Please send me a direct message if this is something you are interested in, and I’d be happy to provide further details and schedule a call to see if it might be a good fit.",1572931154,0,0,1,dataengineering
How do I master for SQL questions for FAANG in one week?,"I have a data engineering interview in one week and I am struggling in SQL. I do understand all of it but unable to solve questions on Leetcode. So far, my experience was in etl, data lakes etc but not much in SQL.

I even have issue in visualisation of the flow of control that my potential query should follow.",1572930964,15,0,10,dataengineering
Monzo Data Team,,1572903899,1,0,1,dataengineering
Am I missing something in regards to BigQuery costs vs Snowflake?,"We're in the process of evaluating Big Query and Snowflake for a near real time reporting solution we are building

For Snowflake we did a trial run and were able to come up with an estimated yearly cost fairly easily

We are now evaluating Big Query and based on our use case it looks extraordinarily cheap. I'm talking many orders of magnitude cheaper

Our use case is to develop various dashboards and reports for one of our teams. The data the reports will be querying is perishable i.e. after a certain time, we no longer care about it and will move it to another table. This means there is a general maximum table size we can expect on any given day

Using this we have done some, admittedly, rough calculations and BQ is looking a lot cheaper. I would have thought pricing would be somewhat similar. 

Am I missing something obvious about BQ pricing?",1572887334,14,0,12,dataengineering
An interview about how the Ascend platform provides an autonomous data orchestration platform to simplify your production dataflows,,1572873538,0,0,0,dataengineering
A Definitive Compilation of Apache Airflow Resources,,1572873337,2,0,35,dataengineering
Acquire a Data Engineer position at Google,"Not sure, that I choose right subreddit for this post, but hope this will work :)
My question is how to get this job at Google? Maybe this question is asked too often, but I didn't find a lot information in web about exactly Data Engineer position.
Currently I have an 2,5 years experience at data engineer position ( and 3 years of Java developer before that), also I started to learn algorhitms(books Algorhitms in Java, Cracking the Code Interview), some concepts of programming(functional programming, different languages (Scala, Python, Rust)), different databases (from relational to NoSQL like ClickHouse, Cassandra),  advanced Spark techniques, Cloud Services, Machine Learning Courses, etc...(I can talk about this for hours)

I just want to know , Is it the right direction? Or maybe should I adjust something? (or everything?:) ) I've heard about Google CLoud Platform Certification, Is it must-have for position at Google?

I'm not sure, will the huge amount of information about ""normal"" Google interviews work for the interview of Data Engineer?
It will be great to hear opinion from people who have an interviews at Google, or maybe from ones who was interviewing somebody for this position.

THanks for help",1572869965,16,0,9,dataengineering
Looking for mentorship or remote internship in data engineering,[removed],1572859413,0,0,1,dataengineering
Good books/resources for Kappa/Lambda Architecture for Data Pipelines,"I have switched from Build/Release Engineering to Data Engineering and relatively new to it. I am currently tasked with creating Data Pipelines. Can you guys suggest good resources (videos, courses, books, etc)?",1572852614,3,0,9,dataengineering
How to really build a career as a data engineer and beyond ?,Currently my first few roles as a data engineer. I want to do my job well and also lay the foundation to be on top of my career in the future. What projects are best for data engineer to complete ( like idk building data warehouses for analytics) and what companies or products to reference as role models. I know DJ Patil was the CDO for Obama so he is a good example. Just ideas like how to be the best you can be,1572841562,5,0,1,dataengineering
"Databricks Delta, Apache Hudi, and Apache Iceberg for building a Feature Store for Machine Learning",,1572798646,3,0,19,dataengineering
Currently use Pandas. Should I Switch to Spark?,"Currently I use pandas in all my airflow dags/scripts. 
Would it be worth me switching to pyspark? What benefit would it provide?",1572795042,19,0,6,dataengineering
Apache Airflow Cluster Issues,"Hi All,

We are using Airflow with few complexities:

1. If I create a Airflow cluster(1 Master and 2 Workers) . Logs are written in Elastic file system and Properties file(run id and InstanceID ) are being written in EFS . However EFS is really slow and We switched to EBS . Every hop is failing because the properties isnt being shared in local EBS volume if we have two workers withour EBS. EFS to EBS performance is boosted from 2 hours run to 10 mins run of a dag in Airflow.
2. We have a RDS table to check the dependencies . We tried to replace control-m with Airflow. 
   1. Daily and Weekly(sunday) scheduled dependencies are fine. For example : For a dag to run all the dependencies should be met . In our case weekly dependencies only will be met only on weekly scheduled day(sunday). Remaining days we are manually marking the dependencies as success . What logic should we think or how to tackle this.",1572758347,3,0,5,dataengineering
Noob looking for guidance,"Hi everyone! 
I am Masters graduate in Computer engineering. I used to work in a Brain machine interfacing lab where we study the relationship between brain waves and motor signals. 
I did few projects on my lab data using python and R to make sense of the raw data we collected. I was wondering if somebody could give me some pointers or suggest me courses that I can do to get started in Data engineering field ! 
I would love any kind of advice!",1572732120,9,0,7,dataengineering
Where to start?,"Hi all,

I’m really new to the concept of data engineering but it intrigues me. I went to school for accounting but in my job I’ve excelled at using SQL and Python to solve issues. I really enjoy creating tools, interfaces, helping others pull data for analysis, etc. Data science isn’t really something that speaks to me, it’s a big buzz word in my work but after reading about data engineering it seemed like something better suited to my interests. 

I know nothing about this field, where can I start at an incredibly high level? I want to learn more.",1572729032,4,0,7,dataengineering
What languages are you using?,"As a data engineer, what programming languages do you use regularly? 

I am asking because I am almost exclusively python and I am curious if this will limit me. If so what would be your suggestions?",1572662536,12,0,6,dataengineering
OSS Great Expectations just released a Self-Updating Data Dictionary,"Check out this really great tutorial/blog on how to implement the ""Self-Updating Data Dictionary"". Its pretty awesome for how low friction it is to implement. Solid tool to maintain transparency across the data engineering / data science team  and whoever else interacts with them.

[Read here](https://greatexpectations.io/blog/20191004_data_dictionary_plugin/?utm_source=reddit&amp;utm_medium=post&amp;utm_name=user-testing)

[Their Github](https://github.com/great-expectations/great_expectations)",1572655634,7,0,13,dataengineering
Career path after Data Engineer?,"I'm thinking about my career and wanted insight into other vets in the DE space on their job title or career path goals...

Since it's a relatively new space, I was wondering what job titles exist currently besides ""Senior Data Engineer"" or ""Data Architect"". What's after that?",1572633287,11,0,21,dataengineering
Google to acquire Fitbit,,1572620397,2,0,0,dataengineering
Are There No Good Airflow Tutorials?,"Im a junior DE who just started using Airflow.

Ive built a couple of dags now im still not sure I get it. Things like data must be at rest between tasks just dont make sense to me. Like cant i just pass a dataframe between tasks?

Are there any good real world examples out there? Things like grabbing files off a ftp and importing, or pulling data from an api. It just hasnt clicked for me yet.",1572570912,38,0,24,dataengineering
Spark Streaming?,"Hey guys,

So I have log data coming into Kinesis and I am currently pulling that log data with vanilla python, I need to run business logic on this log data and have close to real time results. I noticed spark streaming seems to be something that works well for this. The problem is I am having a hard time understanding why. As far as I can tell it breaks the data into time series chunks? the something below would be the business logic. Any help would be greatly appreciated.

Logger -&gt; Kinesis -&gt; SOMETHING (Spark Streaming?) -&gt; Dashboard/Web page/Database",1572547153,2,0,2,dataengineering
Chinese users attack Notepad++ app after 'Free Uyghur' release | ZDNet,,1572543761,0,0,14,dataengineering
Would taking a data engineer role for a year or two help me towards my end goal of becoming a data scientist?,"I currently work as a statistician and have employed some data science techniques into my work. In use R, SQL, git and Azure Dev ops and I’ve also been doing a bit of data engineering work in SQL.

I’ve been offered a potential move into a data engineer role. Although it isn’t exactly what I want to be doing (I would eventually like a data science role) it still looks very interesting and I thought it might be a good basis to keep developing my data science skills and programming skills. 

I have applied for a few data science roles but have been pipped to the post by other candidates because of my lack on experience in building ds models.

So my questions is, is this a move in the right direction? Would having a really good understanding of data engineering help me to develop my data science career? 

In some places I’ve read that data engineering and data science cross over a lot so it sounds like it could be a good move.

Any data engineers and data scientists out there that can offer any information?

Thanks!",1572537607,22,0,23,dataengineering
Seeking guidance to pick up data engineering skills,"I'm seeking guidance on how to pick up data engineering skills and possibly navigate my career that direction. 

I don't have background in computer science. I came from civil engineering undergrad whose career slowly steers toward analytics after an MBA and MS in Econometrics. I'm a jack of all trades master of none data scienctist (possibly even data analyst depending on people's definition). 

- My coding skills are basic. I know R from my econometrics program, and I use python because we use GIS quite a lot. 

- Stuff that I do comprises time series (mostly with commercial package called eviews, sometimes R or Python), some predictive analytics with sklearn, spatial statistics and dashboards (tableau/powerBI). 

- Building ETL. **However**, we use commercial packages - FME for spatial data, talend/alteryx for everything else.

- No big data experience (no spark hadoop etc), no cloud experience (aws, GCP, exp). We're federally regulated, so everything is on-prem. 

- I spend like 80% of my time on data hunting, collection and cleaning. Perhaps only 10% of my time is spent building models, and another 10% on communicating the results and documenting them and putting them into our knowledge management system.

- I don't do NLP or computer vision

I'm sorry for the very long post, but I'd appreciate someone pointing me in the direction where I can pick up data engineering skills. 

I thought about doing UPenn's online MCIT because I don't have the CS education. I've also considered GATech's OMSA since I don't think I have the necessary foundation for the OMSCS. 

If at all possible, I'd like to avoid going back to school. I can't stomach another masters, and honestly I'd rather work on practical projects building something and have something to show for it. Not another paper. Thank you much!",1572477664,10,0,13,dataengineering
Why are more job openings for data engineers than job openings for data scientists?,I apologize if this question has been asked before but I am wondering why this is happening at the moment.!,1572468695,6,0,1,dataengineering
The Complete Data Science LinkedIn Profile Guide,,1572453253,0,0,10,dataengineering
How do large social network clients track and store user journeys?,"Take instagram or twitter for example: how do analysts know if a user navigated to a post from their feed or by searching? I can imagine a data pipeline that sorts UI events on time and attempts to reconstruct the user journey that way. Or the client can keep track of every previous action that leads up to a UI event in a session. Probably several other ways to do this that I'm not thinking of... 

Are there any industry best practices on these or other approaches? Blog links or papers would be very useful as I'm having trouble coming up with anything on google",1572452375,4,0,10,dataengineering
Choosing the right Tools,"Hey everyone, I am wondering how you guys decide which tools to use for each use case. I am currently in this mess of Kafka vs. Kinesis. It seems like Kafka is the right way to go due to the freedom it gives you, but Kinesis just seems easier to set up. I am having a hard time deciding where my use case lands.

I want to send pc data to the stream -&gt; poll/pull from the stream -&gt; analyze -&gt; act on said data. A lot of this data would be published to a dashboard somewhere as well. Any advice?",1572286270,10,0,3,dataengineering
Data Integration with Apache Kafka and Attunity [Podcast],,1572279673,1,0,11,dataengineering
An interview about the Dagster framework and how you can use it to build testable and maintainable data applications,,1572271264,2,0,2,dataengineering
Faster ZIP Decompression,,1572249787,0,0,2,dataengineering
What does data engineering look like for Microsoft snow leopard,At what point in the process do data engineers make contribution and what does contribution look like?,1572148696,3,0,7,dataengineering
Imposter syndome for new/junior DE's,"Hi r/dataengineering

&amp;#x200B;

recently transitioned from a DA role into DE, heavily focusing on MS Stack &amp; Apache Spark (python flavour) 

I'm about a month in and I'm making progress but still feel out of my depth (especially when I started going through the process of making a DW)!. So my question is to the veterans who have gone through the whole beginner to expert process, how long did it take you to get over that initial feeling of being a noob.

Logically I know that for the first 6 months in *most* jobs that my productivity will most likely be a net negative number. 

would love to hear views from folk who have come from non-traditional backgrounds (cs/math/engineering)",1572120848,21,0,17,dataengineering
New DB internals book released,"Incase anyone is interested [https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347](https://www.amazon.com/Database-Internals-Deep-Distributed-Systems/dp/1492040347). Seems to cover some similar material to Data Intensive Applications, but I'm not sure.",1572108153,3,0,15,dataengineering
"Databricks introduces MLflow Model Registry, brings Delta Lake to Linux Foundation",,1572035091,3,0,18,dataengineering
Getting started in DE as a high school student?,"Hey all,

I'm a sophomore(10th grade) who is interested in technology. I've begun getting into programming and the general world surrounding the many different things you can do, and data engineering has caught my eye as an interesting career, given that it's about time to start thinking about the future. 

What could be some things to focus on to get started in a career? There seems to be a lot of different languages/technologies out there(R, Python, Scala, SQL, for technologies/software: Spark, Hadoop, AWS etc.), 

Personally, I'm thinking of starting out with learning some CS fundamentals with Python, then SQL and getting into and playing with databases, analyzing data, but from thereon out there is a lot more to branch out on. Also, I know this is intertwined a bit with data science(I don't like the term but I'm not sure what could be better used), so should statistics also be something to check out a bit?

Thanks!",1572018772,13,0,1,dataengineering
How to Migrate Self Managed Kafka to HDInsight via Gitops - Walkthrough,,1572015661,0,0,2,dataengineering
I want to become a data engineer from a completely non-quantitative background. Critique my plan?,"**About me:**
BSN grad internationally, 3.0 GPA, 29 yo, currently working as a medical coder for a large  hospital. My end goal is to become a data engineer in a hospital, working with medical data.

**What I’ve been doing:**

I’ve taken:

• Intro to CS from Udacity

• Programming Foundations in Python from Udacity

• Currently finishing SQL Bootcamp from Jose Portillo in Udemy (we are working on PostgreSQL right now)


**My plan A:**
• If accepted, continue working full-time and follow the 3.5 year plan of taking one course a semester, thereby minimizing student debt as much as possible.

• While in the program, work towards getting a Data Analyst position.

• After graduation, do the Data Engineering nanodegree from Udacity.


**My plan B:**

• Obtain as much Python and SQL skills from udemy then finish the DA nanodegree from Udacity

• Apply for DA jobs and for WGU’s MSDA program

• Finish MSDA and do Udacity’s DE nanodegree

Any thoughts?",1571991870,24,0,11,dataengineering
Switching from ms sql developer to data engineering,"  For last 10 years  I’ve been working with ms sql server database developer/admin, I like to switch now to data engineering role. 

I am looking at data engineering jobs on zip recruiter.

Can you give me any ideas on projects or books that I can read so I can find a new job. 

I’ve been learning so far about python and then will move to Linux scripting, I have a good background but I am not sure how to apply to the new role.

All my previous jobs have used Microsoft products.

Thanks",1571967784,12,0,7,dataengineering
Job lineage tool,I have a ton of ETL jobs with multiple dependencies. I have this data and I want to visualize it. What open source tools can I use for this?,1571958728,0,0,1,dataengineering
How to use SQL and Lenses Data API to explore Streaming Data in Apache Kafka,,1571936878,0,0,3,dataengineering
I would like your feedback on our data engineering homework assignment,[removed],1571903059,9,0,28,dataengineering
Trying to come up with an end to end framework / process from gathering business need to code shipped.,"Anyone have a process tool or any form of reference to implement this. Something that guides or links together user stories / an erd / GitHub , so one can effectively data engineer ?",1571894031,2,0,4,dataengineering
Is HAVING a redundant concept ?,"Is there a good argument to have \`HAVING\` as part of a SQL dialect ?

It seem to me that most SQL entities have the express role of:

a) Making an operation possible

b) Speeding up and operation

&amp;#x200B;

I don't see \`HAVING\` as having any of those roles.

a)

It seems to me like \`HAVING\` could in all cases be easily abstracted by two extra brackets and a select \*

E.g. \`SELECT COUNT(\*) as cnt, entitiy FROM table GROUP BY entitiy HAVING cnt &gt; x\` translates painlessly into \`SELECT \* FROM (SELECT COUNT(\*) as cnt, entitiy FROM table GROUP BY entitiy) WHERE cnt &gt; x\`.

Is there a situation where such a simplification would not work or would be to cumbersome to write.

&amp;#x200B;

b)

It seems like the optimizations that can be done via \`HAVING\` are minimal and would still be about as easy to implement in the case of only having \`WHERE\`.

&amp;#x200B;

Given a table that's something like:  \`amount, sender, receiver\` and a query of the form:

\`SELECT count(distinct(receiver)) as unique\_receiver, sum(amount) as total, COUNT(\*) as cnt, sender FROM table GROUP BY sender HAVING unique\_receiver &gt; 5 AND total &lt; 6000\`

I can think of two possible optimization:

1. When we are using \`&gt;\` on an expensive aggregated value (in this case \`COUNT + DISTINCT\`), we could use the already computed result of another \`&gt;=\`  aggregated value (in this case \`cnt\`) and eliminate certain rows. So if we figure out \`cnt\` is, say, 3 for a specific \`sender\` in the above query, we know \`unique\_receiver\` &lt;= 3, so we can skip computing it for that \`sender\` since it would be filtered out by the \`HAVING condition\`
2. When using \`&lt;\` the same logic could be used, but for a \`&lt;=\` aggreagted value that's cheaper to compute (though I can't think of a practical case where this might happen)
3. When using \`&lt;\`, given that any aggregated value has already reached a number bigger than what we are comparing it against, we can safely ignore the \`GROUP BY\` combination where this has happened, or finish the query and return nothing if no \`GROUP BY\` is present (in the case above, let's say \`total\` for an account reaches 6001 half way through iterating the rows for a given \`sender\`, when can now ignore any other rows with that specific value for \`sender\` and we can save memory by removing the existing row from the group by)

However, realistically speaking, the above optimizations require knowledge of aggregated metric complexity, when introducing check throughout the query code for certain conditions which would slow things down in most cases and in certain cases (e.g. for SUM or AVG) they only work if all the values in the column's being aggregated are unsigned, or maybe they could work otherwise, but in a reduced scope and with harder check or time spent constructing extra optimization metrics whilst computing the main metric.

Also, realistically speaking, I don't know of any database that actually implements something like this, I can't image that many queries benefiting from this optimization greatly and, if this optimization is being done, I see no reason for not implementing it in the multiple-SELECT scenario where \`HAVING\` is replaced by a \`WHERE\` on the query result.

So... how comes all SQL dialects seem to implement having ? 

Is there some magic behind it that I am missing ?

Is it just legacy standards that kinda stuck  ? (It's not like SQL language that implement it are all ANSI SQL compatible)",1571854892,11,0,1,dataengineering
Has anyone user Redis streams for realtime data processing /ingestion,[removed],1571778574,0,0,5,dataengineering
An interview about the emerging category of data orchestration platforms and how they can be used to bridge the gap between modern and legacy analytics systems,,1571746788,0,0,25,dataengineering
advice or best practices for running R/Python jobs and monitoring them?,"I hope r/dataengineering is the right place to ask this question.  
My small team and I (we're just three people) are running a couple of data dashboards / web applications (based on R Shiny) to provide various departments of our company with additional data insights.   
Our setup is very minimal and consists of a linux server that runs a postgres database, jobs and also hosts the applications via docker.   
our applications are decently optimized and our userbase is not that huge (currently around 20-30 concurrent users max., but we expect it to grow rapidly in the coming year), therefore performance is fine right now.

each project has various data sources that need to be processed in advance in order to query them from within our applications. This results in a lot of small jobs that produce different outcomes and log files. Since the number of processes is increasing rapidly, we're thinking about a suitable way to monitor them.  
our idea right now is to set up a new app that basically just processes the log files and shows an overview over each job with some basic info. 

since none of us has a real engineering background and we're also rather inexperienced in ""scaling up"" these data processes, we'd love to get some advice regarding our setup.

I'd like to narrow it down to these three questions:  
1. what is a good way to monitor these R / Python Scripts? Are there some tools that basically do what we're trying to achieve with our monitoring app?  
2. is our current approach of script based ""data pipelines"" a good practice in the long run, all of them running as cron jobs?  
3. do you have any recommendations for additional ressources to a better platform?",1571731142,8,0,12,dataengineering
Take charge of your data: Scan for sensitive data in just a few clicks | Google Cloud Blog,,1571712594,0,0,0,dataengineering
What are some must-know Spark functions?,"I know this is a pretty granular question, but what are some must-know Spark, specifically PySpark, functions for a data engineer? I need to brush up on it for an interview and after having not used it for a full year, I've admittedly forgotten a lot of it. And thinking about it, I didn't use it very extensively, so my knowledge about Spark is coming up short.

I can do I/O fairly easily, but I'm looking for data-wrangling functions that any data engineer should know. Not sure if I'll have access to documentation during the interview",1571695701,12,0,17,dataengineering
How is Apache NiFi?,"Im a junior DE who just started. Ive been tasked with picking an ETL and building out data pipelines.

Currently, i have a few cron jobs and some simple python scripts to ingest data.

I was considering airflow, but looking at how to install it is giving me a headache. Considering im the only DE, i was considering something like NiFi since its more straightforward, but id really like something python based.

What do you think? Is NiFi a better option?",1571689268,13,0,6,dataengineering
Faster ClickHouse Imports,,1571582427,2,0,8,dataengineering
Production Airflow cluster deployment,"My company is tossing around the idea of moving all data pipelines (60+ pipelines), which are all batch jobs, to Apache-Airflow. Anyone have any advice, gotchas, or “if I could start over, I would change...” related to deploying Airflow in production. 

Thanks for help!",1571530822,23,0,12,dataengineering
Chemist turned Data Engineer,"I am a PhD chemist that found themselves in a data engineering job. I was hired by a large lab to essentially build their data pipeline from scratch. The way the lab handles data right now is that everyone does their own thing in excel and all the data is stored in excel files or paper reports. The lab has grown to the point where they analyze so many samples that this takes them too long and its difficult for them to manage. The PIs want to grow the lab even more so it will eventually become even more unmanageble for them, that's where I'm suppose to come in. 

Here's the catch- I am the sole person who works on this. There is nobody here who does similar work. Even my supervisor doesn't really know much about what I do- I only see them every couple of days. My background is primarily in chemistry but I also have some background in CS, which is why I got this job. Though my CS background is mostly in terms of data analysis (data cleaning, wrangling, making visualizations, etc...basically things that were relevant to my chem work or data analyst internship I did) and a tiny bit of data science from a DS course I took in grad school. I don't have any experience building data pipelines. I didn't even know how to classify my job for a long time (my actual title is something really generic like ""scientist"") until I did some research and realized the type of work I do seems to fit data engineer/analyst the best.  

I've been in this job for about 2 months now, so far what I've been working on (as requested by my bosses) is making mini python apps (which mostly use pandas) for the lab staff to read in raw instrument data and make a report of final results for them or automatically fill out report templates they have. Basically, trying to get them away from doing dozens of copy-pastes in excel and also add some consistency in terms of format. 

I don't really know what is suppose to come after that. I'm looking for suggestions as to where I should start in terms learning that would be helpful for my job. I've read about lots of different software/packages like spark, airflow, etc, etc. 99% of things I've never heard of before. I'm not sure what would actually be useful for the kind of work I do. This isn't like a large company that produces millions of data so I feel like somethings might be overkill or not applicable for what they need. I also don't have the resources that a large data company would have. Basically, sometimes I feel like I am way over my head and not remotely qualified for this but I want to use this opportunity to learn and gain as much experience as I can. I took this job mostly because they strongly encourage taking time while at work for research/self learning (it is mostly a research lab after all)",1571495028,7,0,14,dataengineering
Tool to automatically aggregate / summarise data,"I'm a product manager and have access to some (read only) product usage tables which show the number of sessions and actions completed in various web products.

I have manually created scripts which aggregate these into another reporting DB which has simpler aggregated stats - e.g. number of sessions by month etc. 

Are there tools that would take care of this aggregation automatically? Or at least in a less manual way? It would have to be self hosted.

Thanks for any pointers!",1571473536,5,0,1,dataengineering
How realistic is it to build your own data glossary / catalog,,1571473455,16,0,4,dataengineering
"Science vs Engineering - Remote, Difficulty","Hello,  


In which specialty is it more common to find remote positions: Data Science or Data Engineering? Which lends itself better to remote work technically?  


Also, many people with non-technical backgrounds are retraining into Data Science careers. Do you think going straight into DE instead is harder or easier?  


Interested to learn your opinions. Thanks!",1571437721,15,0,16,dataengineering
Blog: 🚂 On Track with Apache Kafka: Building a Streaming ETL solution with Rail Data,,1571415524,0,0,5,dataengineering
Online Masters in Data Eng World?,Any suggestions for an online masters somewhere that could be done while working that would be a good fit for data engineering/ data architecture? Even just a solid computer sci/eng masters with a school that is known to have strong data-related courses?,1571407181,9,0,4,dataengineering
"Stuck between not enough data to justify HDFS, but need/want certain functions at a local level (ie. not going to the cloud) that appear only available in Hadoop. Any Atlas and Ranger data cataloging and security / auditing analogous systems for local FS, rather than HDFS? Advice?","Currently have situation between having not much data (mid single-digit to low double-digit TBs (not per data set mind you, but the average total amount)), but desiring tools similar to those that come bundled with hadoop packages, specifically Sqoop, Atlas, and Ranger (which can be easily installed via Ambari with Hortonworks).

Basically, it would cost less to get more drives to store data and RAM to process data right now than to store on HDFS and process with spark, etc than to pay for support from Hortonworks (and in this case not allowed to have HDP without paying for enterprise support), but would still like some way to catalog data sets for end/business user discovery and labeling as well as maintaining audited access to the filesystem. 

Essentially, thinking about how to have a standard data lake architecture like this

[ https:\/\/medium.com\/@rpradeepmenon\/demystifying-data-lake-architecture-30cf4ac8aa07 ](https://i.redd.it/zcjbox2cg9t31.png)

but with Hadoop components (mainly HDFS storage, Sqoop EL, Atlas data catalog, Ranger goverance, and Zeppelin data analysis) replaced with something cheaper and non-Hadoop-based that can still run locally on/across our servers. Most importantly is to have a Sqoop alternative (which we currently use to import 100M+ rows from 100+ column-wide tables each day via virtual tunnel to a remote Oracle DB).

Any general or platform/project/architecture recommendations for this situation?",1571388069,15,0,10,dataengineering
New to data mapping,What are some basics I should learn before interviewing for a data mapping position?,1571353602,3,0,1,dataengineering
Google Cloud 100 Gbps Dedicated Interconnect and HA VPN are GA | Google Cloud Blog,,1571343442,0,0,1,dataengineering
Introduction to NoSQL Databases (Apache Cassandra),"Hey all,

I recently completed a small project utilizing Apache Cassandra on a Docker Container as I wanted to gain an appreciation for NoSQL databases and their strengths and weaknesses compared to relational databases. I wanted to share my project and observations/thoughts within an article and I figured I would share the article with you all. I'm just now completing my first year as a data engineer (and as a programmer in the professional setting), so I'm really open to feedback on how I can improve my approach, code skills, and/or enhance this project. I do plan to dive into Spark next as I believe that my current workplace could really benefit from Spark, so I'll likely write up another article about that.

[Article](https://medium.com/@caitlin.ch.johnson/building-a-python-data-pipeline-to-apache-cassandra-on-a-docker-container-fc757fbfafdd?source=friends_link&amp;sk=a69746accea0cdb45c86000062075e01)

Thanks in advance for any feedback!",1571322210,0,0,3,dataengineering
Free sandbox for playing with Spark?,"Hi all,

I was wondering if anyone knows of a free (or free trial) of a sandbox type environment for playing with Spark.  At a conference I once had the chance to try Databricks which I really liked, but for idle playing around I'm not sure I can justify paying much for it.  It doesn't seem like there are too many free options out there but I thought I'd ask in case I am missing something!  I wouldn't mind paying a small fee for a month or so but really want to avoid subscription services.  Thank you!",1571267548,7,0,16,dataengineering
Do you regret migrating to GCP?,"I'm considering a migration from AWS to GCP.  Has anyone done so and regretted their decision?

I'm sure there are annoyances on both platforms.  Are there any I might be overlooking?  Any huge gaps in GCP offerings?",1571169615,39,0,13,dataengineering
What title would you give this job?,"I recently accepted a new position at my company where I will be doing a number of duties. This position is brand new so there is some wiggle room around a title name. I'm wondering if anyone has suggestions on what to call this?

Primary tools / languages:

R, Spark, SQL, Python, javascript.

&amp;#x200B;

What I'll be doing:

* Using my understanding of the many databases my company has to make recommendations around what data we can pull and what we could learn from it
* Develop ETL scripts to combine, clean, and normalize data from multiple internal sources
* Develop and maintain connections to said sources which include cloud storage (AWS, Google) as well as Oracle and MySQL db's 
* Build standardized datamarts and / or functions to quickly pull relevant data for new projects
* Occasional need to build / maintain Shiny web applications
* Assisting in building / validating ML models
* Parsing / mining free text fields using NPL techniques

&amp;#x200B;

Would this fall under a Data Analyst description or does the scope of this go beyond typical Data Analyst duties?",1571158552,6,0,1,dataengineering
Secure Self Service Kafka Portal w/ Namespaces+Top Security Features,,1571153411,0,0,5,dataengineering
An interview about Dataform and how it helps you to keep your data warehouse in good working order,,1571143979,1,0,13,dataengineering
AWS Athena UI - Issues,"People try drawing a comparison of AWS Athena with BigQuery, and usually, end up hating Athena's unpredictable and oftentimes broken UI and go for BigQuery. 

A few things I have noted so far:

* Sessions expire randomly and oftentimes an nginx stacktrace is shown to the user during the active session.
* Query's error stacktrace is generic (gives a similar error for a variety of different unrelated SQL/syntactical errors).
* Switching between tabs and query editor *randomly* erases your query which you were working in the tab.
* Tabs are incoherent: Sometimes query you were working on in tab 1 appears to be in tab 3 after you switch back from the history view.
* Oftentimes, on a misnamed column in query, gives out an error saying Query cancelled by user.  
 (are you serious??)
* The overall UX is so bad: clicking on a query string in a history view sometimes randomly asks you to open the query in a query editor tab, and sometimes doesn't even bother asking and simply navigates to the query editor.

A few other things too, but don't recall them right now. The question really is: Why can't Amazon work on fixing one of its primary products?",1571065076,5,0,7,dataengineering
How do I map a large set of unique values of job professions into categories?,"I am working with a data problem that involves predicting the income of the person.

One of the features of this dataset is 'Job Profession'. Now in this column of the profession, there are a set of distinct unique names. Here are some of the values --&gt; https://pastebin.com/rrD3P1Wr

Now I want to map these jobs into industries which they are representing to reduce the dimensions in my model. So can someone suggest me on how should I go about mapping them?",1570936372,0,0,1,dataengineering
What do you guys use for source to target mapping specs?,"Places I’ve worked have used excel to map source to target with logic. The issue is that the specs eventually get out of sync with the actual code. 

They just aren’t easy to maintain. Also,  everyone writes their specs a little differently so there is a lack of standards.

Wondering what’s being used at the places you work. 

Bonus question: what other common meta data do you guys create / maintain and how? Example: data flow diagrams in Visio, data models using Erwin or something..",1570934382,7,0,12,dataengineering
"How do you determine ""years of experience""?","Specifically in the case where previous professional experience is of little relevance to your current profession?

I'm a Data Engineer, but my first role after I graduated was in 2011 as an IT Consultant - basically ""the IT guy"" in the office that fixes printers, rolls out Windows updates and resets passwords and the like, but also writing the odd SQL statement to pull data out of databases and building the occasional dashboard. It wasn't until 2013 - almost two years to the day in joining - that I got promoted to a reporting analyst position and started doing this full time. It was still nothing too complicated, though, and there was certainly no data modelling or database design involved.

It wasn't really until 2014, when I got a job as a Data Architect, that I really started to do ""Data Engineer"" type things - building OLAP models, creating ETL pipelines etc. I even got to go on a course for it. 2016, when I changed jobs, was really a pivotal point for me as I started designing huge enterprise-level data warehouse solutions for top-tier clients, and all of the ETL processes and architecture that goes with that. I even started learning Python around this point.

With this in mind, how would I specify my ""years of experience""? From the start of my professional career (8 years), or from the start of my ""Data Engineering"" career?  (5 years)?",1570875895,7,0,8,dataengineering
Dataframes instead of a database?,"My main setup includes airflow for scheduling, Postgres for the data warehouse, sqitch for migrations, dbt for creating views (I literally select * from these views, dump the data to csv and stream it to our visualisation platform). All transformation is done with pandas or dbt. 

When I need to get things done quickly, I load the data into pandas, clean it up, and send it to our visualisation tool. From there, other teams create charts or merge the data with other data etc. From this point of view, our third party visualisation tool (Domo) is kind of the data warehouse and anything I have is the staging area. Since I am dealing with so much data from so many sources (many of which change frequently and without warning), I am beginning to feel like putting stuff in the database and managing schemas is just too much overhead - there have been so many times when I just wasn’t made aware of a proper unique constraint or new columns were added and I need to alter the table and backfill data. 

I have been testing just dumping the pure raw data in a container on azure blob, reading it with pandas and outputting a transformed version in parquet format in a different container. It seems to be quick and efficient but i am worried that I might be taking a step backwards that I might regret later. I really do like Postgres and dbt a lot too, but I suppose on azure blob I can eventually move to Spark and still query files. Plus the storage is essentially unlimited. 

Any thoughts?",1570850719,20,0,12,dataengineering
Top takeaways from Kafka Summit SF and industry trends,,1570718767,2,0,6,dataengineering
Why I Recommend My Clients NOT Use KSQL and Kafka Streams,,1570643697,0,0,16,dataengineering
News Recommendation Algorithm idea and implementation," 

Hi,

As I am still learning data science so there may be some mistakes in my implementation/ approach and I highly appreciate your opinions, points, suggestions, modifications.

I am working on a leading financial services company and for their premium users, I need to implement personalized news.

Through I have found out some idea how to approach the problem, I don't know whether my idea is actually the workable solution or not.

**Problem Statement**: Recommend user's news articles.

**Data**: user's personal information such as age, login time, portfolio information, topics that users have high affinity to. ( this is still WIP still suggestions on data would be beneficial)

**Solution**:

Steps

1. ***Finding readers with similar interests:***

As we have mixed data we will be using [hierarchical clustering algorithm using the Gower](https://stats.stackexchange.com/a/15313/260511) similarity coefficient.

It will help to group similar users who have similar tastes.

**2.** ***Topic modeling***

To understand news articles we will use the [LDA algorithm](https://blog.insightdatascience.com/news4u-recommend-stories-based-on-collaborative-reader-behavior-9b049b6724c4) to identify topics in news.

We can also use it to understand how much a news article is relevant to the topic.

To identify the diversity of topics I will use Jaccard similarity. as we do not want many topics and similar topics.

For mapping news with users, we will use the number of clicks of the user on particular news containing the topic.

**3.** ***Recommendation***

As we have a Group of clusters which has information about their affinity toward the topic,

we can find out similar news article which is similar to the topic using [cosine similarity](https://www.machinelearningplus.com/nlp/cosine-similarity/) and recommend that article to the user group.

Again any doubt/ change/ modifications are appreciated.

Also if you think it is the right solution then please highlight.",1570626994,4,0,2,dataengineering
People who work in financial service companies: data engineer. What's your organization pain points?,"Hi, everyone.

I aim to apply for this job as a data engineer. ( [Link](https://careers.bloomberg.com/job/https://careers.bloomberg.com/detail/74658)) however, I'm pretty sure my skills are lacking; Because I had one year in MSc Business Analytics, majorly focusing on machine learning and data modelling; and I'm pretty bad at it.   


Preparing more can make me confident, I reckon. So, for this job,  I need to learn and fill the gaps. Having done research, I found a data engineer course in udacity; not sure whether it can give the skills I need.  


What're you guys thoughts? Can anyone who works in similar companies share your daily activity?  


Any advice would be super helpful!!!  


Thanks!",1570617908,5,0,2,dataengineering
Data Profiling Daily Files with Python. How do you store that information for trends and alerts of anomalies?,"We get a ton of files everyday and I’d like to perform data profiling and storing all that information for reporting, tracking trends and sending alerts when there are anomalies or schema changes. 

Is there a best practice of doing this? This is important because every full moon we would get some bad files and process it, which creates a lot of time spent researching and potentially backtracking things in our databases. 

I’d like to do this in Python if possible? Our files are typically less than 1gb so I’d imagine Pandas and other “Non” Big Data technologies would be sufficient.",1570593364,5,0,10,dataengineering
An interview about the architecture of Rockset and how they built a serverless platform for fast and flexible analytics on your semi-structured data,,1570560096,0,0,8,dataengineering
"Manage, debug &amp; secure Debezium-&gt;Snowflake flows via Kafka with Lenses.io. Hands on",,1570549065,0,0,4,dataengineering
Any Data Engineering Tutors ? I just promoted to a job as a DE from an analyst role at my company and head is spinning. Want to get ahead and impress leadership.,"I tried the Wyzant but it had no tutors. Really want to brush up on making api calls from source data ( marketo or salesforce ), staging to AWS S3  Data lake ( via ec2) and then elt into snowflake dwh. 

Goal to understand AWS and Snowflake and how to tie it all together with python and sql.",1570502934,3,0,4,dataengineering
Any good ways to practice python for data engineering ?,I guess to say if I wanted to practice python for data analysis I would load spreadsheets into python and clean csv files via pandas and create  new columns that are derived from calculations.,1570500742,2,0,5,dataengineering
"How is Python applied to database management, optimizing processes, and cleaning data?",I just started learning Python and want to learn how to apply it to data engineering. How can Python 3 be applied to data quality or DB management or optimizing/automating processes? Is there any specific code from Python I should learn regarding this? Does anyone have any way for me to practice this type of scripting? I don't want to be a developer or anything. Any help would be great thanks.,1570493705,14,0,17,dataengineering
Setting up Airflow on Azure &amp; connecting to MS SQL Server,,1570482575,0,0,1,dataengineering
What are some good practice projects for data engineering?,,1570467402,20,0,12,dataengineering
Graduate going for DE interview,[removed],1570464465,2,0,2,dataengineering
How do you get started and get work as freelance/consulting Data Engineer?,,1570439352,9,0,6,dataengineering
California Consumer Privacy Act CCPA could cost companies $55 billion,,1570385350,6,0,19,dataengineering
Unsure what salary to ask for,"I have an interview for a Data Engineering position on Tuesday with a start up company. I have 2 years of experience as a data analyst and 2 years as a data architect. Online it says I am applying to be the first data engineer of their company. I don’t know how much to ask for. Or even interview questions because there is no info on this company online anywhere like Glassdoor. 

Is 95K a good offer? Is it too high for a start up? Is it too low? It says salary is dependent on experience. They say I should have strong SQL and Python skills including data modeling and develop and maintain architecture, automate data, and use Python to process data files.",1570334374,5,0,0,dataengineering
Web developer or data engineer — which occupation has better long-term prospects?,"which one will be more ""future-proof""


which one has the better work/life balance

which one can help you transition to higher-paying jobs",1570277861,19,0,17,dataengineering
Raspberry Pi Data Eng Projects?,I have a largely unused Raspberry Pi kicking around that I’d like to do something with.. any ideas for projects that could use the hardware with some data engineering related tasks?,1570203761,13,0,13,dataengineering
"Junior data engineer with no/little technical background, looking for guidence",[removed],1570188438,2,0,1,dataengineering
How often do you get given a schema for your sources?,Just interested really how often when you get a data source you also get a schema and how is it defined? I have seen swagger being used but wondered what other people get?,1570186962,6,0,3,dataengineering
Data Analytics in E-Sports – Future Prospects – Jobs – Everything You Should Know,,1570174904,0,0,18,dataengineering
"Noob question: when people say ""learn SQL"", which one are they talking about? MySQL or Postgresql or is it something else?",Or is SQL different from MySQL/Postgresql?,1570172999,6,0,4,dataengineering
ETL Whiteboard Interview Tips?,"Hi all,

I have an interview coming up next week for a job I am really excited about -- data engineer at a major company in the Bay.   Part of the interview is whiteboarding an ETL case -- they'll give source and target, and want me to write an  ETL (I choose Spark) to get the data from point A to point B (I am assuming it will be OLTP -&gt; OLAP type stuff -- maybe aggregating some kind of metric).

Basically, I have been feeling really overwhelmed trying to prepare for this as I don't have an easy way of verifying whether the nonsense I am coming up with is correct.  Do any of you have any tips for whiteboarding ETL?  Or some resources I can take a look at -- I would love to find some solid tutorials that I can use as a baseline, but so far I haven't had a ton of luck.  Any advice would be really appreciated!

For background, I'm a data engineer at my current role by name, but I mostly end up doing something closer to software dev work (2 years of experience, non-CS but technical major).  I have written some Spark streaming jobs which essentially take in Kafka messages, add a few calculations and put them into HIVE tables.  Our data volume is so low, I have never really run up into the kind of volume issues that I am expecting to be quizzed around.

Thank you so much!",1570154323,7,0,12,dataengineering
Big Data DevOps Engineer,"I see lot of company posting DevOps Job that requiring Data engineer backround, is that even a thing ? Data DevOps ?",1570143697,11,0,7,dataengineering
Snowflake vs redshift?,Anyone have experience with these two data warehouses? What are some pros and cons of the two?,1570132519,0,0,1,dataengineering
Running a scraping platform at Google Cloud for as little as US$ 0.05/month,,1570129285,0,0,1,dataengineering
What does your data intake look like?,"I've been working in my company's data engineering department before data engineering was a thing, and most of our pipeline slowly converged to industry standards, but our log capture process isn't something I've seen in other companies. We actually connect to each web server and copy the rotating log file via SCP. This decision was made long before projects like Flume and Kafka were available, and it stuck.

While this makes autoscaling trickier, it makes batch processing much simpler. We don't have to worry about waiting for all data to arrive for a given hour, or records arriving late, or duplicates. We just upload everything to a bucket in S3, run a Pig script, and our data warehouse is updated. So I realize I have this knowledge gap. How do you handle duplicates or out-of-order records in your ETL?",1570120585,4,0,1,dataengineering
How can I utilise my current role to set myself up for a data engineering role?,"I am currently a data/BI reporting analyst and want to move into data engineering.

My current role consists of the following:

* Understand reporting requirements from whoever (finance, marketing, operations etc)
* Extracting the data from SQL database (MS SQL Server, MySQL, Redshift)
* Load into Tableau and build the report
* Repeat

What I've been doing lately:

* Data from different sources e.g. sql databases, flat file CSVs need cleaning/joining/blending/aggregating so I use Python/Pandas to do this sort of stuff then export a csv to be used in Tableau
* Automating this via cron jobs (looking to schedule these scripts into an Airflow workflow?)
* Loading some of that data into a MySQL database which I've got write-access to (perhaps loading data into a staging db and then a final db using a star/snowflake schema?)
* Thinking to rewrite Python scripts from Pandas to PySpark to get some experience in this (not sure if it's a good idea as Pandas seem to be doing the job but hear that data engineers use Spark more than Pandas)

Limitations:

* No write access to Redshift data warehouse (because it's the data warehouse I'm only allowed to read not write)
* Not allowed a dev server to run scripts on because company architecture does not support Python (is that a thing? Assume it is)
* Hosting a db on AWS is expensive therefore not allowed
* Current MySQL which I have write access to is version 5.something that doesn't support window functions or CTEs

Question is, what can I do, and what is the best route to take, in order to spice up my current role in order to set myself up for at least being considered for a data engineer role?

My thoughts are:

* Schedule all scripts via Airflow
* Convert Python scripts from using Pandas to PySpark
* Extract all the data I need for my Tableau reports and then some from the different data sources and drop them into the MySQL db (I have a few tables in here anyways so would be nice to get them all in one place. Only problem, again, is that it doesn't support things like window functions or CTEs)

What are your thoughts? Brutal/savage honesty is more than welcome",1570120094,14,0,12,dataengineering
I get JSON files dumped into an S3 bucket periodically and need to load this data into Redshift. How do I go about building this pipeline?,"Hello everyone, 

So as the title says, I get JSON files dumped into S3 frequently and need to load this data into Redshift on a near-realtime basis.
How do I go about this? I have come up with this approach -

1. Publish new files arriving into bucket to an SQS queue using S3 Event notifications. 
2. Write an always running python script that reads from the SQS queue, transforms the data and loads it into Redshift 
3. Put all this into a Docker container and run the container on an EC2 instance.

Would this work out? Also had a couple of questions - 

1. How do monitor and react to failures in the pipeline, do I have to set up my own notification system? If there are failures when processing files, how do I rerun the transformations for the failed messages again?
2. Is there an easy way to ""backfill"" the data? If already have files sitting in S3, is there a way to load them into Redshift without writing a custom script to do this one-time activity?
3. If the volume of the data that comes into S3 increases (and it most likely will), how do I scale this solution? Do I just spin up more containers? Or handle it in python using multi-processing?

Thanks for taking the time to answer the questions in advance!",1570119438,13,0,4,dataengineering
Data engineer plan (revision needed),"This is an updated plan from the ideas shared [on this thread](https://www.reddit.com/r/dataengineering/comments/da0dqt/rate_my_data_engineer_study_plan/)


Once again, what skills would you add/remove? How would you order it?

1. SQL
2. ETL
3. Python
4. Bash
5. AWS 
6. Spark
7. Docker


And realistically, how much of the above would I need to know to land a data-engineering position? I am an MIS undergrad student who already knows some SQL, data structures &amp; algorithms, OOP, and some basic python.",1570086066,25,0,17,dataengineering
Snowflake COPY INTO command - Purge not working.,"Per the title, I'm using an Azure-hosted Matillion instance to copy files from Blob storage.  (Or rather, a single file).

This file needs to be deleted upon successful completion of my ETL process.  I have the PURGE option set to True, but when I run the job it won't delete the file that it ingested originally.  

Any ideas what I'm doing wrong here and/or what I need to check or change?

Thanks",1570050831,9,0,3,dataengineering
Free/Open Source Data Lineage Tool?,"Are there any solid options for free data governance tools that ideally include metadata management, data lineage viewers, data catalog, etc.

Looking to build a proof of value for an internal company demo.",1570047882,4,0,13,dataengineering
Here’s What Makes Apache Flink scale,,1570042888,0,0,1,dataengineering
Technical Questions I Could Expect?,"Do you guys have examples of interview questions you receive for a job with these requirements:

Usage of AWS for data-warehousing and processing (the following are relevant Redshift, S3, RDS / Aurora, Lambda, SQS, Kinesis)

Scripting tasks using Python (strongly preferred) 

Complex data structure, data processing, data quality and data lifecycle.

Operational management / DBA of DBMS servers (Oracle, MySQL or MSSQL)

Data platforms and data modeling frameworks.

When I applied, the job specified two years hands on experience with the above, but I'm actually a new grad.  They said they're open to junior positions for this role.  AWS is a big part, I was just wondering what sort of questions I would receive?",1570028501,3,0,9,dataengineering
Domain driven data architecture,"https://martinfowler.com/articles/data-monolith-to-mesh.html

Hey everyone I’m curious what are thoughts, compliments, or criticisms on this article. It looks like the idea is to decouple and abstract data processing steps and data itself. 

What are the downsides to this approach over a traditional data lake?",1569975996,1,0,6,dataengineering
Spark error when unpacking RDD items,"I wrote a script on Jupyter notebook to read a sequence file and perform operations. The script works fine on Jupyter. 

&amp;#x200B;

        def acct_tuple(x):
            return (x[0], json.loads(x[1])['coverage_info'])
        
        def flat_map(x):
            return x
        
            def get_tuple(x):
            return {'acct_num': x[0], **x[1]}
        
            def get_relevant_fields(x):
            return (x['acct_num'], x['cov_leg_dt'], x['cov_leg_end_dt'], x['rest_list'] if 'rest_list' in x.keys() else [], x['loc_list'] if 'loc_list' in x.keys() else [], x['pls_list'] if 'pls_list' in x.keys() else [])
        
            def map_tuple(x):
            return (x[0] ,)
        
            def flatten_map(record):
            # Unpack items
            _, cov_leg_dt, cov_leg_end_dt, items, [line], [pls] = record
            line_cd = line[""line_code""]
            ply_id = line[""policy_id""]
            company_cd = pls[""pls_company_cd""]
            for item in items:
                yield (ply_id, cov_leg_dt, cov_leg_end_dt, item[""set_coverage_cd""], item[""set_pt_state_cd""], item[""set_limit1_amt""], item[""set_limit2_amt""], item[""set_limit3_amt""],  item[""set_limit4_amt""], line_cd, company_cd),1
        
        	
        rdd = spark.sparkContext.sequenceFile(input_directory)
        rdd = rdd.map(acct_tuple)
        rdd = rdd.flatMapValues(flat_map)
        rdd = rdd.map(get_tuple)
        rdd = rdd.map(get_relevant_fields)
        rdd = rdd.flatMap(flatten_map)
        rdd = rdd.map(map_tuple)
    

However, when converting to a Python script, I get an error:

&amp;#x200B;

2019-10-01 14:12:46,901:ERROR: \_, cov\_leg\_dt, cov\_leg\_end\_dt, items, \[line\], \[pls\] = record 

2019-10-01 14:12:46,901:ERROR:ValueError: not enough values to unpack (expected 1, got 0)

&amp;#x200B;

Any suggestions? I can post a sample RDD if that helps but I believe this is due to a structural difference between writing a script on Jupyter and a .py file?",1569960620,5,0,0,dataengineering
Trying to understand part of Pyspark code,"I'm working on a Pyspark project and I'm trying to understand the last part of the code below.

what is     

    .map(lambda x: x[0] ,)) 

doing here?  Is it taking the first element and adding an empty string?

&amp;#x200B;

    result = (full_rdd_group_val
        # Flatten tuples
        .flatMap(flatten_map)
        .map(lambda x: x[0] ,))",1569950996,0,0,1,dataengineering
Project Ideas to develop skills and understanding?,"I am a beginner in the field of DE and I wanted to know what are some projects that one could do to develop their skills and maybe add to their resume.

Would a simple ETL like extracting data from an API and transforming or aggregating it and storing it on an RDBMS(MySQL,postgreSQL)  count as a task or a project?

Thank you very much for your help and suggestions.",1569949343,12,0,13,dataengineering
An interview about how the open source Kedro framework makes it faster and easier to build your end-to-end data pipeline for machine learning projects,,1569948915,0,0,3,dataengineering
[Airflow] Shared resource across DAGs? Similar but opposite of a pool,"I am designing a process that will have multiple DAGs. Each DAG can have a branch where it is dependent on something running on an ec2 in AWS. This ec2 process has a long setup and teardown time, but low run time. So ideally I would like each branch to be queued until they're all ready to run, start the ec2 once, run for each DAG, and teardown once.

I thought about creating these as sub-DAGs, but ideally I want to be able to preserve history runs so that I can more easily identify a problem if one arises.

Is sharing a resource like this across DAGs even possible? To me it seems like the opposite of a pool. Rather than lessening concurrency to a connection, I want to increase it.",1569947841,8,0,1,dataengineering
Constraints in Conceptual Data Modeling,"Hello all, how or where do we put constraints in a data model? For example using the image attached, how can we specify that a person cannot be married to himself or herself? do we add the constraint as a statement or there is a way to update this model to show that constraint?

Thanks!

https://i.redd.it/qmzzxj441up31.png",1569893387,2,0,1,dataengineering
Anyone here looking to mentor a newbie?,"I'm a software developer getting into the world of data and would love to have someone who is more experienced then me to chat with. Some of the things I'm currently exploring are Docker, Airflow, Postgres and data warehousing / data modelling.  I know the odds are low but wanted to ask anyways.",1569888815,35,0,16,dataengineering
What are your best practices around AWS Lambda?,"Hi All,

Going through architectural possibilities and trying to work out how AWS Lambda's will fit in.

We use them for small jobs and Airflow/Batch for bigger jobs, I love and hate Lambda at the same time but maybe there are ways to minimise the pain points.

In particular, I'm interested in how you support:

* Scheduling
* Monitoring / Retries / Etc
* Deploying
* Orchestration / Dependency Management

This is all nicely bundled up with Airflow and I can keep it all in version control for the Batch jobs, but the Lambda's are starting to get out of control. Particularly because we need to custom compile on a version of Amazon Linux (pandas/numpy).

We loosely have step functions for dependencies, cloudwatch rules for scheduling, I could ingest the cloudwatch logs for monitoring (haven'y investigated SAM yet) but it all feels a little messy and makes me want to just throw everything in Airflow/Batch.

Thanks!",1569882501,5,0,7,dataengineering
Ingesting database with python [help],"Hello fellows. I need help on creating a data pipeline in Python.

I need to move data from a TEIID service to an staging area on a SQL Server DB. The data volume is pretty big, there are tables with 200+ million rows. What are the best ways to do this?

For writing the date into SQL Server I'm trying to use the ctds driver ([https://github.com/zillow/ctds](https://github.com/zillow/ctds)) which has a Bulk Insert ([https://zillow.github.io/ctds/bulk\_insert.html](https://zillow.github.io/ctds/bulk_insert.html)) option which apparently can be used with in-memory data, avoiding disk I/O. But to use it, I need to explicitly wrap textual data with ctds.SqlVarChar (for CHAR, VARCHAR or TEXT columns) or ctds.SqlNVarChar (for NCHAR, NVARCHAR or NTEXT columns) due to FreeTDS not encoding data on the client side.

For reading the data from the TEIID I'm using a hacky to use SQLAlchemy ([https://github.com/kafran/sqlalchemy-teiid](https://github.com/kafran/sqlalchemy-teiid)). It's also possible to use psycopg2. Using psycopg2 I can fetchmany results and iterate over the rows to encapsulate the data with the ctds data types before send it to Bulk Insert. But I'm using the SQLAlchemy approach to use pandas to consume the data and pass the data to pd.to\_sql(method=bulk\_method) ([https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to\_sql.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html)).

Pandas to\_sql pass 4 parameters to the bulk\_method: the pd\_table, the connection, a list with columns (key) and a zip with row tupples. Right now I'm stuck constructing the bulk\_method to iterate over the data to send it to ctds.connection.bulk\_insert; Can someone illuminate me? As I'm pretty new on this, I need help manipulating these Data Structures.",1569874459,12,0,3,dataengineering
DataOps on Apache Kafka - AWS MSK with Lenses.io,,1569863817,0,0,1,dataengineering
Data Engineer's role in data privacy and security.,"For those of you working in industry as a data engineer, does any of the responsibility of privacy and security fall under your role?   


For example if you're building out APIs, do you also have to implement the authentication and authorization to protect access? If you're collecting or using sensitive data, is it your responsibility to implement measures to anonymize the data, or even implementing auditing functionalities to prove you are following regulations?

Excited to hear your experiences!",1569778347,2,0,5,dataengineering
"Spark, Hadoop, Hive etc?","Hey guys, I have been researching tools like the ones mentioned in the title but im having trouble visualizing what they could be used for. I was wondering if someone could give me a use case so to speak of why you use X technology to solve Y problem. Sorry if this post is not allowed I am pretty new to Data Science and Data engineering",1569775906,9,0,18,dataengineering
"As a Masters student with almost 2 years of exp, Should I even apply to DE jobs with 4+ min. yrs of exp. requirement?","I keep seeing roles for which I have matching skills but I lack the exp. they want. Should I apply to these roles in hopes of:

1. They somehow shortlist me even though I lack the exp and I pass the interviews? \[i.e. make an exception\]
2. They have another opening for someone with my experience?

&amp;#x200B;

Any other job hunting tips would be welcome. I am currently using LinkedIn as just one job portal is a lot of work to apply along.",1569670093,3,0,1,dataengineering
Interview for Data Science Engineer,"I'm doing a job switch. I have had the experience of being an ETL developer and worked on PySpark . Apart from that have done some side projects in Hadoop and MapReduce. I am attending an online test in 2 days for the role of Data Science Engineer (where they expect me to have an experience in DE but some knowledge in DS). They have not disclosed anything on what skills will be tested. To be prepared for it, What skills would you recommend me to brush up before the test.",1569668901,9,0,7,dataengineering
"DBT, Presto, Minio","Hey everyone, I'm curious if anyone has used DBT with Presto using Minio. 

I'm wondering if it makes sense to use these tools together. I know that Presto and Minio have a docker image and that Presto is partially supported by DBT. Would it make sense to try to add DBT on top of Presto and Minio to do the T in ELT?",1569603575,7,0,1,dataengineering
Few basic things every data engineer should know about,"Hi all!

I will be starting in Data science team as a Data engineer. I am a recent CS grad. I have been coding in scala for a couple of years now and switching the job for the first time! I have done a fair bit of Python programming but have not written any production level python code.

I would really appreciate it if someone can guide a few basic things a data engineer should. I know this is a very broad question and might seem somewhat vague.

But assume if you were in my team and what are things you expect me to know once I join your team.

Any suggestions are welcome! Thanks",1569603532,3,0,4,dataengineering
Rate my data engineer study plan,"1. Python (I've already started learning this)
2. SQL
3. Bash scripting
4. Cloud (AWS)
5. Hadoop
6. Mapreduce
7. Hive

What would you add/remove and is it possible to learn this in 1.5 years (im currently a MIS junior undergrad)?",1569590753,32,0,22,dataengineering
How to build real-time SQL dashboards on event data from Kafka,[https://www.confluent.io/blog/analytics-with-apache-kafka-and-rockset](https://www.confluent.io/blog/analytics-with-apache-kafka-and-rockset),1569535613,5,0,8,dataengineering
"New user management with namespaces, improved SQL, alert integration plugins and lots more! All new Lenses 3.0 features to make building &amp; managing streaming data flows on Apache Kafka and Kubernetes a walk in the park. Read our release blog now!",,1569509127,0,0,1,dataengineering
Moving csv from S3 to RDS MySQL using Airflow?,[removed],1569491947,0,0,1,dataengineering
Kafka Data Pipelines with Robin Moffatt - Software Engineering Daily,,1569491618,0,0,11,dataengineering
ArangoML Pipeline – A Common Metadata Layer for Machine Learning Pipelines,,1569412299,0,0,1,dataengineering
AWS and Postgres,[removed],1569405041,1,0,1,dataengineering
Pyspark Flatten RDD error,"I'm trying to flatten data in an RDD. The RDD is structured as a 4-tuple with the first element, primary\_id and the second element of the tuple a list of list of dictionaries, third and fourth elements a single list of dictionaries.

&amp;#x200B;

         rdd=   [('xxxxx99', [{'cov_id':Q, 'cov_cd':'100','cov_amt':'100', 'cov_state':'AZ'},
                          {'cov_id':Q, 'cov_cd':'33','cov_amt':'200}, 'cov_state':'AZ'},
                          {'cov_id':Q, 'cov_cd':'64','cov_amt':'10'}, 'cov_state':'AZ'},
                          [{'pol_cat_id':'234','pol_dt':'20100220'}],
                          [{'qor_pol_id':'23492','qor_cd':'30'}]),
        
             ('xxxxx86', [{'cov_id':R, 'cov_cd':'20','cov_amt':'100, 'cov_state':'TX'},
                          {'cov_id':R, 'cov_cd':'44','cov_amt':'500, 'cov_state':'TX'},
                          {'cov_id':R, 'cov_cd':'66','cov_amt':'50', 'cov_state':'TX'},
                          [{'pol_cat_id':'532','pol_dt':'20091020'}],
                          [{'qor_pol_id':'49320','qor_cd':'21'}]) ]

&amp;#x200B;

I want to flatten the data so that it appears in the format

&amp;#x200B;

https://i.redd.it/ge3a1qoeloo31.png

how would I do this in Pyspark?

&amp;#x200B;

Here is what I have attempted but this gives me an error: Too many tuples to unpack

&amp;#x200B;

&amp;#x200B;

        def flatten_map(record):
            try:
                yield(record)
                # Unpack items
                id, items, line, pls = record
                pol_id = pls[""pol_cat_id""]
                pol_dt = pls[""pol_dt""]
                qor_id = pls[""qor_pol_id""]
                for item in items:
                    yield (id,item[""cov_id""],item[""cov_cd""], item[""cov_amt""], item[""cov_state""], pol_id, pol_dt, qor_id), 1
            except Exception as e:
                pass
        
         result = (rdd
            # Expand data
            .flatMap(flatten_map)
            # Flatten tuples
            .map(lambda x: x[0] + (x[1], )))

&amp;#x200B;

I can post the complete error if required but for sake of brevity,

&amp;#x200B;

        ValueError: too many values to unpack (expected 2)",1569391662,3,0,2,dataengineering
What would make more sense? Creating an ETL pipeline for historical data first and then applying that foundation to current streaming data or work on a pipeline for current streaming data first and then working on backfilling historical data?,"I am thinking of a personal project to enhance my learning but not sure where I should start. I can download a bunch of historical data and figure out how I would like to modify it before storing it or I can work with a current data stream and do the same but I keep going back and forth between which route would make more sense. It seems as if in both cases the step I do second would benefit just from having figured out how the transform phase would be applied but I'm not sure if there are any pitfalls or benefits I am not thinking of. 

If anyone could offer any guidance I would greatly appreciate it!",1569348578,13,0,12,dataengineering
AWS MSK with Lenses.io - The Perfect Combo | Read why Lenses is the right DataOps platform for AWS MSK,,1569339478,0,0,0,dataengineering
AWS MSK with Lenses.io - The Perfect Combo,[removed],1569336758,0,0,1,dataengineering
An interview on the open source MinIO platform for fast and flexible object storage for data intensive applications and analytics that runs everywhere,,1569322111,0,0,1,dataengineering
Step by step tutorial how to collect Google Analytic events to your own cheap data warehouse on AWS with Snowplow,,1569283780,0,0,0,dataengineering
The Machine Learning Data Science Path,"I've created a [blog post](https://kamwithk.github.io/path.html#path) detailing different courses, books and places people can learn about data science/machine learning from.

It    categorizes the sources, and gives details on the main differences    between them to help decide whether the course is right for you. Make    sure to take a look:

[https://kamwithk.github.io/path.html#path](https://kamwithk.github.io/path.html#path)

Any feedback would be greatly appreciated!

I do keep this updated on my [twitter account](https://twitter.com/kamwithk_)!",1569282755,0,0,4,dataengineering
Need help backing out gracefully (or not).,[removed],1569256200,0,0,1,dataengineering
Effectively Managing your PostgreSQL Data | Full Tutorial,,1569202678,0,0,3,dataengineering
"Started my first job as a Data Engineer, comming from a software engineering background. Got a few questions.","Hey,  
I just started my first job after university as a junior data engineer. My company uses sap data services to manage their classical data warehouse. In addition to a bigger hadoop cluster for more specific use cases.  
**I am very keen on agile methodologies and software engineering principles**. And pretty excited to work with hadoop, writing jobs in scala and python.

But I do have a lot of questions regarding the more legacy data services system. For example: **How do you test the pipelines you write/develop?** **How do I implement a useful continuous integration pipline? Is it possible at all?** And without being able to use git as your vcs aren't you lacking a lot of useful development features?

I know that developing software worked before people got used to TDD etc. but it does definitely have advantages to work with more confidence in the code and have a continous integration pipeline especially once the development team gets a little bigger. **So does data warehousing with ETL Tools just not care about those advantages? Or is the way of developing via UI just so fast and robust enough that it doesn't matter?**

Not really excited about clicking my programs together tbh, but I guess I'll get used to it for now. Still would like to know how bad it is to work without testing etc.",1569114837,0,0,1,dataengineering
Why we chose Apache Spark for ETL,[removed],1569050445,0,0,1,dataengineering
Airflow implementation into a new small project,"Hi Guys,

I'm trying to incorporate Airflow into my project. My main goal is to learn Airflow while developing the project further. Currently, I've developed a pipeline where I take a CSV file and upload it into Mysql. 

Now I'd like to add Airflow on top of it. So my goal is to execute the pipeline using Airflow. At this moment, I execute it manually.  

One way of implementing it I think would be putting my project code under AIRFLOW\_HOME directory and then from a new python module under dags folder import the pipeline module and then with the Airflow operator to execute the pipeline. Haven't tried it yet, but think going this direction.  Is that the right direction ? I'm quite new to Airflow, so any ideas appreciated.",1569012241,4,0,5,dataengineering
Why we chose Apache Spark for ETL (Extract-Transform-Load),"I recently published my article Why we chose Apache Spark for ETL (Extract-Transform-Load) - http://bit.ly/30AUh70, on
   
 Medium
   
 . Have a look.",1568986557,0,0,1,dataengineering
Top 5 requirements for AWS-Native data pipelines (video). What do you think? Agree?,[removed],1568933779,0,0,1,dataengineering
Help Transitioning from SQL Server to Airflow and Structured Pipeline Systems,[removed],1568929015,0,0,1,dataengineering
New Data Engineer,"Hey guys, I work at a small company and I have been tasked to build out our ML pipeline and data pipeline from scratch. Literally nothing exists right now, we will be handling a lot of logs and the goal would be to create something that is as close to real time as possible? (if this is possible?) So im thinking something like streams (Kafka) but was wondering if any of you had any advice or resources that would help me out.

It would be greatly appreciated.",1568901848,20,0,15,dataengineering
Workday Pipeline,"Hi, I'm a newbie working with HR data in Workday, and was wondering if anyone's had any experience on constructing a data pipeline to move this data out of Workday and prepare it for analytics (probably in Python). The data doesn't appear to be too big, so big data tools are \*probably\* not necessary.

Any suggestions on where to start would be greatly appreciated!",1568898408,6,0,2,dataengineering
Transitioning to Data Engineering as a Data Analyst,"Hi everyone! I recently graduated with a B.S. in Mathematics and a minor in Computer Science and took a job with a company as a data analyst for their IT department.  I've always envisioned myself working a few years as an analyst and then transitioning into a data science role, but over the last month or so, I've begun second-guessing myself as I've seen more and more articles about the increasing difficulty of landing a job in data science as there seems to be a flood of candidates with professional degrees in Math/Business Analytics/Comp Sci/etc.  That said, I am very interested in maybe focusing more on building my skillset around data engineering concept.  I read Jesse Anderson's book on switching careers to Big Data but still have a few questions and was wondering if you guys might be able to help...

1. With my computer science minor, I learned C++ and feel as if I have a pretty strong coding foundation.  I'm working to get myself familiarized with Python, but would you guys recommend any other languages that I should be learning?

2. In terms of data engineering tools specifically, I have a good SQL background in terms of querying and took a class in RDBMS so I have a basic understanding of database architecture.  I've read quite a few articles about necessary tools in data engineering but each one seems to recommend prioritizing some over the others.  For those working in the field now, what do you feel are the most important to learn, whether it be Hadoop, Spark, Hive, Kafka, cloud technologies, etc.?

3. What avenues did you find the most helpful and worthwhile in your learning process (textbooks, online courses, etc.)?  Are there any specifically that you would recommend (definitely interested in purchasing or renting a few of the O'Reilly books)?

4. I understand that one of the best ways to build up my skills would be to perform independent projects, and I currently have a goal of creating a pipeline for horse racing data online and then modeling based on that data.  However, I know that doing an independent project for Big Data is a lot harder, so how would you recommend I go about getting practical experience in working with big data or performing an independent project in that regard?

5. When did you first feel comfortable applying for data engineering positions if you were self-taught in data engineering principles?

6. Is there anything you wish you would have known or done in your own data engineering learning process or have any other recommendations for someone in my position?

I know that's a lot of questions, so please don't feel like you have to answer all of them.  However, I really do appreciate any and all help!",1568858708,5,0,16,dataengineering
Searching for Cloud Architect and Engineers,"Putting this out there.  In the near future, our org will be posting positions for a Cloud Architect and a few Cloud Engineers (AWS).  Our budget can’t afford super-seasoned pros who are looking for 175-200k.  So we are looking for folks who have SOME experience who are ready to that step and have the chops.  We are located in Roanoke Virginia. Our “Principal” Data Architect is open to having preliminary discussions with anyone who is interested and has experience.  DM me if interested and feel free to pass along.",1568846129,0,0,1,dataengineering
Two csv files with contradicting data in order to get me to slip up.,"I'm currently taking an at home ETL test for a job interview. They gave me two different csv files that contain tables, with the only same value being ID. John as the ID as 1 in both tables but Jane has the ID of 4 in one table and 1 in the other.

I'm assuming this is a part of the test, so I'm wondering what the best way to go about that is and what they're expecting.

Thanks!",1568838424,0,0,1,dataengineering
"I'm the new designated Data Guy in my company, and I need some help","I work in a small company, where we have about 15 databases running in Microsoft SQL. I was tasked with creating a Data Warehouse in Redshift from the information contained in theses databases. The databases have the same structure, but each contains data from a different operation.  

At first, I developed an airflow pipeline with a pipeline that first detected all the databases available with a sql query, and then started multiple dynamic tasks for the extraction of each database simuntaneously and after that aggregated them, transformed them and finally loaded them to Redshift. However, i'm having a lot of headache with Airflow, and was looking for a cloud-based solution like Databricks to perform this task. But I do not know how I can perform this pipeline   that detects the amount of databses and automatically performs the extract query on all of them at the same time. Can anyone help me understand if Databricks is the right tool for me?",1568834305,16,0,14,dataengineering
what source control does your team use ?,[removed],1568822203,0,0,1,dataengineering
An interview about using stateful computation on data streams with the SwimOS kernel to improve your analytics,,1568811881,0,0,3,dataengineering
"YouTube's Database ""Procella""",,1568811195,0,0,2,dataengineering
Can you learn platform agnostic Data Engineering?,"I'm learning a lot as an engineer but wanted to continue studying a bit first thing in the morning. As I look through online courses I find most are nested in a cloud provider environment or tool-set. That is, it's easy to take courses to be a DE on Google CP, or AWS, or with some specific tool like airflow. 

What are platform/tool agnostic skills a data engineer can work on and does anyone have any resources for them?",1568761147,0,0,1,dataengineering
Portability of Business Logic,[removed],1568644611,0,0,1,dataengineering
Way to detect integrated data in a datalake?,"so this is my first time manipulating this type of data and i'm looking to learn how to do it

i have this schema that i have to follow (which is made in .owl )

and i'm looking for a way to integrate that schema into my datalake

and also another question ,is there a way to autodetect added or removed files onto the data lake ,i found an extension called Owlready2 that could help (with python), but i'm struggling to find how to integrate it

​
thank you :)",1568590486,0,0,5,dataengineering
REST as real-time data source,[removed],1568586255,5,0,3,dataengineering
I have an interview next week for a DE position. What are some good questions to ask the interviewer (MLE Lead)?,"Hey guys, I'm a senior engineer and coming from a backend background, I'm pretty excited about the data engineering field and I view a transition into a DE role as the ideal next move in my career at this point. For that to happen, I've been doing a lot of reading and studying on my own: Python, SQL, Spark/Kafka, data warehousing, ETL/ELT, cloud, distributed systems..etc

It seems hard to get into this field without direct professional experience in DE, but I was lucky to land this interview with a company and I really want to do everything I can to get my foot in the door.

So, from your experience, what are some thoughtful questions to ask the interviewer? and what are some things to look out for?",1568576000,6,0,22,dataengineering
Actian Avalanche,Anyone have experience with Actian Avalanche or its tools? Starting analysis on cloud DW platforms and just curious if someone has opinions without all the sales fluff?,1568468218,0,0,1,dataengineering
Thoughts on Druid datawareshousing tool,"Hi All,

I'd like to use Druid in my own project which I deveop for practicing purposes. Initially I wanted to use Redshif but since my goal is to dockerize the whole application and Redshift doesn't have an official image in the hub(as far as I know), I want to use Druid instead.

I have no idea how it works, need to learn about it. 

I'd like to hear thoughts of someone who used it. 
Do you have any recommendations/tips or maybe you'd suggest other tools ?",1568452581,6,0,6,dataengineering
Hiring Data engineers in Roanoke VA,Anyone who would consider relocating to Roanoke VA? We are looking to expand our Data Warehouse team pretty significantly in the very near future.  Looking for a blend of senior and junior developers who bring different experiences and skill sets to the table to be value add to our team,1568411338,10,0,9,dataengineering
Data engineering in the world of Virtual Reality and live streaming,"An interesting, in-depth look into how to architect for real-time event analytics in a [live streaming VR example](https://rockset.com/blog/real-time-analytics-virtual-reality-live-streaming/).

&amp;#x200B;

![img](19o0ej1zhem31)",1568397825,0,0,6,dataengineering
Streaming dataset architecture (Python),"I have a stream of data that I am subscribed to in Python. A few minutes later more information is known (the live feed is junky), and this is pushed into a SQL database. I want to listen to the feed, and then periodically update the older stuff with the SQL info when possible. What data structures should be used to do this in Python? Do I stream into a pandas dataframe (appending rows?) And then periodically replace the old chunk with a SQL query? What's the best way to handle the concurrency issues with this? Is there a better data structure than a data frame?",1568384046,4,0,4,dataengineering
"Hello guys, need guidance","Hello guys, first sorry for format Im from mobile, mods feel free to remove if this doesnt belong here.
So Im finishing with python and I wanna dive into data engineering, I need help with guidance, what to start first with what should I get my focus on, what tools you recommend etc..
Thank you for your time and thank you in advance",1568381942,4,0,0,dataengineering
Backend architecture recommendations project,"Hi,

I am currently working on a project where I have to define the backend architecture and would like to hear your recommendations. Is this a good subreddit for this or would you ask somewhere else? Anyway, here some infos about the project:

## Requirements
I need to store big datasets, currently up to 100MB, but I would like to support 1GB datasets as well. These are multiple arrays of floats and it would be good to only retrieve parts of them. These will add up, but at most +1GB/day
We use tensorflow to infer and apply models. I need workers for this and they should be scaled dynamically for concurrent users.

## Current architecture
This is our current tech stack, it got initially defined by someone else who left the company for a prototype and got expanded by me.
* Managed kubernetes on azure cloud
* Django webserver (using gunicorn and nginx)
* Unmanaged postgres to store Django tables
* Also stores datasets as JSON in text field in table, this has to be changed
* Volume with original datasets
* I think we do not really need them, this is currently only used to transfer them from the backend (Django) to a worker which imports them.
* [Celery](http://www.celeryproject.org/) as task queue
* This is the usual recommendation for Django projects and was used in a previous project
* Workers to import and process dataset
* Workers to run tensorflow on them
* Rabbitmq as message broker

## Problems
There are some problems with the current architecture:
* Storing data like this works currently, but I do not expect this to scale
* Can you recommend a database for this? Should I store the files in a volume or use a DB? Same DB as for Django? NoSQL or relational? Also there are some [managed databases on azure](https://azure.microsoft.com/en-us/product-categories/databases/), do you think any of this is a good and cost efficient idea?
* Celery and tensorflow do not work together, this leads to some bugs with multithreading. In /tensorflow issues they mention that this is unsupported.
* Any recommendations on how to continue? Possibilities I can think of are to create a sidecar container for tensorflow. How then to communicate with the celery worker? Another possibility is to communicate with Django over rabbitmq directly.
* Can I get rid of the volume to store the original files? How to transfer them to to the import worker?
* Currently only a single worker for tensorflow, I guess I can scale this automatically using kubernetes, but I have not looked into this yet, any tipps?

When you would start fresh, which technologies would you use? We use Django because most here know Python. Would you use a message queue like rabbitmq or microservices with REST APIs? Any kubernetes recommendations for scaling the tensorflow workers?

Thanks :)",1568360258,2,0,9,dataengineering
countBy for an RDD,"I have an RDD data structure in the following format (list of tuples):

&amp;#x200B;

 \[ (‘xxxx91’, \[{item\_code:20, item\_cat: 30,}, {item\_code:10, item\_cat:30},  {item\_code:20, item\_cat:10}\] , \[{‘company\_code’: 20, ‘company\_info’  : 203}\]) .....

&amp;#x200B;

I need do do a countBy of the following fields: item\_code, company\_code and company\_info. 

so output would look like this:

&amp;#x200B;

item\_code            company\_code          company\_info             count

20                                   20                          203                               xxxx

10                                  20                          203                                xxxx

&amp;#x200B;

Notes: the first element of tuple is an id, the second item is a list of one or many dictionaries and a third element is a list with a single item(dictionary)

&amp;#x200B;

I'm new to spark but here is my attempt so far:

 Output  = full\_rdd\_test.map(lambda x: (x\[1\]\['item\_code\], x\[2\]\[‘company\_code'\]).flatMapValues(lambda x: x) 

&amp;#x200B;

I have not been able to get this to work. any help would be appreciated",1568304512,0,0,1,dataengineering
What to expect in a junior data engineering role interview?,"I have an upcoming interview and the only thing I have been told is that I will be given a real world case study where I have to solve some sort of problem (might be given a dataset, not sure)

What should I expect? Can anyone give me examples of such case studies?",1568297926,15,0,11,dataengineering
Is Data Modeling/Architecture a Big Part of the Job?,I just started a new job and didnt realize there would be so much of this.,1568252626,7,0,6,dataengineering
Columnar DB Best Practice,Is the star schema the most optimal for columnar databases? Or would having a few large tables that are built wider (more columns) be better?,1568219975,2,0,3,dataengineering
The future of Prefect,"What do you think the future of prefect.io looks like.

Is it worth investing the time to learn it?

I actually tried and it's simple and clean to use.",1568215482,6,0,6,dataengineering
Deciding if an external data source should go in a data warehouse,[removed],1568210641,3,0,3,dataengineering
What frameworks or tools should I avoid?,"Are there any tools which I should avoid because they are outdated or don’t receive proper support?

For example - is Spark something I should dive into? Apache Drill? Or are there new tools or frameworks which solve similar issues in a better way? 

I just want to double check before I decide on a tool. My primary use case is to move a lot of staging data to object storage in parquet files and then do analysis and transformation on them. This is instead of loading the data into PostgreSQL because the creation and maintenance of the database table/ migration scripts tends to take a disproportionate amount of time.  I kind of just want to query the raw data or use pandas/ dataframes.",1568169533,15,0,12,dataengineering
Will they give me coding problems in a data engineer interview?,[removed],1568158172,2,0,1,dataengineering
Loading MySQL backup files into BigQuery — straight out of Cloud SQL,,1568154423,0,0,1,dataengineering
Merge same grain facts or keep seperate,[removed],1568151954,0,0,1,dataengineering
Advice on building a batch data pipeline,"I've been tasked to build a data pipeline for my company.  However, I have relatively little experience and not many people to get help from.   Here is my task.

Input: Manual drops of decent size raw binary data  \~2-3 times/week

Output:  Data stored in SQL database

My initial design:

1. Use Airflow as my workflow scheduler
2. Create Spark jobs that Airflow would execute.  As of now, I see at least 3 types of Spark Jobs
   1. Parse the binary
   2. Manipulate the data
   3. Store into SQL server
3. Basically, each DAG would perform the above step 2 for a single file.  So 10 files means 10 DAG(?)

&amp;#x200B;

Couple questions

1. I see lots of people using Docker/Kubernetes alongside Airflow and Spark.  Is a container applicable in my case?
2. I'd like to implement ML where I can.  Would that happen while I'm manipulating the data BEFORE I load it into SQL, or after?  Or both?

&amp;#x200B;

Any advice is greatly appreciated.  Thanks!",1568133987,10,0,3,dataengineering
Data Lake Architecture - How do I handle Raw Data and privacy,"How does this work with GDPR/compliance issues? I was wondering what a solid data governance strategy would be regarding being able to delete from raw data with changes in compliance in the future, as well as best practices for dealing with raw data in general.

I'm also wondering how to set up the architecture in such a way that if we deem a feature to be useful in the future we can pull it into analyses and data warehouses easily.",1568124431,8,0,16,dataengineering
How should I connect Postgresql with Elasticsearch to visualize in kibana the insert/delete/updates made in the DB?,,1568121804,6,0,2,dataengineering
An interview about building the Vector project to unify delivery of logs and metrics for better system observability,,1568079006,0,0,5,dataengineering
Do I need to learn software development cycle? If so where?,"I am a growing data engineer. I am in graduate school with no professional experience as of now. I am concentrating on building ETL pipelines with airflow. I am learning SQL and python for most of my time. Even though I know to use these technologies, I am worried if im missing out my job opportunities because of not understanding software dev cycles as a number of data engineer jobs are listed as software engineers? Should I learn that too? If so can you guys suggest good resources?",1568059204,8,0,12,dataengineering
Entry into Machine Learning Engineering?,"Is data engineering a good fallback option if I'm unable to land MLE/Data Science roles? I understand that I need to stay abreast of some big data frameworks and/or have specific projects to be able to break in. I'm starting to feel MLE roles are few and far between and my portfolio for a DS/Analytics job is not upto the mark IMO.   


P.S My post history might seem like its all over the place. Because I am. I would appreciate any help I can get.",1567976245,4,0,11,dataengineering
Master student here who wanna be a Bigdata Engineer!,"Hi guys

First of all, sorry for my broken English.

I'm a Master student, joined this community today to see how people in the DE field think and live in the world. and also I'd like to get some energy from you guys to keep studying without being tired.

I'm studying Data Engineering only for a year. A year ago, I didn't know about DE (cuz I was an Android Developer) and I just started studying DE because I was attracted to Bigdata at a data conference. Doing my research in my lab, I've built Hadoop cluster, Spark cluster with Docker and made them connect to MongoDB. I studied HDFS, Distributed system concepts, and Spark streaming as well.

DE is new to me still, hope I get some advice about what I should study and prepare to be a Bigdata Engineer. 

am also wondering how you guys studied in the past and how to stack your career.

It's too general question, sorry but please share your experience and some helpful advice to me. I'd also like to meet a friend or mentor to talk about engineering, school, career, experience, and personal opinion.

I read some recruiting sites to check what kind of data engineers companies want, they want Hadoop, Spark experience with real 'big data'.

My plan is, for now, starting a personal Spark project on Docker and push into my git.

God bless you guys and thanks!!

&amp;#x200B;

p.s.

I read [this post](https://www.reddit.com/r/dataengineering/comments/d0pcmq/master_student_looking_for_personal_projects/), the answers helped me a lot. I think I should read [awesome data engineering](https://github.com/igorbarinov/awesome-data-engineering) and [road map](https://github.com/hasbrain/data-engineer-roadmap) first!",1567879503,2,0,5,dataengineering
Master Student looking for Personal Projects &amp; Guidance to be a Data Engineer,"Hi Everyone,

Currently am a doing my Masters' degree in bay area. I have  5+ years of  experience as a ETL developer primarily on Informatica(Data Integration). My course work in masters program is more towards Software Engineering but am more interested to be a Data Engineer. So, am planning to do personal projects and self learn technologies required to be a Data Engineer. I have 2 more semesters to go and my target is to get comfortable in big data technologies by then and land in top companies as a Data Engineer.

Here is my plan - 

Am good in Java but not python.So, my plan is to learn Python by solving leetcode problems in python. 

Then learning Hadoop &amp; PySpark technologies for handling data in distributed systems, 

Airflow(I see this in many forums).

Would appreciate any suggestions or lead to plan my learning path with a personal projects to get good hands on experience.

Please suggests any kind of resources which will be helpful for me to land as a DE  in top tier companies.

Thank you!!!",1567817418,19,0,19,dataengineering
Question: User Agent parsing and Analytics warehousing.,[removed],1567784002,4,0,1,dataengineering
Arrived for my first day of work and my 6 month contract was swapped out for a 3 month. Is this sketchy?,"I had worked here previously but quit due to low pay.

I finished my masters and they agreed to meet me halfway on my requested pay. I never received a contract while negotiating. They gave it to me the day I arrived, claiming they had emailed it to me two weeks before. I started my first week while my manager is on vacation. 

When I arrived, they gave me the 3 month contract instead of the 6 month one they offered me over phone and email.

Is this sketchy?",1567727253,0,0,1,dataengineering
Is a BSc in Stats a good choice if i want to become a Data Engineer?,"I know that it would propably be best to go for a CS degree, but even so, is a Stats undegrad a good alternative? The program that i want to enroll has these courses that i think are very relevant to data engineering:

* Data Structures

* Algorithms

* Databases

* Database design

* Data mining

* Information Systems analysis and design

The rest are pretty much classic Statistics stuff (also with programming courses in R and Python). What do you think, is there any course that you would consider crucial for data engineering and its missing?",1567723624,38,0,10,dataengineering
Monitoring Apache Airflow with TIG stack: part 1,,1567722736,0,0,0,dataengineering
What are some most recent breakthroughs in DE?,This article[article](https://link.medium.com/qo7ZCc1SJZ) highlights some of the key technologies that changed the landscape of DE. So I wonder what are some of the most recent breakthroughs that are not in the article.,1567700029,7,0,2,dataengineering
CICD with databricks?,I was wondering if anyone had experience and wanted to share some methods that their team came up with regarding integration of databricks with their CICD pipeline. My company uses bitbucket server (not directly supported by databricks) and jenkins pipeline. I have read several articles regarding this but was wondering about how other companies were solving this problem.,1567695407,3,0,6,dataengineering
Facebook DE onsite interview in two weeks. What can I do to prepare?,"Against all of my expectations, I've landed an onsite interview with Facebook. They contacted me via LinkedIn, I had my phone interview (4 SQL questions and 4 Python, answered 4/4 on SQL and 3/4 on Python) and now progressing to the onsite phase. It will be a half hour ""culture fit/product sense"" interview followed by three one hour ""full stack"" DE interviews. If you couldn't tell from this opening paragraph, I'm having full-on imposter syndrome.

In terms of my skillset, my main background is as a traditional BI developer - MS SQL, Integration Services (with a bit of Azure Data Factory), Star Schema and some light Python to call API endpoints - all in the name of end-to-end managing several huge data solutions for marketing data. In February this year I landed my first DE post for a start-up in London, and it's my first real exposure to Hive, Airflow, ""production-level ""data lakes and more advanced Python. 

In short, I feel a bit out of my depth. I feel like I have very good product sense and enough of a light spread across the DE toolkit to come up with broad solutions, but I'm worried that my lack of experience in tools such as Spark, Numpy and Pandas is going to hold me back. 

I'm obviously massively excited for this opportunity and want to make sure I do my best. I'm committed to studying if needed. What advice would you have for me? Am I worrying too much about my relative, self described ""inexperience""?",1567689609,24,0,12,dataengineering
Question: How to Manage One Source of Truth Within a Company,"I’m not sure if this is the best community to post this question but it feels like the data engineers are ultimately the ones that have the most control over this topic:

Background:

I’m at a growing tech company with a decentralised structure to analytics.  There is an analytics team (BI) with a few analysts and data engineers but then each of the departments have their own  group of analysts. For example Marketing has 4, CX has 3, Sales has 3 etc.

There has been a problem within the company around inaccurate data, misrepresented numbers, changing definitions etc.  The CEO asked the analytics team (BI) to clean up the mess and get to one source of truth.

Right now our data is structured in the following layers:

Data lake with raw data
Modelled facts and attributes
Tableau SQL extracts

We currently have a production cluster and a dev cluster.

We let all the departments have scratch space in DEV where they can build tables, test data, do adhoc analysis etc. but we don’t let them push anything from DEV into Production and we don’t let them build Tableau visualisations off their scratch spaces in DEV.

Lately we have been getting a bit of pushback from analysts within different departments that want to start engineering their own data sets and start building reports off those sets.

My question is what strategies have you seen that work well in maintaining one source of truth across all 3 layers and yet give departments enough flexibility to do their work?

We have talked about a few strategies such as:

1. Let everyone build whatever they want but only lock down certain one source of truth data sets, tableau extracts etc.  This method would require visually showing what reports meet this standard and flagging the one source of truth data sets.

Pros: BI team doesn’t have to police as much, departments have more flexibility and speed to push out adhoc stuff 

Cons: organisation of data warehouse becomes messy, and lots of unsupported and unvetted data sets, executives will need to be educated to know what reports are stamped with the one source of truth vetting, 

2. Require all teams to work with the data engineers to build their data sets and push to production. Limit production to only things that have been vetted by the BI data engineers.Limit tableau visualisations to only Production

Pros: Keeps data warehouse clean, all data objects are vetted and validated, no worries about incorrect data sets, No worries about any executive reports getting pushed up by rogue data sources

Cons: slows down some of the adhoc work, departments may feel too restricted

I would love to hear how other companies are managing this and what works well.",1567656130,36,0,44,dataengineering
Data Engineering,"I’m new to Data Science and if I want to be a successful Data Engineer, where should I start?",1567634055,1,0,0,dataengineering
Udacity Data Engineering Nanodegree - Any Improvements?,[removed],1567624773,2,0,1,dataengineering
Astronomer v0.10 released,,1567523907,0,0,1,dataengineering
An interview with Pete Soderling about building and growing the Data Council events and helping engineers build businesses,,1567516314,1,0,13,dataengineering
Workflow orchestration suggestion needed,"Hi everyone,

I would like to know if there are alternatives to Airflow in regards to scheduling processes.

My stack is as below:

* ETL is done with Pentaho Data Integration mostly, we are working to create a docker image so it can be launched without a server.
* DB is Postgresql/Redshift

Our requirements is that jobs can be simple to schedule and support dependencies.

I'm also checking alternatives as Cronicle, Easy Scheduler, resque-scheduler and Chronos, but maybe we are overseeing others.

Appreciate your help.",1567392945,13,0,2,dataengineering
Has anyone done the Data Engineer track on dataquest,"I've been looking for a good introductory course to learn more about this field and there doesn't seem to be much. I found a course on dataquest and it seems to be the most affordable one. Has anyone done that program, is it good for a beginner?",1567386625,12,0,17,dataengineering
Technical Interviews for DE roles,[removed],1567371930,1,0,1,dataengineering
Does this look like a data engineer's resume?,,1567296640,31,0,24,dataengineering
Looking for data pipeline design suggestions,"I'm trying to come up with a design and architecture of a data pipeline for getting JSON  data streams for device stats (uptime, status, last seen connected  timestamps etc.) and want to build a dashboard to show those stats  within 5 min intervals. I was thinking of exploring both existing cloud  service solutions (aws/gcp services) and also a complete on-prem  solution. So, with that being in mind, what should be ideal tools for  this?

This is what I was thinking in terms of AWS for instance:

* AWS  native: \[Kinesis\]: \[Firehose &gt; Data Analytics &gt; Data Streams\]  &gt; Table (Dynamo/RDS with Lambda) &gt; Dashboard (Quicksight, or JDBC)
* Linux (Docker): Kafka &gt; Spark Streaming (or Flink) &gt; Postgres View &gt; Dashboard (JDBC)

I'm  trying to do any pipeline coding in Python and SQL and was wondering  how feasible either of these approaches would be in terms of build time,  scaling, performance, maintenance and multi-tenant model. Any  suggestions or alternatives are welcome and thanks for looking.",1567240218,8,0,12,dataengineering
Advice on managing messy data features,[removed],1567227222,1,0,1,dataengineering
FREE hands-on Workshop in Seattle - Create a data warehouse in Amazon Redshift with Etleap and AWS,"Calling analytics-focused data engineers!

We're joining forces w. @awscloud in Seattle on 10/29 to give a FREE hands-on workshop where you'll learn how to set up an analytics-ready data warehouse on Redshift.

Save your seat today by registering: [**https://info.etleap.com/devdaysoct29**](https://info.etleap.com/devdaysoct29)",1567205724,0,0,4,dataengineering
Master thesis in Data Engineering,"I am a MSc student in Data Science and Engineering and I am in the first semester of the 2nd year.

In some days I will have a meeting with a really promising company and most likely I will have to propose a good master thesis topic in order to get an internship and the thesis possibility.

My initial idea was *feature engineering for machine learning* since it seems a topic that will be much needed in the future. Any suggestion or different ideas will be very appreciated, thank you in advance.",1567184064,0,0,1,dataengineering
Data Modeling Alternatives to Erwin,"I've been keeping an eye out for a functional alternative to Erwin with better cross platform support and a lower price tag, but I'm finding no real competition and a number of FOSS projects that were just abandoned.  Am I missing something?  What tools are folks using for data modeling?  Is Erwin that good that they can dominate the market and command a substantial fee, or are people using combo tools that do data modeling + other features and Erwin is just the only tool out there that's specialized?",1567173915,1,0,1,dataengineering
Apache drill instead of a relational database?,"I’ve been using Postgres to consolidate data and prepare clean views which are then loaded to a third party visualisation too. Ultimately, no one uses the database other than my team -all interaction with data is through the visualisation tool. 

1- explore data with pandas in jupyter
2- create tables with sqitch (database migration tool) 
3- create views with dbt (great tool) which ultimately become the “raw data” in the visualisation tool where people can join it to other data etc
4- bulk copy data to csv, upload to the visualisation tools REST API

Many of the files we receive as raw data are reports already, sent by clients or other departments. This is denormalized data and we have no control over the schema changing. Because of this, I am finding little value in using our data migration tool where I need to add all changes in order, including column additions. Loading the data to the visualisation tool is just streaming a csv file through their rest API. 

It seems like my current process, although fairly neat and well documented, has a lot of overhead. 

Should I just load these files to azure blob as parquet or Avro (or even gzip csv) and query the data with apache drill? My only major concern is deduplicating data and making sure I select the latest version of each row (typically, we append only - we do not do much upserting these days). It seems like Drill has a lot of the same features as the database does. 

Since the visualisation tool requires a schema, this can be used as the read schema and I can pay no attention to write schemas (which ensures that I capture all columns without needing to make changes). If someone needs a new column to be added, I need to change the visualisation tool schema anyways. Since the data would be in files, I can loop through and bulk upload the data to the visualisation tool without needing to export it from the database first. 

(My other consideration is to just load everything in jsonb moving forward and then use dbt to pull out the required columns for the final view - much less maintenance)",1567169399,5,0,3,dataengineering
I think data engineering is quite tough! Probably harder than data science?,[removed],1567126718,1,0,1,dataengineering
Data engineering requirement,What do I need to be a data engineer I am studying computer engineering and want to know what I can’t do to become a data engineer any minor that will help me and make me get a job,1567125613,5,0,2,dataengineering
Looking for opinions on NoSql DB,"Hey folks, just as a quick background, I'm not a dataengineer by trade. I operate in the analytics space, and I have used DBs primarily as an end user (so I'm very comfortable with queries). I find myself in a position where I will be potentially setting up infrastructure for the first time for a company that does not have much experience in this area. I have not hidden my skillset, and should I get an offer I would simply be expected to operate within this area.

&amp;#x200B;

With all of that out of the way, I'm looking for a database recommendation for what can be summarized as vehicle data from multiple sources. 

* Think physical engineering data: speeds, rpms, pressures, voltages, etc. 
* There is not an internal, formalized understanding of what data is useful and what is not
* What gets recorded and what doesn't will be inconsistent as a result (so I don't have schema consistency)
* Final use case will be querying the data and analyzing it externally (BI, or python/R) to get insights about the vehicle population, vehicle-to-vehicle comparisons, and individual vehicle stats.

With all of this in mind, I'm currently doing research on columnar databases (hbase in particular) and doc stores (mongoDB); I'm hoping to get something setup tomorrow to run some tests. Can you all offer some recommendations on what I should investigate, and maybe why you would choose it?",1567101042,14,0,10,dataengineering
Which course would be more beneficial for me as DE - Distributed Systems in Java or Machine Learning?,[removed],1567098412,0,0,1,dataengineering
Streaming data pipeline design suggestions,[removed],1567093038,0,0,1,dataengineering
How to sift through the plethora of tool alternatives,"Our team is in the process of some rapid growth in the near future.  We are expanding our team, looking for experience and in some cases some entry level engineers who are ready to learn and can prove it (if there is interest message me)

We are also in the process of evaluating our stack and will be moving from our legacy tools and warehouse to the cloud (aws).

Where to start. There are tools out there to do everything. We are trying to limit our toolbox as much as possible, looking for proven products that can serve multiple functions. We have pretty much narrowed down to Snowflake and Redshift.

We are looking at Talend, Glue and other native products, Mattilion.

interested in CDC stacks like Attunity

I think that we are pretty set on Python+Spark for any custom dev that can’t be handled quickly with someone else but lack in experience there.

Curious as to what tools you employed for your architecture. Ours will be common to many.

Ingesting into a data lake where the data is discoverable, queryable with metadata and cleansed to some degree for ML.

Curated data will be pushed to a DW with ELT.

My biggest concern is finding something that can manage the workflows, all of them. Want to build dependent workflows that can push/pull data from place to place, managed from a single stack.

I’ve seen the rave on here about Airflow. Are there other products that can do ALL of what I need?",1567036873,22,0,13,dataengineering
Supplementing my knowledge as a newly hired data engineer.,[removed],1566998165,0,0,1,dataengineering
The MLOps NYC conference agenda is now online,,1566991468,0,0,0,dataengineering
An introduction to the practice and philosophy of Knowledge Graphs,,1566973482,0,0,11,dataengineering
An interview on what data engineers need to know about building tools and platforms for data analytics,,1566922614,0,0,19,dataengineering
r/datascienceproject is live,,1566919284,0,0,0,dataengineering
"Hi! I have just joined this giveaway organized by Remote-how, where you can win a 1-month workation from Bali. Flights, accommodation, and coworking space are covered! You just go and work remotely from paradise - are you in?Sign up to get a chance to be one of the lucky winners!",,1566916697,1,0,0,dataengineering
Creating a datamart?,"If I want to create a few departmental data-marts for reporting, and I already have a data warehouse...am I right in thinking I can just create a new db which will serve as the mart? and load it either during the initial ETL process which feeds the DW, or even a second ETL process once the DW is up to date? I'm having trouble finding anything specific online about building a DM as well as a DW, there is plenty on WHY you might need both but not much on building both at the same time. 

Also, if my DW is built using dimension and fact tables, and those fact tables are built around specific departments, can I use those fact tables as the marts? I'd imagine there will probably be too much data for them to be used as such for quick access to reporting, but just wondering if this is viable in theory.",1566906595,5,0,7,dataengineering
What are some of the core concepts I should become well versed in during my undergrad to DE?,"I'm trying to plan out what kind of electives to take to become a Data Engineer, I'll probably take the Distributed Database course, and perhaps a Big Data Technologies course. Are topics related to ML/AI or Operating Systems also important?",1566897810,3,0,1,dataengineering
GroupBY of JSON data in Pyspark,"Iam working with a very large JSON dataset called json\_dataset of the format below. I need to i)  get an aggregate amt1 grouped by cov\_cd  ii) aggregate acount of amt1 grouped by cov\_cd. how would I do this?

Below is my attempt but this doesn't work:

json\_dataset.filter(lambda x: len(x) != 0).flatMap(lambda x: x).map(lambda x: (x\['cov\_cd'\], x\['amt1'\])).reduceByKey(lambda key,value: key+value)

&amp;#x200B;

&amp;#x200B;

    json_dataset = [{'code': 'P',
      'state': 'NY',
      'cov_cd': '001',
      'amt1': '262154',
      'amt2': '0',
    '},
    
    'code': 'P',
      'state': 'NY',
      'cov_cd': '003',
      'amt1': '10000',
      'amt2': '2000',
    '},
    
    'code': 'P',
      'state': 'NY',
      'cov_cd': '005',
      'amt1': '7000',
      'amt2': '3000',
    '},
    
    'code': 'P',
      'state': 'NY',
      'cov_cd': '001',
      'amt1': '9000',
      'amt2': '3000',
    '}]",1566863065,8,0,4,dataengineering
Is this a more difficult field to break into as an entry level compared to web dev or app dev?,[removed],1566760426,0,0,1,dataengineering
Giving Back With Data Engineering,"I’ve become very interested in the use of programming, data science, and data engineering for a cause that helps the community. I’m wondering what everyone’s experience is with using data science/data engineering/software development for solving social issues. Where could and have these these skills been used for social good?",1566697695,6,0,26,dataengineering
"How much Mathematics, and how well must you know ML to get into Data Engineering?","Hi all, I started off my first software career in front end development after a coding bootcamp back in 2016, I now work as a fullstack developer using JavaScript/Java.

I've been reading a bit about data engineering, and I feel like its an interesting role to get into, I've recently been accepted into an online Masters in CS program at Georgia Tech. Originally, I've planned on focusing on the Computing System specialization taking various OS courses (high performance computing, advanced OS, compilers, etc) but if I wanted to get into the data engineering path, is it also important for me to take some courses such as Big Data, ML, and other Math heavy courses?",1566600101,10,0,0,dataengineering
"Optimus is the missing framework to profile, clean, process and do ML in a distributed fashion using Apache Spark(PySpark).",https://github.com/ironmussa/Optimus,1566586982,6,0,3,dataengineering
SQL intelligence on NoSQL Amazon DynamoDB,"California firm Rockset announced its serverless search and analytics engine now does real-time SQL analytics on the NoSQL Amazon DynamoDB database service.

The company specializes in such functionality, pointing its SQL-based analytics engine to NoSQL data stores such as Kafka and S3 storage buckets, along with Amazon DynamoDB.

[https://awsinsider.net/articles/2019/08/20/dynamodb-sql.aspx](https://awsinsider.net/articles/2019/08/20/dynamodb-sql.aspx)",1566582782,0,0,1,dataengineering
Best practices for managing data flows,"Soon my organization will receive data on a regular basis that needs to go through an ETL process into a DB to be consumed by a BI tool. The landing zone, DB, and BI tool are ready. However, I am struggling with coming up with a solid data processing plan from the landing zone into the DB.

Background on environment:

* Several csv files will land in S3 hourly
* DB is a PostgreSQL on AWS RDS

Background about myself:

* Analyst background with strong SQL knowledge and some DB management skills
* Almost no programming skills, but willing to learn if needed
* Only person in data team, thus solution needs to be easily manageable by one person (for now)

I was thinking of using AWS Data Pipeline tool, mainly because it doesn't require programming and supports notifications on fail/success out of the box. I could use a Lambda function to fire the Data Pipeline every time a new file in S3 is detected. Only thing I am worried about is scalability of this solution, since I wouldn't know how to easily recreate new pipelines and version them for documentation.

Since I am totally new to data engineering, what are some of your best practices and tips from people that have been in this field for quite a while? Is above plan a good start? Would you use different tools? Any push in the right direction is very helpful.",1566470800,23,0,15,dataengineering
Why would I choose to use Kafka over Pulsar?,"Aside from how it's more well-established, is there anything in the underlying technology itself that makes Kafka preferable for some use cases compared to Pulsar?",1566435761,1,0,4,dataengineering
HELP URGENTLY! Visa / USCIS/ someone to re-write the draft letter (proof course related to work),"My friend needs someone to re-write the draft letter (proof course related to work).
I can send you the word document.
There are 8 courses
In the document there are explanations of how each work help in the current job as a data engineer.",1566363189,0,0,0,dataengineering
Top 6 data engineering frameworks to learn,,1566320196,15,0,32,dataengineering
How we built a tool for validating big data workflows,,1566287393,0,0,5,dataengineering
How to filter an array of array of JSONS?,"I have a data structure that looks like this:  \[\[{'field\_amt': '200', 'field\_nbr':'300'}, {'field\_amt': '700', 'field\_nbr':'450'}\], \[{'field\_amt': '400', 'field\_nbr':'470'}, {'field\_amt': '800', 'field\_nbr':'300'}\], \[{'field\_amt': '200', 'field\_nbr':'600'}, {'field\_amt': '300', 'field\_nbr':'900'}\]\]",1566245256,7,0,0,dataengineering
Apache Spark for dotnet developers,,1566241291,0,0,3,dataengineering
"An interview about the HPCC platform, its journey to open source, and how it handle the full lifecycle of big data for enterprise scale analytics",,1566239710,0,0,3,dataengineering
Spark: Aggregating your data the fast way,,1566239482,0,0,1,dataengineering
Azure datalake vs Blob?,"Any recommendations on azure blob versus azure datalake gen 2?

I will be storing:

-	raw files prior to being processed (csv and excel)
-	processed files (parquet)
-	airflow logs 
-	maybe some database pg_dumps


It seems like the services are quite similar but azure datalake seems more appealing with hierarchical naming. 

Also, what is the best way for me to allow people within the company to upload files to this storage? Can I have an SFTP point to the blob storage perhaps? Or should i use Azure Files for this and then transfer the files to Blob later?",1566237363,6,0,8,dataengineering
Python ETL job to capture website stats and analytics data,"Hi people, 

I want to build an ETL pipeline which will capture the data from New Relic. For people who dont know New Relic is a monitoring tool, analytics tool. It is much like google analytics, where in it helps us identify which pages our customer are  visiting most, average time par page, number of request happening par hour, hows the health of our services. 

Coming back to the original question, New Relic has end points to capture this data. We have been using SSIS script task to call out to these rest end point to capture this data. Now we want to expand this project and SSIS is not very good at managing. I want to ask how can I use python, Do you have any recommendation on what free framework to use to build such ETL job in Python",1566228586,2,0,3,dataengineering
How do you reorganize/refactor your pipeline?,"How do you guys reorganize/refactor your data pipeline? I had built mine up to a stage where I found I didn't have the data in the format I needed, so now I needed to 

- Trace up the pipeline to see at which transform to reorganize the data schema.
- At the pipe to rewrite the transform (point of reorganization), I have to remember or reconstruct the data schema in my head.
- While I rewrite the transformation to reorg the data, I have to take into account any downstream pipes that might be affected.
- Change all downstream pipes to use new data schema
- Break the old connections and reconnect pipes in DAG to reflect changes.

This was a lot of work, and I feel like it took longer than it should have. What sort of strategies do you guys use to mitigate the pain? Or do you not try to reorg/refactor, especially since db migrations in addition to a pipeline migration might be painful? If that's the case, wouldn't your pipeline get more inefficient as multiple people add their own custom variations of a pipe onto the pipeline?",1566150877,7,0,9,dataengineering
Something that I think data engineering folks would find helpful!,,1566098287,3,0,2,dataengineering
Learn Data science and Data Engineering - My path,"Hi Guys,

Please check out my post. This post is targeted towards people who want to get into DS/ML /DE or starting in DS/ML/DE.

This list is compiled from various resources. I also posted the data science curriculum that I am learning.

https://medium.com/p/how-to-learn-data-science-my-path-ba7b9aa94f63?source=email-1d8fcdc16d73--writer.postDistributed&amp;sk=47d16e88a2bcb0635ab14792b0092fb6

I can suggest bookmarking this one and will be helpful going forward.

Good luck:-)",1566083638,4,0,24,dataengineering
Google search schema,"I have an interview coming up, and I've been told that one of the questions will be how I would structure the data if I were implementing Google Search. I've tried to think of various options, but I'm at a loss of how to make the database schema and how to effectively make the unstructured to structured data transformation. Is there a canonical way to transform and structure the data for something like Google Search?",1566064959,4,0,3,dataengineering
I am thinking of changing careers to become a data engineer - can anyone please critique my game plan?,"Hello there! I have a BSN from a SEA country but I realized too late that I didn’t want to become a nurse.

I’m currently working as a medical coder for a large hospital, and I am interested in becoming a data analyst, and quite possibly transitioning into data engineering or data science (likely the latter but we shall see!). 

Which game plan is better??

**Game Plan #1**

~~• Udacity’s Intro to Computer Science - used Python. I learned about recursion, strings, lists, dictionaries, boolean statements and loops.~~ **FINISHED**

~~• Udacity’s Programming Foundations of Python - Object-oriented programming, classes, method overriding~~ **FINISHED**

• Udacity’s Intro to Data Analysis - learn about Python libraries such as NumPy, Pandas and Matplotlib

• Udacity’s Data Analyst nanodegree - Develop a proficiency in Python and data analysis libraries and SQL, practical stats, data wrangling, data visualization with Python.**After this nanodegree, I will start applying for jobs.**

• WGU’s Masters in Data Analytics

**OTHERS**:

• Also doing Eric Matthes Python Crash Course on the side to build more projects with

• Codeacademy’s Learn SQL course

• Udacity’s SQL for data analysis

* Going through Kha Academy math courses on my free time to refresh/learn ~~Algebra,~~ Statistics (what I'm currently doing), Pre-Calc, Calc, Linear Algebra &amp; Differential Algebra.

&amp;#x200B;

**Game Plan #2**

* Apply and finish WGU MBA program while working
* Apply for analyst positions
* Get a BSCS degree in WGU while working as an analyst

&amp;#x200B;

**Which plan is better? Which has a higher likelihood of getting into data analysis -&gt; data engineering or data science?**

**Anything you would change in my plan?**",1565996377,17,0,21,dataengineering
Data Mapping Tool recommendation,"Somewhat like flow-chart tool like Lucidchart, I'm looking for one for (logical) data mapping - purpose is to create a pdf/picture [like this](https://cdn.softwaretestinghelp.com/wp-content/qa/uploads/2019/01/Introduction2.png)

Thanks",1565991352,2,0,2,dataengineering
Top 10 blog posts to help you transition to data engineering,,1565968116,0,0,20,dataengineering
"Airflow: Lesser Known Tips, Tricks, and Best Practises",,1565950408,1,0,20,dataengineering
Comparison of IPFS and EdgeFS for data-intensive Edge Computing use cases,,1565905122,0,0,0,dataengineering
Data Virtualization (Denodo) Question,"Please excuse my ignorance on the topic. I'm an engineer by degree, but my knowledge in this world is limited.

We have a Denodo connection to some time series data. The data is high frequency, with a new reading every 5 minutes. We have \~200 unique properties that contain about 20 different variables. So basically it's 20 different columns (22 including a unique identifier and timestamp), updating every 5 minutes, for every one of our 200 properties. 

We've been having reliability issues with querying this data into visualization platforms. What it comes down to is I can't pull more than 3 weeks worth of data. Any more than that and it just won't load. So my questions are:

1. Without more details, is it possible to know why this data would not load? Would this be a Denodo issue or should we be looking at the original DB?
2. If I didn't care about the 5 minute frequency, how hard would it be to take a daily average by unique identifier, for each variable, and put that into another table? What would that process look like? 

We have a good IT team currently looking into this, but we just lost most of our developers so progress is slow. Any insight would be much appreciated. Thanks!",1565878820,6,0,2,dataengineering
HDFS Partitions vs. Snowflake Micro-Partitions -- How can I re-create Hive Partitions in Snowflake?,"Hi there,

I am trying to migrate data from HDFS over to Snowflake. I am able to replicate the tables pretty much exactly, but what I'm having trouble understanding is how in Hive, I can PARTITION tables, which allows me to overwrite a specific part of the table (e.g. an entire day of data), with no issues.

&amp;#x200B;

Now, apparently Snowflake has CLUSTER and Micro-partitions, but the documentation is actually terrible. The only workaround I can get to do something like an overwrite of a partition is to write this:

&amp;#x200B;

insert overwrite into \[table\]

select \* from \[table\]

where date != '2019-07-18'

union all

select \* from \[table\]

where lead\_createddate = '2019-07-18'

&amp;#x200B;

Does anyone have any docs or sample code around how to setup PARTITIONS in a table within Snowflake?

&amp;#x200B;

Thanks!",1565802042,1,0,1,dataengineering
Are you happy with the current data engineering tools out there?,[removed],1565800469,1,0,1,dataengineering
How We Solved Our Airflow I/O Problem By Using A Custom Docker Operator,,1565789717,0,0,15,dataengineering
"Coursera Specialization Data Engineering on Google Cloud Platform has 30 days of free trial. Launched in July 2018 along with 11 other new specializations from Wharton, Johns Hopkins, Duke, Rice University...",,1565765414,5,0,5,dataengineering
Need career advice in transitioning into data engineering,[removed],1565725352,0,0,1,dataengineering
Hi ya'll. What are you guys using for data versioning? I'm thinking about using delta lake but I want to see what other options are out there.,delta lake vs ..???,1565711138,8,0,14,dataengineering
DE roles at FAANG,"Noticed a lot of DE roles in FAANG being in the US rather than the UK. I am currently a DE and there are definitely DE roles in the UK, but they tend to be at startups / other companies vs FAANG. I am therefore wondering if taking another role e.g. analyst role at FAANG and then moving to the US through a DE role is smart or best to stick to DE roles in London and see what happens? 

(Note: I have seen a handful of DE roles at FAANG in London but they are either more database/SQL type or very software engineering orientated; so this sucks for someone in the middle for example).",1565705086,22,0,5,dataengineering
Data Virtualization (like Denodo) as staging layer?,"Denodo provides great capabilities to make data available from multiple sources, regardless of the format. Have any of you seen the use of Denodo as a staging layer, effectively making data available (cache) before processing it inside a database?",1565679218,0,0,1,dataengineering
"Comparison of 17 ETL Tools, and the Case for Saying ""No"" to ETL","ETL pulls data out of the source, makes changes according to requirements, and then loads the transformed data into a database or BI platform to provide better business insights to make data-driven business decisions. Here is a comparison of 6 open source and 11 paid ETL tools to comparison what’s best for your business: [17 Great ETL Tools, And The Case For Saying ""No"" To ETL](https://blog.panoply.io/17-great-etl-tools-and-the-case-for-saying-no-to-etl)

‘No ETL’ here means that the ETL process is supplanted by Extract, Load, Transform (ELT), where data transformation happens in SQL as needed for downstream use, rather than upfront during the loading stage.",1565677293,4,0,0,dataengineering
Building a Data Science product that demonstrates Data Engineering skills,"Hi all, 

I apologize if this question is too vague/broad but I would like to get some basic insights from the community (I'll probably be posting more questions in the future ha).

I'm a grad student who will be starting the [Insight Data Science Fellows Program](http://insightdatascience.com) next month. During the program, I will be required to build a data product where I take data from somewhere on the web, do some processing, do some kind of statistical analysis, and visualize the results. 

As I'm also very interested in careers in data engineering, I would like to incorporate data engineering best practices so that I can demonstrate skills in both data science &amp; engineering. Are there specifically tools/technologies that I should incorporate in my project that would be (a) beneficial for my own understanding of data pipelines and (b) something noteworthy to talk about on potential DE job interviews?

Thanks!",1565657897,11,0,28,dataengineering
How to design a SQL database,"My boss has asked me to setup a SQL server on AWS, which our IT team will do, but I need some tips and resources on how to design or build the structure of the database. I have some basic knowledge of SQL queries, but not much on design. I've tried Googling things like SQL database design, but I'm not finding what I'm looking for. Most SQL tutorials start with a prebuilt DB and go into queries or just create some simple tables. I'd like to know why a database has a certain structure or how to create a design that is easy to maintain. Maybe I'm using the wrong terms for things, is database design a thing?",1565652060,2,0,1,dataengineering
CS undergrad with 1 year left--what does it take to get junior data engineer positions?,[removed],1565638908,1,0,1,dataengineering
Optimizing Spark Query,,1565630125,0,0,1,dataengineering
An interview about how the Fivetran platform is designed to handle data replication as a service,,1565618977,0,0,7,dataengineering
Web Service to AWS Kinesis,"Any opinions on a tool/framework/approach/whatever that consumes data from a web service and publishes that data to AWS Kinesis?  

Service &lt;----&gt; Something ----&gt; Kinesis

I'm guessing most folks are writing apps with the help of the Kinesis Producer Libary (KPL)?   This approach doesn't seem trivial.  Any words of advice?",1565617822,5,0,1,dataengineering
Airflow vs Azure Data Factory?,Can someone explain the difference between using Airflow vs Azure Data Factory to schedule ETL jobs? Is there a difference? Which one is better?,1565577441,15,0,12,dataengineering
Help with Airflow,"I recently started using Docker airflow (puckel/docker-airflow) and is giving me nightmares.

I wanna run a bash script using BashOperator. But when it runs it cannot find the script location.

this is my code:

    from airflow import DAG
    from airflow.operators.bash_operator import BashOperator
    from datetime import datetime, timedelta
    import os
    
    default_args = {
        ""owner"": ""airflow"",
        ""depends_on_past"": False,
        ""start_date"": datetime(2015, 6, 1),
        ""email"": [""airflow@airflow.com""],
        ""email_on_failure"": False,
        ""email_on_retry"": False,
        ""retries"": 1,
        ""retry_delay"": timedelta(minutes=5),
        # 'queue': 'bash_queue',
        # 'pool': 'backfill',
        # 'priority_weight': 10,
        # 'end_date': datetime(2016, 1, 1),
    }
    
    
    dag = DAG(""ranks"", default_args=default_args, schedule_interval=timedelta(1))
    
    
    t1 = BashOperator(task_id=""execution_rights"", bash_command=""chmod +x /Users/konradburchardt/Desktop/docker-airflow/script/rank.sh "", dag=dag)
    
    
    file = '/Users/konradburchardt/airflow/dags/rank.sh '
    
    
    t2 = BashOperator(task_id= 'rank_check',bash_command=file,dag=dag)
    
    t3 = BashOperator(task_id=""Step_2"", bash_command=""echo ' Step 2 Complete' "", dag=dag)
    
    
    t1 &gt;&gt; t2 &gt;&gt; t3

&amp;#x200B;

    [2019-08-11 21:15:35,115] {bash_operator.py:105} INFO - Temporary script location: /var/folders/56/0x5zxzq119b6wn0j_cchfzxw0000gn/T/airflowtmp4rarv7mk/create_filegzb7c3by
    [2019-08-11 21:15:35,115] {bash_operator.py:115} INFO - Running command: /Users/konradburchardt/airflow/dags/rank.sh
    [2019-08-11 21:15:35,126] {bash_operator.py:124} INFO - Output:
    [2019-08-11 21:15:35,131] {bash_operator.py:128} INFO - /var/folders/56/0x5zxzq119b6wn0j_cchfzxw0000gn/T/airflowtmp4rarv7mk/create_filegzb7c3by: line 1: /Users/konradburchardt/airflow/dags/rank.sh: No such file or directory
    [2019-08-11 21:15:35,132] {bash_operator.py:132} INFO - Command exited with return code 127
    [2019-08-11 21:15:35,140] {taskinstance.py:1047} ERROR - Bash command failed

I also checked my usr/local/airflow/dags/ and I did have [test.sh](https://test.sh) in there. So I tried also adding that path there but it didnt work. It seems it keeps loking int tmp files which I have no idea where that is.

&amp;#x200B;

Any idea of how to solve this and how to change this tmp file to my dags folder?

 Im can use vanilla airflow or I can also use docker airflow.",1565560705,20,0,3,dataengineering
The Data Analysis &amp; Machine Learning ebook bundle ends in about two days (PSA),,1565446983,1,0,33,dataengineering
How should I start learning data engineering?,"Hello
I am 16 and I really like programing especially stuff that has to do with data and wanted that to have something to do with my job when I finish school but wanted to start learning now . So what are some good resources that can help me start learning data engineering. 
Thanks",1565445347,6,0,2,dataengineering
Pinterest Tech Talk: Aggregator-Leaf-Tailer architecture for low-latency queries on large datasets,"Dhruba Borthakur (founder of RocksDB) discusses the implementation of [Rockset's](https://www.rockset.com/) low-latency operational analytics engine. Topics covered include the Aggregator-Leaf-Tailer architecture, smart schemas, converged indexing, and serverless data management.

Video is here: [https://www.youtube.com/watch?v=tDgmtReLS8c](https://www.youtube.com/watch?v=tDgmtReLS8c)",1565396514,1,0,5,dataengineering
Getting started with Data Lineage,,1565357433,3,0,21,dataengineering
Development workflows in AWS Glue,"Greetings.  I'm evaluating AWS Glue.  I'm aware of a code management and deployment solution in AWS detailed [here](https://aws.amazon.com/blogs/big-data/implement-continuous-integration-and-delivery-of-serverless-aws-glue-etl-applications-using-aws-developer-tools/).  But haven't found any other solutions.

Does anyone have a solution (or ideas) not based in the AWS Console? Important points for me:

* Fully (or significantly) automated
* Environment variable store
* Github integration",1565346056,1,0,0,dataengineering
How to choose workflow automation tool?,"I've recently begun a project at work to automate the process of extracting, calculating, and extracting data sets from multiple data sources and then changing their formats, doing transformations and then loading the data into the cloud.  I've looked at Apache Airflow but I also know Prefect is a new offering. I've looked at some less well supported tools. 

How does one go about deciding on a tool to use automate workflows. What factors are important to consider?",1565314275,12,0,23,dataengineering
"Big Data Project with Hadoop, Tajo, and Spark","I am looking for a big data engineering project to experiment with Hadoop, Tajo, and Spark. I am planning on implementing a Hadoop cluster with 2-nodes, with Tajo and Spark. 

I am looking for large data updating frequently (I am thinking of every hour) to use for this project, any good recommendations?",1565306779,1,0,1,dataengineering
Seeking mentorship,"Hey guys as the title suggests I'm looking to see if anyone is kind enough to let me bombard them with questions on my path to become a data engineer. I currently work as an Aircraft router for a major Airline but have found myself becoming my department's adhoc Data-analyst. I find that working with data is much more rewarding than my actual job which is very routine. While I don't mind googling my way to learn the required skills Id like to talk professionals who can share some industry standards and best practices.

Also If anyone else has found a way to transition to data engineering from completely unrelated field please share your story! 
What was the most challenging aspect of becoming a data engineer?

What was the easiest?",1565281013,3,0,3,dataengineering
Anyone here completed Udacity Data Engineering Nano degree in two months? or less?,"I know the answers here is “depends on the person”.  I am trying to decide between paying monthly and paying at once. I have gone through the syllabus and I felt I can spend more than 20hours a week. From my three years of experience as a Data Analyst, I am proficient with Python and SQL along with basic understanding of databases. So I believe it could be easier for me to complete in shorter time. I want to do this because currently I have  bits and pieces knowledge of data engineering and would like to become organized in my learning.

If anyone here with my background has ever done it less than 5months(estimate by Udacity) please share your experience. Thank you!",1565272319,10,0,10,dataengineering
Career advice for undergraduate.,"Looking for some advice and thought this was the right sub. Please tell me if it isn't.

I'm  21 years old, and I'm a mexican Computer Systems Engineering  undergraduate. I'm very close to graduation, I only have three subjects,  and my Terminal Project (A project that lasts about a year) left to do.  This summer I applied to a Data Science summer internship at a big  company and have been working there for 2 months now.(My contract ends  in a few days).  Well, it turns out I was offered a data engineering job  in the US, because I'm an American citizen too (born and raised in  Mexico to an American citizen). I've really questioned the fact that  they really want me to work there because, well , it's only been two  months and I'm an undergrad, but it really is happening. At first they  did not even think about it because I don't have a degree yet, but they  really liked my work and talked about it. And they want me to go ,  despite not having a degree.

This  may sound mundane to some but to me it is a very big opportunity,  because I've always wanted to work abroad, and I'd be paid very well.  And also, my family's economic situation is not the best and we've  always planned to go live in the US.

I  looked for ways to finish my degree while abroad, but it is almost  impossible to do it (because public school). Unless I take a break and  come back and finish, which I honestly think  it's not very realistic  because I doubt the fact that I'd come back. I'm still looking for  options though, I haven't fully given up on finishing while abroad.

I've  been advised not to drop out of school , and I think it is the best  thing to do (I mean I might need a master's degree later on) but I'm  very afraid of the fact that this kind of opportunity might not show up  again. I think it could be the opportunity of a lifetime (and maybe it  isn't). And well, some of the bosses think I should go, my boss thinks I  should finish and stay. I think I should point out that I study in one  of the best public universities in Mexico, so I'm not dropping out of  any school.

So, I'd like to know,  what is your opinion on this? How do you think  I would do if I work on  data science or other IT related area , and in the US, without a degree?

All comments are welcome and I'd appreciate your insight.",1565223612,0,0,1,dataengineering
Best Path to Data Engineer Role out of College?,[removed],1565222343,0,0,1,dataengineering
As a data engineer what has been your biggest accomplishment?,"I'm applying for other jobs, but I have a hard time coming up with accomplishments to put on my resume. I've migrated some back-end databases for front-end reporting but that seems trivial. I feel like most of my accomplishments have also been cross-functional.. i never have done anything useful by myself. Any thoughts here?",1565159723,24,0,11,dataengineering
Cheapest way to run Airflow on Kubernetes in a production environment?,[removed],1565136472,0,0,1,dataengineering
How to commit airflow code?,[removed],1565118116,11,0,4,dataengineering
Building petabyte-scale analytics with BigQuery and HLL,,1565109775,0,0,1,dataengineering
MapR’s business assets acquired by HPE,,1565103553,1,0,1,dataengineering
London Salaries: Permanent vs Contract,"I'm an Aussie moving to London working in the DE space and trying to understand some of the salary differentials I'm seeing. 

Seems like perm roles as a Data Engineer top out at about 70k. Yet I'm seeing contract roles between 400 to 700 a day. Which assuming 48 weeks of work is between 96k and 168k. Now I understand the inherent risks with contract roles but the difference is much larger then I'm used too. Am I missing something here?",1565083202,9,0,9,dataengineering
An interview about the open source Amundsen platform for data discovery and how Lyft is using it to improve their analytics workflow,,1565041143,0,0,7,dataengineering
Good material to review before a job interview,,1565019840,3,0,3,dataengineering
Spark for Beginners on YouTube,[removed],1564748338,0,0,1,dataengineering
Batch Job Scheduler Options,"Hey, I'm an ETL developer looking to do some side projects of my own to develop some new skills.  I'm running some python scripts to collect data from various sources and I'm looking to schedule these scripts.

At work, we use Autosys which works decently enough for our batch jobs and I've previously used Control M at a prior employer.  

I'm looking to learn a new scheduling tool.  For reference, I'd be running this on a Raspberry Pi 3 so it can't be a huge resource hog.  I could use cron for some of these, but eventually I'd have conditions between scripts so Cron wouldn't really get the job done there (since it appears to only use time-based scheduling).

Thanks!",1564710626,2,0,3,dataengineering
Has anyone moved into data engineering after being a business/data analyst?,"I may have an opportunity in the near future to transition internally to a data engineering role, coming from a business/data analyst type role. Right now I use a lot of sql and tableau, but don’t get overly technical for the most part. I’m curious if anyone has a similar story, and how it went for them. I have been working for about 9 years in various analyst roles, but feel like there’s a plateau at some point for this type of role. I think learning a more technical position will help me career-wise in the long run.   

But, I’m a bit overwhelmed and daunted by how challenging the new role may be (big data, airflow, complex sql, python, etc).  My degree is in finance and a minor in MIS so I’m vaguely familiar with the concepts, but not so much in practice.  

 If you’ve done this, did it work out well, did you regret it? Would love to hear from you folks. Thanks for reading",1564699363,27,0,22,dataengineering
Traversing the Land of Graph Computing and Databases,,1564674222,0,0,9,dataengineering
Snowflake: the details of our first Data Warehousing project in the Cloud,,1564614944,1,0,1,dataengineering
Operational Analytics: What every engineer should know about low latency queries on large data sets,"Operational analytics is a very specific term for a type of analytics which focuses on improving existing operations. This type of analytics, like others, involves the use of various data mining and data aggregation tools to get more transparent information for business planning. The main characteristic that distinguishes operational analytics from other types of analytics is that it is “analytics on the fly,"" which means that signals emanating from the various parts of a business are processed in real-time to feed back into instant decision making for the business. Some people refer to this as ""continuous analytics,"" which is another way to emphasize the continuous digital feedback loop that can exist from one part of the business to others.

&amp;#x200B;

The definition of an operational analytics processing engine can be expressed in the form of the following six propositions:

1. **Complex queries**: Support for queries like joins, aggregations, sorting, relevance, etc.
2. **Low data latency**: An update to any data record is visible in query results in under than a few seconds.
3. **Low query latency**: A simple search query returns in under a few milliseconds.
4. **High query volume**: Able to serve at least a few hundred concurrent queries per second.
5. **Live sync with data sources**: Ability to keep itself in sync with various external sources without having to write external scripts. This can be done via change-data-capture of an external database, or by tailing streaming data sources.
6. **Mixed types**: Allows values of different types in the same column. This is needed to be able to ingest new data without needing to clean them at write time.

This post discuss each of the above propositions in greater detail and discuss why each of the above features is necessary for an operational analytics processing engine: [https://rockset.com/blog/operational-analytics-what-every-software-engineer-should-know/](https://rockset.com/blog/operational-analytics-what-every-software-engineer-should-know/)",1564606381,1,0,13,dataengineering
Any GCP Dataflow (Apache Beam) users in the house ?,"Hi all  


I am new to GCP Dataflow (Apache beam). I have some questions on how to architect the pipeline dynamically.   


Let say i have a  Branching PCollections as in this pic ( [https://beam.apache.org/images/design-your-pipeline-multiple-pcollections.png](https://beam.apache.org/images/design-your-pipeline-multiple-pcollections.png) ). If someone from other team(like data anlysts or PM who dont have knowledge on apache beam but know little python) wants to just add another PCollection C names. How to build this kind of platform.  


Love to hear if you have any other thoughts and suggestions. The main goal is to add a  branch to pipeline dynamically for people who dont know apache beam. This new pipeline is nothing but some python methods.

&amp;#x200B;

Thanks in advance",1564602505,5,0,6,dataengineering
How do I increase the ssh connections coming to EMR from Airflow ?,"Excuse me I posted this in AWS sub and not much responses.

Not sure if this is the right place but Need help from a Big Data Expert. We use Airflow (c5.4xlarge) for Launching jobs onto EMR (1 Master of r5.4xlarge; 5 Core nodes of r5.4xlarge; 2 Task nodes of r5.12xlarge) with auto scaling.

Airflow ssh's to EMR to launch spark-submit jobs and we use GDC and S3 and we are talking 200 DAG's with some 4k tasks etc.

I am looking for ways to tune this ingestion flow to increase the parallelism/concurrency of this flow so that More jobs can run in parallel as we always have bunch of jobs queued in Airflow.

I have increased the Concurrency in Airflow to 128 but where should I correspondingly increase this number in EMR so that it can accept more connections from Airflow. Right now it only runs 50-60 Yarn jobs at any given time (Apps Running from Resource Manager UI).

Also please suggest any other ideas to make our Flow/Cluster more performant as this was designed 1 year back",1564588073,13,0,5,dataengineering
What should I look for in a Linux server?,"Local machine has 24gb RAM, i7, 2.4GHz CPU on Windows. Manipulating time data in pandas is very slow currently as I need to do indexing and mapping at multiple stages, working with 500k rows at a time/per dataframe. Employer offered to let me do the processing on a remote Linux server to speed things up and free up my machine for smaller projects to work on concurrently. However, I'm not sure what I should ask for. How much computing power and memory do I need for work like this?

I have taken to Google but many guidelines are based on ML type projects. That may be what this data is used for later but beyond my scope.",1564534287,9,0,2,dataengineering
Data Engineering Tech Stack at Tile,"[https://medium.com/me/stats/post/b97bfe250b8f](https://medium.com/me/stats/post/b97bfe250b8f)

Wrote about data infra at Tile. Also, Tile is looking for an engineer to hire in Vancouver. Let me know if you are interested in helping.",1564525616,8,0,10,dataengineering
Best places to get help for Spark?,"Hi Folks,

I see in the Apache Spark documentation, that the advised first port of call for help is Stack Overflow.

While there seems to be plenty of questions being asked over there, I tend to find that there are not a lot of answers.

So, if you have a problem with Spark, where do you ask questions?

Thanks",1564440942,6,0,2,dataengineering
PSA: Data Analysis &amp; Machine Learning ebook bundle,,1564436544,6,0,22,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor.",1564408165,0,0,0,dataengineering
Data Engineering + Career Break,"Background:

I have been working for 3 top tech companies (think Google/Amazon/Facebook/Apple/Twitter/LinkedIn/etc) for ~5 years in data/analytics related roles, the most recent being a Data Engineering role (1+ year).

I have a Bachelor in Statistics and am doing a part-time Masters in Computer Science / Software Engineering (both UK target universities).

I also picked up freelance technical Data Engineer work. Because of my performance, they are happy to extend the contract to 6+ months, where I can work 2-3 days a week (I have 2 long term contracts), and the pay for 1 of them is about 2x my salary at the tech firm anyway.

Question: I was thinking of leaving my tech firm (so I leave with a total of 5 years industry experience), doing the freelance work 3 days a week, and completing my Masters / upskiling on Python for 2 days a week, and having the weekend to breathe until I graduate from my Masters (Oct this year - May next year I would be 'off').

Advantages:

The work I am getting through the freelance work is 'actual' data engineer work, not just SQL/database stuff. Data Engineering (in my opinion) is basically Software Engineering focused on data, but in my current role I will never get that type of work (as my manager is very backwards and other roles in the company are similar). Coding, building pipelines, using a cloud stack etc all prepare me well for future / other roles in the market.

My Masters happens once in my lifetime, better to give it my all as the tech companies will always be there anyway. Technical roles are never going away. :)

Freelance + Masters gives me freedom to prepare for interviews (my Python is shakey) so in 3-6 months I am actually fluent again (I was fluent before but because in my recent roles I didn't really use it much I've gone rusty). It also means I've finished my studies when I go back to the market, pay will be better if anything, and with Brexit probably even better. I was also tempted to go on these 12 weeks courses which guarantee you a software engineering role, but tbh, I only really see myself in a data engineering role.

Disadvantages:

I am being offered senior / leadership positions already (2x my salary at min) outside of the firm. The only issue is I have 0 time to prepare for interviews, have a few technical creases which I want to iron out, and taking a leadership role at the moment could be dangerous as there is even less time then to finish my Masters. Tbh I don't want to take such a role either if I know I'm shakey. I'm not that bad either, 4-6 months heads down and preparing and I will be good, as its all in my brain just needs to be brought forwards. :)

It may look like 'I can't cope / took a break'. I will still be working, just less (probably 'more' actually as its more relevant haha).

For my Masters dissertation, it would be 'open source' not 'company based' then. Not sure if this is allowed, will have to check with my advisor, but I know several of my classmates who are freelancers / not working and they still complete a Dissertation.

If I proceed my story would be 'was doing a part time Masters, worked 5 years in tech, moved to full time Masters to focus on that while keeping my data engineering skills fresh by doing freelance contract work, back into the market now'. Is this sound? Anything I have missed? Would this make me unemployable in the cool tech/finance etc companies and/or anywhere?",1564401690,19,0,0,dataengineering
Blogpost about the machine learning lifecycle,[removed],1564398939,0,0,1,dataengineering
An interview about a new pattern for data integration that reduces the amount of effort required to find connections in numerous data sets,,1564398463,3,0,1,dataengineering
What does your data stack look like?,"I work in a corporate environment that is looking to upgrade it's data stack. We work as analysts out of a MySQL environment that is updated daily by jobs running from Retail Data Warehouse.

Some of our views are extremely slow and painful to work with, but it's what we know. We work across 100s of millions of rows.

We are looking at MapR as a solution with Tableau. However, the team is not a huge fan of Apache Drill when we are so used to MySQL. A lot of our operational reporting comes out of Excel, where we have queries with date parameters (something that Apache Drill does not support to my knowledge). Even though we want to move to Tableau, we won't ever escape Excel, and I think losing this ease of use will be a huge impact to the team, and will hurt the change management piece.

We have been discussing what it would look like with Azure Cosmos DB. However I'm not sure what that would look like, or what 'best practice' is. We need something to manage our 'big data', but also offer an easy way to get operational reporting data, and the ability to deep dive into low level data for analysis.

My team are not IT experts, they are commercial analysts and accountants. SQL is not their passion, however they are willing to get their hands dirty when needed, but I don't want to push them too hard. I'm not sure what 'goes' with Azure's stack to make it easier for them?

I would love to hear your opinions or experiences with using Azure for your big data store + day-to-day. Likewise, it would be great to see what your organizations are using for operational reporting. While we are not there yet with actual data science, we are getting started on our journey.

Thank you for any information!",1564382614,9,0,11,dataengineering
When to use a database versus Avro or Parquet?,"Currently using a database but I am interested in exploring other data formats. 

Typical usage is querying a lot of rows to fully replace data over a rest API (specifically a tool called Domo which has a streams API where you can send compressed CSV formatted data in parallel). Most data is denormalized with a lot of columns but we typically do not need all columns. A lot of data is not joined to anything at all, but we do commonly filter out duplicates (select distinct on columns, order by updated_at desc) and we tend to append data rather doing in place updates to rows. 

ETL is done with the pandas library which has a to_parquet method which is neat. We are normally using pandas to get the data clean enough to bulk copy it into the database. It seems like I could change this from to_csv to to_parquet essentially? 

Our entire warehouse is about 1.5 TB but the largest single table is a few hundred million rows which we partition by month. A lot of dimension tables are snapshots partitioned daily and we simply join rows to the specific partition rather than using type 2 SCD tables. 

I don’t know much about Avro but the compression and schema evolution seems interesting. Any advice about when a database is not the best tool for the job?",1564365138,6,0,8,dataengineering
What would be the best data store for fast write/read of billions of float32s? SQLite? Hdf5? Other?,"I am trying to create a library for sparse training. So it would need fast read/write of not only the machine learning model weights, but their optimizer momentum values as well. At the moment I would only like to experiment with values in the billions. 

Each parameter would be a float32 value. 

I think I can just use a single column, but there may be some instances where multiple columns would be helpful. For example, each column would represent a different layer in the machine learning architecture. 

Each update step in the training would be looking up and writing values do the data store, so I am looking for the fastest database for my situation. Having a database at all would already significantly reduce ram memory, so that that is not a concern for me.",1564242269,3,0,3,dataengineering
Optimizing Spark I/O,[removed],1564235076,0,0,1,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor.",1564223452,0,0,1,dataengineering
Dealing with frequently changing data from many unrelated sources?,"I manage a data warehouse in the call center industry. What this means is that we have internal data which is somewhat stable, and then we have client data for hundreds of clients:

-	excel or csv files summarising performance - these could be complete dashboard reports in xlsb format with 40 sheets, merged cells, headers etc. You can imaging how crazy this data can look.  
-	as a side note - many of these files are actually generated from common tools but they are reports, not raw data (Verint, Genesys, Avaya, Cisco, etc. ). They can be shared on sftp sites, sent as email attachments, or POCs load them onto an sftp server for us 
-	REST APIs (Salesforce, servicenow, zendesk, playvox, logmein, appfollow, etc)

I currently load this data into PostgreSQL tables. Each client is a different schema and there is zero interaction between them. We explore the data and convert everything to csv format and then use COPY to bulk load the data. 

We use sqitch for database migration. But realistically it is a pain to deal with changes to client raw data. New columns can be added or removed without warning. We don’t know until it happens, and then we have to alter the tables. I could automate adding new columns, but it kind of goes against the idea of using a migration tool and sometimes a new field is literally just a bad file with shifted columns or some other error. 

For REST APIs, we load the results into a single jsonb column and then create views or flat tables by querying that table ([dbt] data build tool is used for this). This works well - we have all of the fields no matter what in the raw data, and at worst we just need to alter the view if people need it as a real column. 

Should I use the same approach for client files? Just load everything as a jsonb field and flatten it from there instead of trying to figure out the data types per column to create the tables and deal with frequent changes? We do use the database for unique constraints and indexing columns, but technically the visualisation tool could be used to deduplicate data. 

Or would a tool like Avro make any sense here? I read that there are some features to resolve changing schemas. Ultimately the only thing I am doing is loading this data to a visualisation tool through a REST API - so I dump the data to disk from Postgres and then stream the csv file to our visualisation tool. I typically do not need all columns. I assume I could read data from avro, convert to csv as required by the API, and stream it as well?",1564192264,7,0,2,dataengineering
What is the future of Data Engineering?,"I am currently very fortunate to work at a FAANG as mid level DE. I’ve been exposed to a plethora of technologies and concepts in this space and really love what I do. However, I’ve been deliberating on what the future holds for this role in the industry. Though I love building resilient data warehouses and optimizing compute/etl in pipelines, I can’t see myself doing this forever.

Personally, I love to learn and build solutions to solve challenging problems agnostic of whether they are front end or backend realted. But I view Data Engineering as a small (but integral) piece of the puzzle.

In your opinions, what do you see data engineering becoming in industry over the next 10 years?  What other engineering domains will work more closely with data engineers over the years? What domains have skillsets that are ”transferable”?

I have a background in analytics,  but like to do engineering more than analytics. Data Science isn't something i’d consider.",1564176329,6,0,19,dataengineering
Interesting problem: Analyze differences and drift between two DBs that are being simultaneously maintained.,"Say we have 2 DBs, like postgres and bigquery that should be kept in parity. Obviously there are major differences in syntax and optimization mechanics and overall feed process design, but in the end the two DBs ought to have the same data set.

How would you go about analyzing difference and any drift between the two. I suppose ""final"" views could be created in each that take compile some aggregate numbers and then they could be compared. Seems inelegant. 

Does anyone have even conceptual advice? References for similar problems?",1564168300,2,0,6,dataengineering
(X-Post) Need Help Picking an ETL Tool,"Cross Post: [https://www.reddit.com/r/ETL/comments/ci2swa/need\_help\_picking\_a\_tool/](https://www.reddit.com/r/ETL/comments/ci2swa/need_help_picking_a_tool/)

&amp;#x200B;

 Im starting a new job as a solo data engineer.

The  company uses a postgres database, and the ingest data from various file  formats and APIs. They also use Azure to host everything. They are big  on open source software since they are a non-profit.

I know they also wanted to use python, or at least its in the JD.

So  what are my options as far as tooling? I am relatively new to this and  will be working solo. So if Im not asking for too much, id like  something easy to learn and maintain.

Any suggestions?",1564166869,9,0,1,dataengineering
ANNOUNCING: Snowflake + Etleap,[removed],1564159980,0,0,1,dataengineering
"General wisdom and learnings, not necessarily specific for Google Cloud: _Democratizing data analysis with Google BigQuery_",,1564140350,0,0,1,dataengineering
"Does anyone have a good guide or article with step by step code to build a basic oltp db and write an etl script to transform it into a olap , star schema ? Newbie here and want to put together a simple project in SQLite if I can to get my head wrapped around oltp to olap etl’s",,1564101597,2,0,1,dataengineering
Data Engineer at McDonald's,Anyone worked as or know anyone that is a data engineer at McDonald's global headquarters in Chicago? What is the experience like?,1564082633,8,0,10,dataengineering
"""Engineering"" my mobile app data - data are stored on a text-file","I built an Android analytics app to track my day-to-day stock trading; it's basically the usual time-series graph where X is date and Y is $. This is just a simple descriptive analytics and nothing more really.

The financial data (or ""a portfolio"") came from 2 remote systems, and 5 more additional financial portfolios are also extracted each day. As of today, all 6 financial portfolios are stored on 6 separate text-files, where each text-file has 2 columns (date and value) separated by a space, and they are all located on the phone's main memory.

&amp;#x200B;

Would the kind DE folks offer me some advice on how to better engineer these data? 

Would there be a process I could/ should follow? Should I migrate my data to SQLite or something? 

Thanks DE folks!",1564024918,12,0,4,dataengineering
What does your data platform look like?,[removed],1564007561,0,0,1,dataengineering
I have an interview coming up for a DWE role I'm really not qualified for. Help?,"No judgment, please. Just need the best way to prepare.

As the title said, I have a second-round interview for a Data Warehouse Engineer role, which happens in less than 48 hours. I'm really a newbie in DE, with my recent focus more in DS/ML/analytics and past experience in just working closely with DBAs/data analysts.

When it comes to DW concepts, like snow flake, data marts, dimension types, etc., a lot of that stuff is really foreign to me. And I've never really worked in a DW production/enterprise environment, so my experience with tools like Oracle, PL/SQL, etc. is super limited. My qualifications are basically: 

* Good proficiency with SQL (Postgres, some stuff in SQL Developer with coworkers)
* Using ODBC in MS Access (please don't laugh) as kind of the client-side interface for our core business functions
* ETL batch scripts that I've written in Python for personal projects

I'm really not sure how I made it past the recruiter screen, but my approach has just been to just emphasize the experience I do have. But at some point, if I'm asked about deeper DW concepts, I'm pretty screwed. 

Is there any hope for me?",1563922472,4,0,1,dataengineering
I created a tutorial to submit PySpark applications to an AWS cluster from your local machine. Check it out!,,1563920224,0,0,17,dataengineering
Is data engineering under IT?,"I work for a large corporation who has been using Excel dashboards for reporting for the last couple of decades. I originally was from operations (client facing, managing the floor) but got into data related stuff and automation about 5 years ago. 

Currently, I have a 3 node Docker Swarm cluster on Azure. Most services run as containers except for the database which is installed directly. I am under the team which owns the excel based reporting and am in charge of automating the work and feeding our visualisation tool with data. This is NOT under IT. 

1) airflow for scheduling along with a lot of custom hooks and operators to move data around (from REST APIs, files, websites, and various databases).  

2) PostgreSQL for the data warehouse. Sqitch as our migration tool, and dbt (data build tool) as to create our views and tables. 

3) Prometheus stack for monitoring.

4) Gitlab runners for deployment. We are using gitlab for a lot of the features (container registry, issue tracking, version control). 

5) Jupyterlab for data exploration, traefik, portainer etc. 

Persistent storage is a weak area which I need to improve (might have a separate post about this soon). Right now it is dependent on individual folders on specific nodes. 


We execute thousands of tasks per day and run several projects simultaneously. Our project manager keeps adding more and more work, clients are reaching out directly, etc - unplanned work is turning this into a 24/7 job. 

-	We have to deal with changes very often because we receive data from clients who are free to change anything at any point without warning. 
-	There is nothing to factor in the work I have to do with managing the infrastructure (stack files, testing new tools, etc). Actual crashes or other issues. 
-	I have no one internally to go to for any advice on anything technical or data related. Our team only acknowledges us getting data into the visualisation tool on time. There is zero interest in how it is done. 


Should this role typically be under the IT wing? I am considering doing a demo of our setup to someone from the IT org. I feel like it goes way beyond “loading Excel files into a visualisation tool”.",1563916247,9,0,3,dataengineering
What python framework / design-pattern should I use for a small job of (1) copying a file from an external drive to a laptop in minimum I/O time AND (2) simultaneously reading it into pytorch/whatever and running FFTs on rows and saving the result.,"The file is 400GB. FFTs need to be run on all the data. I want to do FFTs on CUDA and pytorch is most convenient way for me to do that. Rate limiting step is I/O of the harddrive/device. It will take 10mins to copy the file from the harddrive/device to laptop. I have a requirement that the processing also needs to have been mostly done in that time (non negotiable...). There is also extra processing to be done after the FFTs in certain edge cases that is O( n^3 )). No internet access possible and only one laptop available for processing.

I will outline a few ideas, but please advise if in your experience there is a better design pattern or framework!

1. Just copy the file from device to laptop (either with shutil or the user literally drags and drops it themself using Windows). And have a python process always running that will read that raw binary file as it is written and process it in chunks. This is possible as the IO speed of the harddrive is far less than the IO speed of the 500GB (or 1TB) SSD the laptop will have.

2. Use faust to read the file from harddrive and write it to SSD, while sending it to torch and graphics card for processing. https://faust.readthedocs.io/en/latest/

3. Use asyncio?",1563906346,1,0,2,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor.",1563891924,0,0,0,dataengineering
An interview about how the data mesh architectural and organizational pattern can lead to a more maintainable data platform,,1563890166,0,0,5,dataengineering
Beta launch of Band Protocol and CoinHatcher,,1563883734,0,0,16,dataengineering
Investigating the dataset,"Hi! Currently working as an intern and my task is to find the reason why the project performs poorly. There are several standards that need to be reached but, for some reason, most of them always remain unmet, and even if they reach standards, very rarely more than 2 standards out of 5 are met.

Currently I created several spreadsheets that contain information for separate locations (cities), I look at which users have success or not, etc. 

I plan to start evaluating the standard that is met the most and see why it can't meet the goal in the other cases, I hope I will be able to find 1 or 2 parameters that would affect the whole outcome for the corresponding standard.  


So, the thing is, my DS background is quite weak, and I just started the internship, but I am willing to learn more because I really enjoy it. 

&amp;#x200B;

What would be your suggestions to do? I am open to all suggestions and tips!",1563870224,1,0,1,dataengineering
Recommended learning resources for scaling up a data project?,[removed],1563823302,0,0,1,dataengineering
Quickbooks Desktop,[removed],1563820600,0,0,1,dataengineering
Band Protocol — A Protocol for Decentralized Data Governance,,1563796975,0,0,1,dataengineering
Just got my first DE job. What should I study?,[removed],1563727377,0,0,1,dataengineering
Optimizing Spark Job (spark-submit/shell),[removed],1563726624,0,0,1,dataengineering
Optimizing Spark Job (spark-submit/shell),,1563725176,0,0,1,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor.",1563715643,0,0,0,dataengineering
Path to tech savvy?,"Hello,

&amp;#x200B;

I'm looking for a roadmap to build a foundation of tech skills.  

&amp;#x200B;

I recently started a DE job with no background in computers/languages.  After 4 months I feel solidly proficient in SQL, and am looking to start acquiring the next pieces of a skill set.  I'm taking classes on Azure (the comany's cloud service), but Python/Apache/Bash/Batch/Hadoop all seem like worthwhile skills--all of which I'm set on learning in the future.  

&amp;#x200B;

However, it's a (somewhat) overwhelming amount of content, and there's definitely years of effort ahead.  Can any previous self-starters propose a logical itinerary of where to start?  And maybe within these modules, what sub-skills to start with?

&amp;#x200B;

Thanks!",1563642956,0,0,1,dataengineering
Does Title Matter?,[removed],1563602830,0,0,1,dataengineering
How to insert by overwrite to kudu?,,1563529438,0,0,1,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor.",1563518077,0,0,2,dataengineering
awesome-apache-airflow,,1563488178,12,0,18,dataengineering
What do you look for in a GOOD Data Science &amp; Engineering role?,,1563470257,3,0,5,dataengineering
Entry level dataset for becoming a data engineer?,"I want to study all the necessary materials to become a Data Engineer to expand my career options but I am not sure where to start.

Currently, I am working as a reporting analyst for a big corporation. My daily job involved data manipulation, data cleaning and data visualization with Tableau. I am comfortable with Python( I know how to write functions, loops, have a basic understanding of OOP, call third-party packages like Pandas, etc). But my knowledge base is not sufficient enough.

So what books should I read first or just start building Data engineering projects? I am aware that data engineers work involve Hadoop or Kafka which are used to handle huge amount of data(Which I don't have on my PC nor can my hard drive store it). So what project(large enough so that there is even a point to consider HDFS) should I try at a beginner level",1563393025,13,0,15,dataengineering
Mono Repo or Multi Repo,[removed],1563382318,0,0,1,dataengineering
Dell Boomi vs SSIS?,"Apologies if this isn't the correct forum.  I realize these tools have some fundamental differences, but in a simple use case of moving data from 100 Oracle tables into 100 SalesForce objects can anyone comment on any obvious pros and cons?  We already have SSIS/CozyRoc running on-prem and moving a handful of tables over, but management is interested in Boomi (long story) so I'm curious about other peoples' experiences.",1563377916,5,0,2,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,[removed],1563284839,0,0,1,dataengineering
"An interview about Clickhouse, an open source, columnar data warehouse built for massive scale and speed to enable interactive analytics",,1563242492,1,0,0,dataengineering
An interview about the Cloud Factory platform for data labeling and social good in developing nations,,1563242486,0,0,4,dataengineering
Industrializing batch ML algorithm using Apache Beam/Dataflow (on Google Cloud Platform),"Hello 

We plan to industrialize some batch ML Alogrithms using Apache Beam and Dataflow as a runner. 

The pipeline job would be something like 

* Read from GCS a .json file
* Compute the output of the algorithm on a JSON element
* Write the JSON elements into a file in GCS

The most interesting part is the second one. In order to be the most flexible, we agreed on a contract with the Data Science team. The algorithm should be serialized as a pickle which will have a predict method, see the code below 

    def predict(X):
    """"""
    :param X: a list of JSON objects representing data points. 
              Example:
              [{""DAY"": ""D1"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak""},
               {""DAY"": ""D2"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Strong"": ""Weak""},
                ....
               {""DAY"": ""Dn"", ""Outlook"": ""Overcast"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak""}]
    :type X: list of JSON objects 
    
    :return: a JSON list with the output of ML algorithm. 
             Example (classification play tennis game)
             [{""DAY"": ""D1"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak"",
               ""Go to court"": False},
              {""DAY"": ""D2"", ""Outlook"": ""Sunny"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Strong"": ""Weak"", 
               ""Go to court"": True },
                ....
              {""DAY"": ""Dn"", ""Outlook"": ""Overcast"", ""Temp"": ""Hot"", ""Humidity"": ""High"", ""Wind"": ""Weak"",
               ""Go to court""; False}]
    """"""

However Apache Beam is currently not supporting [Python 3](https://beam.apache.org/roadmap/#python-3-support) (Python 2 will be deprecated in 2020). In addition to that, the Java SDK is the most complete Beam SDK. 

A major thing to consider is the dependencies of the algorithm (some classification ML  uses pandas version X another regression ML depends on version Y of numpy ...etc)

Knowing that Dataflow handles dependencies  differently across SDKs:

* In Python, a requirment.txt must be specified [among other things to do](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/)
* In Java, it suffices to build a jar containing all the .pom dependencies  a.k.a [uber jar](https://github.com/lukecwik/incubator-beam/tree/dataflow_uber_jar)

Here are some solutions :

1. Use the Java SDK, because it is the most complete and because we don't want to use python 2 anymore, and instanciate the pickle\[1\] in Java and manage to pass a batch of JSON elements in the DoFn\[2\] to compute their score and get it back\[3\]
2. At worker initialization by Dataflow, a Docker image will be downloaded containing the ML algorithm and all its dependencies\[4\]. Using the Java SDK a batch of JSON elements in the DoFn will be passed to that container\[5\] and the output will be gathered back 

Are those viable solutions to tackle this use case ? Mabye they are too complex or even not feasible ? Maybe Dataflow/Apache Beam is not the way to go ?

We only have one constraint form the Data Science team: being able to use Python 3 and all the DS ecosystem that revolves around it (pandas, pytorch, scikit learn, numpy ...etc.) Maybe the pickle format is not the most suitable one to use ML alogrithm ? (eventhough we will also have to deal with Tenserflow algorithms, but I think it's a different subject that maybe needs a different pipeline)

&amp;#x200B;

Any help/hint will be much appreciated,

Many thanks  

&amp;#x200B;

This is me talking to myself: 

\[1\] no clue how to do that 

\[2\] the communication between Java and python maybe too expensive 

\[3\] how do we deal with the algorithm's dependencies that are mainly in python and inexpressible in Java (maven)

\[4\] is it possible ? how ? 

\[5\] may also be expensive",1563233210,2,0,2,dataengineering
Free hosted on demand Spark clusters,"Hello r/dataengineering!

&amp;#x200B;

We've been working on a service to provide deployed and managed Spark clusters to help data engineers, ML peeps, analysts deploy their Spark code easier than ever (as simple as one command in the terminal). [https://www.aidalabs.io/](https://www.aidalabs.io/)  There's a free cluster tier option available as well to test deploy your code to the cloud! I would love any feedback! PM me if you have any questions. We currently support Pyspark.",1563205977,0,0,3,dataengineering
Using pandas to copy dataframes to Postgres,"Hi, I am attempting to use a pandas dataframes to create a Postgres table, at first this code ran, but it didn't look like it actually created the table in the DB, any thoughts, here is the code: 

&amp;#x200B;

import psycopg2 as psy

import pandas as pd

from sqlalchemy import create\_engine

&amp;#x200B;

con = psy.connect('dbname = 'dbname' user = 'user'')

cur = con.cursor()

&amp;#x200B;

engine = create\_engine('postgresql+psycopg2://'user':'pw'@'myhost'/'mydb'')

&amp;#x200B;

df.to\_sql('df', engine, if\_exists='replace')

con.commit()

con.close()",1563203606,0,0,1,dataengineering
How to write unit tests for your SQL queries,,1563201905,2,0,25,dataengineering
How to get failure notification by email on Airflow? I already used smtp gmail but not work at all.,,1563188007,1,0,0,dataengineering
Introducing Dagster - Nick Schrock - Medium,,1563182277,16,0,0,dataengineering
https://medium.com/@schrockn/introducing-dagster-dbd28442b2b7,[removed],1563181393,0,0,1,dataengineering
What might be a good Master's project involving data engineering and big data to attract employers?,"Hey. A little background about me. I am a international student currently pursuing my Master's degree in Information Technology in the U.S. I have no professional experience apart from a 3 month summer internship as a data engineer where I dealt with building a dashboard. I have completed my AWS certified Big Data Specialty exam and AWS certified cloud practitioner certifications. I am creating my personal portfolio with ETL jobs involving AWS environment. In 6 months I am to start my project work to complete my Master's degree. What might be a good project that is complicated enough and use case specific that might help explaining my interest and experience in data engineering?

&amp;#x200B;

Since I don't have any professional experience, I'm worried as to how I can approach my job search. Kindly share your ideas. I don't mind spending time to research about complicated architectures and technologies to incorporate into my project. Any professional data engineers out there, kindly provide your suggestions as to what might help impress data engineering employers?",1563147600,0,0,1,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.," Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle.

Regard,

Graduating Student, a fellow redditor.",1563106876,0,0,1,dataengineering
Query Nested Json/Parquet Structure?,[removed],1563071103,0,0,1,dataengineering
Snowflake Cloud Data Warehouse,,1563064904,6,0,11,dataengineering
Junior Data Engineering Position Interview Tips for an APM?,"Hey everyone, 

I'm trying to get into a more technical role, and so I found out a couple of days ago that I have a technical call with a hiring manager Monday morning for a junior DE position that I applied to on a whim. My role right now is that of an associate product manager. So along with all my other duties, I assist in programming our data pipeline, but not I'm not super involved in the meat of the ETL stages. Since I work with Spark SQL I'm familiar with SQL queries, and I use pandas quite a bit as well for data explorations.

Since I only have a couple days to prepare, I've been cramming leetcode and doing their database and algorithm questions. Any other advice on how to prepare? My background is not in CS and in my current job, my coding doesn't get very deep. I'm kind of worried because of this, as I'm not great at being given a problem and coding up a solution, analyzing time/space complexity, etc. 

Any help is greatly appreciated!",1563055897,2,0,1,dataengineering
The Twelve-Factor App,"12 principles which provide guidance to build application that can scale easily : [https://12factor.net/](https://12factor.net/) .

I  guess many of you like me follow this principles implicitly, It is nice to put words on it.  


Sum up :

**1. Codebase:** Multiple apps sharing the same code is a violation of twelve-factor =&gt; share code via library

**2. Dependencies :** A twelve-factor app never relies on implicit existence of system-wide packages =&gt; use pip/virtualenv in python for instance

**3. Config:** Store config as constants in the code is violation of twelve-factor, which requires strict separation of config from code =&gt; stores config in environment variables

**4. Backing services:** A deploy of the twelve-factor app should be able to swap out a local MySQL database with one managed by a third party without any changes to the app’s code.

**5. Build, release, run:** The twelve-factor app uses strict separation between the build, release, and run stages

**6. Processes:** Twelve-factor processes are stateless and share-nothing

**7. Port binding:** The twelve-factor app is completely self-contained

**8. Concurrency:** Twelve-factor app processes should never daemonize or write PID files, instead  rely on the operating system’s process manager (such as systemd)

**9. Disposability:** processes can be started or stopped at a moment’s notice, processes shut down gracefully when they receive a SIGTERM signal from the process manager...

**10. Dev/Prod parity:** The twelve-factor app is designed for continuous deployment by keeping the gap between development and production small

**11. Logs:** A twelve-factor app never concerns itself with routing or storage of its output stream

**12. Admin processes:** Twelve-factor strongly favors languages which provide a REPL shell out of the box",1563042447,5,0,1,dataengineering
How to keep my 'airflow scheduler' running?,"After I disconnect from ssh session, my airflow scheduler status changed to S on ps aux and actually didn't run my DOG. I have run it by daemon process.",1563041416,5,0,0,dataengineering
The Twelve-Factor App,[removed],1563041315,0,0,1,dataengineering
Any interest in building out a side-data product/project in a team?,"I see a lot of ""how do I break into data science/engineering"" posts on here and am wondering if there'd be interest in building out some kind of project together in a small-ish team. I don't really have ideas for projects at the moment but I'm sure a team could come up with some. Might be better than working through those online courses, no?",1563030873,14,0,8,dataengineering
Looking for advice on how to get educated and break into data engineering,"Hi, I have been in data analytics for several years, but I would really like to break into data engineering, and eventually shoot for a data architect role. I have an MBA, I know SQL very well, some Python and various other BI and ETL tools like SSIS and Tableau. I have been applying for data engineering jobs, but it seems clear I just don't have the skills they are looking for. As far as education goes, which path do you recommend: going all in and getting an online masters in CS, or taking online courses like dataquest and Udacity and building up a Github portfolio? I am currently taking the dataquest data engineering course.",1562977566,13,0,15,dataengineering
Вакансия,"**Data Engineer**

В нашу команду требуется опытный Data Engineer, которому интересно консолидировать информацию из различных информационных систем. Наш сотрудник будет принимать активное участие в создании и дальнейшем развитии нового решения.

**Требования:**

\- аналогичный опыт от 1-го года, по разработке ETL – процессов  (извлечения, подготовки, загрузки данных)

\- высшее техническое/математическое образование

**Профессиональные навыки:**

\- Уверенное владение Python (в т.ч. библиотеки Pandas, Numpy, Scipy, Scikit-Learn, Seaborn), Scala или Java

\- уверенные знания SQL

\- знание и практический опыт работы с алгоритмами машинного обучения (machine learning), построения математических моделей (нейронные сети, логистические регрессии, кластеризация, регрессионный, факторный, дисперсионный и корреляционный анализы)

\- знание инструментов, библиотек визуализации данных

\- опыт разработки проектов с анализом текстов или изображений с нуля

\- технический английский

**Обязанности:**

\- проектирование, разработка и поддержка инфраструктуры для хранения и обработки больших данных

\- обеспечение полноты и доступности данных для решения задач в области статистического анализа

\- проектирование и настройка систем отчетности для разовых и периодических выгрузок данных

\- участие в задачах анализа систем источников, составление моделей данных

\- работа в команде по гибким методологиям ведения проекта

**Условия:**

\- оформление согласно ТК

\- график работы с 09.00 до 18.00 (обсуждается)

\- уровень оплаты  обсуждается на интервью

\- молодой и дружный коллектив

\- офис в шаговой доступности от метро Красносельская, Бауманская

\- здание в стиле Loft, офис Open Space

\- кухня с необходимой техникой, чаем, кофе, конфетами

**О нас:**

Молодой международный исследовательский проект по экономике Юго-Восточной Азии",1562923915,2,0,0,dataengineering
"Apache Airflow Cluster, Tasks and Node Affinity",[removed],1562912931,0,0,1,dataengineering
Should we add a wiki to r/dataengineering?,[removed],1562883009,5,0,1,dataengineering
Anyone going to use the dotnet spark driver?,"I'm quite excited by this:

&amp;#x200B;

 [https://github.com/dotnet/spark](https://github.com/dotnet/spark) 

&amp;#x200B;

Mainly because I love spark and struggle with scala (i'm ok at it) and prefer c# to python.

&amp;#x200B;

Is anyone else planning on using it? (I can imagine no ha ha)",1562878315,2,0,3,dataengineering
[Seeking Advice] What are some good database architectures/platforms for organizing medium-large scientific datasets?,,1562867283,4,0,3,dataengineering
[Request][Repost] Asking for assistance filling up a short survey for a Big Data project.,[removed],1562833707,0,0,1,dataengineering
There's a series of videos from AWS where people discuss how they leverage different AWS services for their architecture. Seems like marketing for AWS but some good lessons to learn,,1562790781,0,0,18,dataengineering
Put your data to work: Amazon Redshift vs Snowflake,,1562777556,0,0,1,dataengineering
Put your data to work: Amazon Redshift vs Snowflake,[https://blog.xmartlabs.com/2019/07/04/redshift-snowflake/](https://blog.xmartlabs.com/2019/07/04/redshift-snowflake/),1562771614,0,0,0,dataengineering
Dataform scores $2M to build an ‘operating system’ for data warehouses – TechCrunch,,1562765066,11,0,6,dataengineering
Data transformation - should I do this in Excel or is Python/Pandas/Something else a better fit?,[removed],1562748681,1,0,1,dataengineering
"Json to ORC, no pyspark","Hi guys!

This is my first post so please let me know if this is the wrong place to post and if there is another forum I should post to.

Question: 

I'm trying to convert JSON files to ORC using python without using pyspark because pyspark doesn't run on AWS Lambda \[""/dev/fd/62 doesn't exist"" error\]

\*There is a github hack involving spinning up EC2, but it's not ideal

Is there any other way (pyarrow uses pyspark and pandas doesn't support writing to ORC so both don't work) to convert JSON to ORC in python?

By the way, I can't use AWS Firehose because it doesn't allow for partitioning the S3 file-folders as necessary.",1562695297,11,0,2,dataengineering
Awesome free open source software for data pipeline testing: Great Expectations,"Check out this OSS for automated data pipeline testing with a lot of solid documentation and also has an active slack channel. It's still in its early stages but there are some big updates coming over the next few weeks.

[Website](https://greatexpectations.io/?utm_source=reddit&amp;utm_medium=post&amp;utm_name=user-testing&amp;utm_content=ml-v1) (this is blocked in Russia but we are working on solving this check out the github and docs in the meantime)

[Docs](http://docs.greatexpectations.io/en/latest/?utm_source=reddit&amp;utm_medium=post&amp;utm_name=user-testing&amp;utm_content=ml-v1)

[Github](https://github.com/great-expectations/great_expectations)

[Hello world blog](https://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a)

&amp;#x200B;

**Also---** We are looking for user testers to get some feedback. In exchange for feedback on our beta data context, profiling, and data documentation features we want to provide you one on one video conference onboarding assistance to ensure successful integration with your project. Feel free to comment if you have any questions otherwise you can sign up here: [https://greatexpectations.typeform.com/to/mN4UdQ](https://greatexpectations.typeform.com/to/mN4UdQ)",1562686598,2,0,15,dataengineering
How to approach data engineering in a startup?,[removed],1562641716,6,0,1,dataengineering
Travis CI + Redshift via SSH help?,"We have a Redshift cluster with staging data that we'll use to validate our transformation models. We'd like to automate this step so I'm trying to include Travis CI integration. In short, Travis yml contains a script to build an SSH tunnel and then connect to the cluster, then run the transformation validations.

The problem is that the connection to the cluster fails on Travis, but succeeds locally. I assume this has to do with permissions and where the call to the cluster originates. The error is :

    FATAL: no pg_hba.conf entry for host ""xxx.xxx.xxx.xx"", user ""guest"", database ""masterdb"", SSL off

In postgresql there's a file called **pg\_hba.conf** that lets you configure users and the IP addresses from which they can call to the cluster. I don't know where to find this in Redshift.

Does anyone have experience SSHing into a Redshift cluster via Travis CI?",1562608659,2,0,1,dataengineering
[Request][Repost] Asking for assistance in a Big Data project.,"Dear Data Engineering SubReddit,

Good day, I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post. This is also my first time doing any sort of research on this subject, so the question are a little on the amateur side, please be gentle. 

Regard,

Graduating Student, a fellow redditor.",1562600224,0,0,1,dataengineering
Any advice for creating a DWH from API response collection?,[removed],1562531603,0,0,1,dataengineering
Are data engineers synonymous with data custodians?,,1562463427,4,0,2,dataengineering
Best free data engineering online resources to study?,"I'm currently a data scientist with an interest in data engineering.  I was wondering if there are any good online courses that are free for data engineering.  I believe some udemy links were posted here recently, but it costs money.",1562437707,3,0,27,dataengineering
Study Guide Recommendations for a new DE?,[removed],1562434199,0,0,1,dataengineering
[Request][Repost] Asking for assistance in a Big Data project.,"Dear Data Engineering SubReddit,

&amp;#x200B;

Good day, my name is Aw Ming Yeh and I am a student in UTAR Kampar Malaysia. Currently I am conducting a study/project on the usability issues associated with Big Data and it would be ideal if I can get the input from experienced people in regards to Big Data. With that said here is a link to the survey that I am conducting. ""[https://forms.gle/J2kLth11eYPNyNUC7](https://forms.gle/J2kLth11eYPNyNUC7)"" Thank you for reading this and much appreciation to the respondents. As a special thanks I will include the organisations name(or not if you don't wish to) in my thesis upon completing it. Again I wish to say thank you for reading and I bid you a good day.

&amp;#x200B;

PS. I am sorry if this make it seem like I am begging but I don't really have anywhere else to go for respondent to such highly specific question. If you know a better place to post this kind of request please don't hesitate to pm me or comment on this post.

&amp;#x200B;

Regard,

Graduating Student, a fellow redditor.",1562349480,5,0,5,dataengineering
Looking for suggestion as my next step towards becoming a competent data engineer,"Hi All,

I've around 9 years of total experience as an ETL guy. I've mostly worked with a GUI tool called Ab&gt;Initio. I've worked in few development and ETL Application support projects. My goal is to become a good data engineer. My current organization uses Talend ETL(Just the DI part as our application is not a correct fit for big data). I have a working understanding of java code which i picked up while learning Talend.

I've been lurking in many forums to get a feel on what's current trend in market and what aligns with my personal choices in career. I started learning python and love it.I want to make a career in tier 1 company (FAANG for example) as a data engineer.

I'm looking for help on how to realize my dream. I would appreciate if someone can give me a path or mentor me on becoming a good data engineer.

Based on my research,this is the path I've in mind.(Please note that I've just read through the overviews of tech stack to come up with this flow)

First Step :-
Python ( Because you need a language to code on.I have understanding of Java but I prefer python because of the simplicity. I personally found python to be friendlier and fun to learn than Java or Scala)
Resources that i am using  :- Python fundamentals in Pluralsight &amp; Automate the boring stuff with python(Udemy and book).

Second Step :-
Spark..specifically pyspark (Because this is probably the most used framework from what i understand.)
I see flink being mentioned here but would appreciate if you guys chip in with your two cents.

Resources that i've identified :- Udemy : Taming Big Data with Apache Spark and Python - Hands On!

NOTE : Please point me to any additional resources that helped you.

Third Step : Learn Airflow. Honestly, I don't know and yet to research on what would be the best resource to learn Airflow.

In Parallel, I also want to sharpen my SQL skills from Leetcode.

Additional Skill : I've acquired AWS certified cloud practitioner cert and aim to complete the Solution Arch Cert in near future because cloud is future.

Please comment on what your thoughts are. I'm really looking at you guys as a mentor.

THANK YOU.",1562315213,9,0,11,dataengineering
Any good documents on spark configurations,[removed],1562307894,2,0,1,dataengineering
Data Engineer vs. Senior Data Warehouse Engineer,[removed],1562265493,7,0,1,dataengineering
Resources to Lead a Data Warehouse / Data Organization Effort,[removed],1562252702,3,0,1,dataengineering
Which Udemy PySpark course should I choose?,"Hi Folks,

Enough with fooling around (&amp; getting stumped) with PySpark.  Its time to do some tutorials.

I notice that there are two reasonably well regarded courses on Udemy:

[Taming Big Data With Apache Spark And Python Hands On](https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/)

[Spark and Python for Big Data with PySpark](https://www.udemy.com/course/spark-and-python-for-big-data-with-pyspark/)

I like that the second one has a bit more of a AWS slant, but it would mean having to find someone to run up Ubuntu.

Has anyone had any experience with either of the above?

Cheers",1562238171,4,0,9,dataengineering
[Research] Can I interview you? Anon option available,"Hi wonderful community of Data Engineers! I am a Product person and my company is building a platform that data engineers will be the main user of. I am looking to interview \~10-15 Data Engineers to create an accurate, detailed user persona that will act as a guiding light for building out features and functionality. 

&amp;#x200B;

I am posting here because I've been struggling to find data engineers in my local community and network. I've read through many posts in your community and y'all seem to be an open and supportive bunch, so I'm hoping there might be some interest in helping out.

&amp;#x200B;

In return for an interview (over the phone, zoom or slack), I will gladly send you some swag, or buy you a coffee through [https://www.buymeacoffee.com/](https://www.buymeacoffee.com/), or make a donation on your behalf to the non-profit of your choosing. I've been told my interviews can be therapeutic :D and one interviewee even left with a series of blogs our conversation inspired him to write! 

&amp;#x200B;

**If you're open to chatting, please feel free to schedule some time with me via calendly at** [**https://calendly.com/skingkong**](https://calendly.com/skingkong)**, or join GoLang Slack** [**https://invite.slack.golangbridge.org/**](https://invite.slack.golangbridge.org/) **where you can find me @sking.**

&amp;#x200B;

**Anonymous Alternative:**

If you are open to sharing, but not interested in speaking directly, I have put together an anonymous typeform that I welcome you to check out. It will take anywhere from 10-20 minutes to fill out (purely depending on how much you share), and consists of 10 questions – 4 multiple choice and 6 long-form answers. If you want to provide your email at the end, the same rewards listed above are available to you :) 

[https://pilosa.typeform.com/to/pSufzS](https://pilosa.typeform.com/to/pSufzS) 

*If anyone is having a difficulty reading this typeform due to the color combination - please comment and I will re-set it!*

&amp;#x200B;

Thank you for your consideration and getting through this long post!! heh ':D",1562178868,4,0,6,dataengineering
Difference Between Data Engineer and ETL Dev?,"I don't see a huge difference between the two. The only thing I notice is that ETL Dev job descriptions usually require less programming and more proprietary tools. 

Is it bad to take an ETL Dev job if you want to be a Data Engineer or BI? Or is it all in the same realm?",1562158278,3,0,0,dataengineering
An interview about testing the limits of scaling Kafka and Cassandra for real-time anomaly detection at Instaclustr,,1562080838,0,0,2,dataengineering
Data Modeling Question,[removed],1562010178,4,0,1,dataengineering
Decisions when creating your Data Lake,[removed],1562000655,2,0,1,dataengineering
"Your input is needed. Please help, nobody wants to do my survey T.T",[removed],1561976972,0,0,1,dataengineering
GUIs vs Code based ETL,"I am fairly new to the BI/ Data Engineering world and have mainly done pipeline design/ ‘DevOps’ (logging, alerting, building internal tools). I’m self taught and have been using GUI based tools like SSIS, DataStage, Matillion, etc. which connect to S3, data warehouses, or tableau. 

Recently I’ve been incorporating some Python with these tools and I love the added flexibility (and programming in general). I certainly plan on learning more Python for analysis/ pet projects. Do you all think that it is worth it to invest in learning code based etl tools? Either for fun, future career opportunities, or any other reason? I’d love to hear any other thoughts comparing or related to GUI/ code based platforms",1561734140,26,0,11,dataengineering
How to store dynamic customer data?,"We are building a B2B app that requires us to copy out a large number of each customer's database tables in order to run a bunch of modeling on. In other words, each client's dataset is different enough that we can't normalize it into a set number of tables on our end. This isn't something I've done before and I'm not certain the best way to get all their data into our system without dynamically creating tables for each customer in our PostgresQL DB.

Is there a better more standard way of handling this scenario? I've struggled to get any results from Google, but I suspect my GoogleFu is subpar.

Any help is appreciated and please yell at me if I'm leaving any info out, so I can update the post.",1561723682,4,0,1,dataengineering
How to start dynamic tables?,"We are building a B2B app that requires us to copy out a large number of each customer's database tables in order to run a bunch of modeling on. In other words, each client's dataset is different enough that we can't normalize it into a set number of tables on our end. This isn't something I've done before and I'm not certain the best way to get all their data into our system without dynamically creating tables for each customer in our PostgresQL DB. 

Is there a better more standard way of handling this scenario? I've struggled to get any results from Google, but I suspect my GoogleFu is subpar.

Any help is appreciated and please yell at me if I'm leaving any info out, so I can update the post.",1561722490,0,0,1,dataengineering
Pyspark with Testing book recommendation.,"Hi, 

I am a data engineer and would love to learn pyspark with good testing practices and examples. Are there any good books anyone can recommend that does this. Most books just read files and do the transformation in all one go without good software engineering practices.I am not a fan of the notebook style approach because of the lack of testing for my CI/CD Pipeline.",1561690426,3,0,9,dataengineering
Building a Data Warehouse From Scratch,[removed],1561677114,1,0,1,dataengineering
"How to get ""familiar"" with the many enterprise tools/technologies out there?",[removed],1561605345,1,0,1,dataengineering
How do you manage the quality of your data?,"Is there a specific division, team or person to do that at your work place? 

Like, in general software engineering, there are QAs or specific teams to manage the quality of the software; I've always wondered if this is the same for data engineering?",1561594494,0,0,1,dataengineering
Alternatives to Alteryx production pipeline? (on windows),"**tl;dr — I’d appreciate any tips/advice/resources for building a production pipeline. **

Hi all

I’m a data science/engineering intern at a small company. One of the tasks they have me working on is improving the production pipeline for some data/modeling. 

A year ago they hired a consultant to come in and set up this pipeline — it’s a hodgepodge of Python and R scripts all put together in [Alteryx](www.alteryx.com). Apparently Alteryx doesn’t play nicely with Python, so the consultant switched between the two at different points in the pipeline. This has caused some issues for the staff here who are unfamiliar with R and have some Python knowledge. Also Alteryx is just a pain to deal with in general (plus they only have one license so only one person can log in to the windows VM at a time). 

The entire company is Windows 7 &amp; 10, and The data lives on local SAP HANA servers. 

I’m exploring the option of creating a cheaper, more flexible production pipeline on Windows, but I don’t really know where to start. 

I’d appreciate any tips/advice/resources for building a production pipeline. 

Thanks!",1561576268,3,0,3,dataengineering
Testing data quality with SQL assertions,,1561567832,6,0,12,dataengineering
FREE hands-on Workshop in Palo Alto - Create a data warehouse in Amazon Redshift with Etleap and AWS,"Calling analytics-focused data engineers!   


We're joining forces w. @awscloud in Palo Alto on 7/16 to give a FREE hands-on workshop where you'll learn how to set up an analytics-ready data warehouse on Redshift.

&amp;#x200B;

Save your seat today by registering: [https://info.etleap.com/devdaysjuly16](https://info.etleap.com/devdaysjuly16)",1561561736,1,0,3,dataengineering
Consulting to Industry - Resume Review,[removed],1561513633,0,0,1,dataengineering
Product Owner/Manager of Data Engineering/Platforms,"I was wondering if we have any folks here who are either in a product management role for data engineering / data platforms OR have someone playing this role in their organization.  

I am currently a Product Owner for the Data team at my current company and admittedly, at the current time, my day-to-day doesn't really look like traditional PO duties.  I've been looking around online for what a PO role looks like in the context of data, and it seems that it can vary depending on the type of business and data products.  

I'm mostly curious for others' experience in this area, so I can gain a bit of understanding of how this role works within other organizations, as I am trying to grow my role beyond what is effectively glorified project management :)

Appreciate any input from the community! Thank you!",1561511628,5,0,6,dataengineering
An interview about how the Prefect workflow engine unifies the needs of data engineers and data scientists with a pure Python API,,1561480527,0,0,6,dataengineering
Noob Question: Difference in day to day of Data Engineer vs ML engineer,I am still new to data world and from what I know that as  DE most our days are spent in tuning systems and making things faster and mostly delaing with distributed systems. But I am not quite sure what a machine learning engineer job entails. Can some one please clarify. TIA.,1561392918,10,0,11,dataengineering
Words relations to their broader categories,"I am trying to develop an algorithm that takes the given keyword and  assigns the keyword's rankings according to the created categories/classification. For instance, if I type ""calculus"", the rankings for this keyword in the category ""math"" will be something like ""0.9"" and for the category ""english"" it will be ""0.1"".  I was wondering if someone has already came up with the database with the similar information about relations between the words and their classes in a mathematical way.",1561391215,8,0,1,dataengineering
"""Guerrilla Analytics"" advocates - implementing best practices for ""Data Receipt"" stage","Hi,

I'm building a software system for my organization with the goal of automating and standardizing provenance/audit, ETL, and build process for our data warehouse, and combining it with a data science ""lab"" pipeline similar to what's in Guerrilla Analytics and also seen in other ""agile data science"" books.

The main goals for the effort are:
* new and existing data provenance are maintained, data are audit-able from initial introduction through to end products (either in our data warehouse, or at the end stages of a data science effort)
* it's fast and easy to introduce new data into our ecosystem / data warehouse, and to create tests and manage risk around new and exist data 
* it's fast and easy to create new data science ""lab"" efforts: environments, templates, version control, containers etc are set up automatically, best practices for folder structures and conventions are followed, etc

Ridge's book suggests many practices for the Data Receipt stage, and I wonder if and how you folks may be implementing the ideas.

* Received data storage and UIDs - are you generating and storing what would otherwise be locally-stored data UIDs, i.e. metadata, in a central place?
* Version control - how are you version controlling your data? Do you centralize this step / process?
* ""Data logs"" - this practice involves storing information about the data and provenance in files kept with the data. How do you use yours - do you use a wiki? Do you use google sheets?",1561388123,0,0,2,dataengineering
Any thoughts on handling query optimization questions in the interview? I know EXPLAIN plan is a way to go about it but what areas of explain plan to stress on,,1561326986,6,0,12,dataengineering
Design - How to come up with raw numbers?,"I'm looking to design a data pipeline and I am finding it difficult to get actual numbers. I'm not talking about the big ""design patterns"" questions, but rather things like:  


1. If I deploy this flask API on the cloud, what would it be the TPS (Transactions per second)?
2. If I connect it to a mysql / nosql database, what is the TPS?
3. If it connects to a separate nodejs application, how many TPS can it handle? If I bundle / should I bundle the two on the same server to get a higher / lower tps? How much better / worse would it be?  


I am well aware that all this will depend on the application itself, what kind of server (ram / processor / network bandwidth). All this adds to the complexity.  


What I am asking is, is there a way to figure these out just looking at the specifications of the various services and some internal testing of the application? Or is a full deployment and throughput testing necessary? I'm looking for actual raw numbers here. I think when I understand the raw numbers better then I can make design pattern questions (should it be monolithic or microservices, what kind of database, etc) better.

&amp;#x200B;

Thank you.",1561278361,9,0,1,dataengineering
Is there any specific reason to learn Dataframe API for spark inspite of knowing the SQL version?,I have quite a bit of experience with SQL and pandas as well. Now I intend to learn spark to add to my skill set. I see that you can do all the data manipulation using spark SQL which is just SQL language. I intend to focus on learning that rather than learning both that as well as the Pandas like dataframe api. I am reluctant because I'm pretty sure I'll get confused with pandas and mess up with the minute variations in terms of syntax. Is it okay to concentrate only on spark SQL or is it essential to learn the pandas like method also?,1561243588,16,0,9,dataengineering
Managing raw data files on Docker Swarm,[removed],1561180771,0,0,1,dataengineering
Has anyone emulated Netflix' use of Notebooks?,I'm hoping for insight on the process that Netflix took in making [Notebooks so handy within the company](https://medium.com/netflix-techblog/notebook-innovation-591ee3221233). Does anyone here have experience trying to set up something like this?,1561082530,18,0,17,dataengineering
Announcing Astronomer v0.9 - Platform to run Airflow on Kubernetes,,1561068226,0,0,8,dataengineering
"Is there such a thing as ""Entry Level Data Engineer""?",It seems most of the job descriptions out there point to a minimum of 2 years of work ex. I would love to hear about fellow redditors' stories about bagging that first DE job.,1561063522,35,0,6,dataengineering
Vehicle Telematics,"Hi All,

&amp;#x200B;

This is my second part of the series ""Telematics"". Please go through it and let me know you feedback [https://medium.com/@ibnipun10/vehicle-telematics-adaa1648c89b](https://medium.com/@ibnipun10/vehicle-telematics-adaa1648c89b)",1561023889,12,0,7,dataengineering
Azure Specialisation,My company is planning to move to Azure. I have never worked on Azure but have worked extensively of AWS. Can someone provide me any free links wherein I can see all the services and how they are used in Azure from a data engineering perspective,1561020021,0,0,1,dataengineering
Anyone have experience in using Delta Lake in production?,"[Delta Lake](https://delta.io/) is an [open source storage layer](https://github.com/delta-io/delta) that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing.",1561014408,0,0,5,dataengineering
Conferences on IoT,Can anyone suggest good IoT conferences around the work related to big dat architecture? Would be very happy to listen to others on how they are leveraging big data and cloud services for IoT data,1561012386,1,0,1,dataengineering
10 Days to Become a Google Cloud Certified Professional Data Engineer,"I just wrote a post about my experience studying for and taking the Google Cloud Certified Professional Data Engineer Exam. Hope you find it helpful.

[https://towardsdatascience.com/10-days-to-become-a-google-cloud-certified-professional-data-engineer-fdb6c401f8e0?source=friends\_link&amp;sk=84b9c3f78772308e0407e28af4819324](https://towardsdatascience.com/10-days-to-become-a-google-cloud-certified-professional-data-engineer-fdb6c401f8e0?source=friends_link&amp;sk=84b9c3f78772308e0407e28af4819324)

Feedback welcome!",1560988901,9,0,17,dataengineering
Getting Your Programming Skills Ready for Data Engineering,,1560960719,6,0,0,dataengineering
Question about Airflow and Version Control,"It kind of bothers me that when you change a DAG there's no record of it, and it can look like past runs used the new DAG.

I mean, you can VC the underlying Python code of course, but in the Airflow interface it looks like if you want to make that distinction you need to create an entirely new DAG with a new name.

This is giving me flashbacks of multiple Excel files I would get from BI with names like Final2CopyFinal.xlsx, i.e. version control by copying, changing and suffix naming.

Anyone run across this issue and/or have any ideas?",1560947118,7,0,2,dataengineering
Automate executing AWS Athena queries and moving the results around S3 with Airflow,,1560931062,2,0,13,dataengineering
Spark interviewing content?,[removed],1560921688,0,0,1,dataengineering
Can an ETL Developer Become a Data Engineer?,"Would ETL Developer be a good career path to becoming a Data Engineer?

From what I've seen, most job descriptions say they use SSIS. I don't know if this would hurt being able to transition.

I'm currently a Data Analyst who wants to switch to Data Engineer, but I have been having hard time cause it seems like a rather difficult switch. I figured I might have an easier time if I take an ETL job first.",1560872200,35,0,14,dataengineering
Minimalist Guide to Lossless Compression,,1560865686,0,0,3,dataengineering
The right tool for the job - producing a delta from delimited data sets,"Hi,

I am currently investigating what it would take to migrate away from our current MS BI stack and migrating to the cloud.

One of the challenges that I have is producing a delta of our current data.  In our current iteration, we daily output full tables from Sybase ASE via BCP, stage the data into SQL server and upsert it into a database which stores all records as type 2.  Individual tables can be up to 80 million records and 30 GB outputted as CSV.

What we now want to do is produce a Parquet file for each of these days, but as delta's only (mainly inserts and updates, but have the ability to deal with deletes as needed).  This processing needs to be done on premises.  

Considering I am new to the big data tool kit, I am trying to determine what the best tool for the job is.  I had considered a kind of lift and shift of the process to Postgresql, but would like to determine whether using Spark or Python/PANDA instead.  As I don't yet have enough experience with these tools, I am finding it hard to determine whether they will quickly do what I want with a data set of this size, compared to using a DBMS.

Does anyone have any thoughts or opinions on which way I should look?

Cheers",1560858442,10,0,2,dataengineering
What type of engineer can do this? What skills and software experience would they need?,[removed],1560843770,0,0,1,dataengineering
Python/Pyspark for relational database engineer," Hello,

I have worked with relational databases (Teradata, Oracle) and have mostly used ETL tools(Informatica, Alteryx) for data manipulation and cleaning. I have no programming experience( R, Python or JAVA).

&amp;#x200B;

I have experience in handling 1-2 TB of data using relational databases like Teradata, Snowflake etc. My company has an internal Databricks environment. I have never used it but I did see that Spark SQL as an option to manipulate data. I would like to learn Databricks in order to handle larger datasets and improve my skills.

Since most of my data engineering work is around creating complex variables. Should I start with Python or Pyspark ? I read that the commands are different for both. Appreciate if anyone can share good links (paid or free)for a beginner like me. 

&amp;#x200B;

Thanks!",1560835528,4,0,1,dataengineering
A blog post on why we use data warehouses?,"Hey guys. I wrote a post on medium explaining why we use data warehouses. 

[https://medium.com/@vijayrajsaravanan/if-you-are-reading-this-post-you-are-looking-to-understand-what-data-engineering-is-how-it-works-aaf8d4db03e0](https://medium.com/@vijayrajsaravanan/if-you-are-reading-this-post-you-are-looking-to-understand-what-data-engineering-is-how-it-works-aaf8d4db03e0)

&amp;#x200B;

Kindly provide suggestions if you find discrepancies and do clap for the post if you like it.",1560823312,2,0,5,dataengineering
[Question] How to configure and run a Spark Job on Spark Standalone Cluster (&amp; avoid insufficient memory problem),"Hi, I managed to run a cluster for 1-master and 4 workers.   
  When I navigate to Spark UI, I find the following resources.   


* **URL:** spark://spark-master:7077
* **Alive Workers:** 4
* **Cores in use:** 16 Total, 0 Used
* **Memory in use:** 246.5 GB Total, 0.0 B Used
* **Applications:** 0 Running, 0 Completed
* **Drivers:** 0 Running, 0 Completed
* **Status:** ALIVE

When I try to run a spark job using the following parameters: 

&gt;spark = SparkSession.builder.master(""spark://spark-master:7077"").appName(""test"").config(""spark.driver.memory"", ""8g"").config(""spark.executor.memory"", ""8g"").config(""spark.cores.max"", 16)

When I launch it, I can see that the Cores in all the workers have been used (4 CPUs), along with 8g RAM. However, I get the following error: 

&gt;Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory

I tried to use 2 CPUs for every worker, but I still see the problem. When I try to run on a local mode  

&gt;spark = SparkSession.builder.master(""local\[\*\]"").appName(""test"").

Things work smoothly on a local mode. However, it's slow and I want to make use of distributed environment.   
Any thoughts on how to dubug the problem?  It seems that it remained \[an unanswered question for many ppl\]([https://github.com/databricks/spark-knowledgebase/issues/9](https://github.com/databricks/spark-knowledgebase/issues/9))

  
P.S: I don't want to use Yarn at the moment. I am trying to use Spark-Standalone mode.",1560809960,3,0,4,dataengineering
"Is there such thing as a ""static"" data pipeline?","I'm relatively new to DE as a discipline and I'm trying to figure out if my past projects remotely resemble contemporary DE practices, especially building data pipelines. Basically, I'm curious to know if there's such thing as a data pipeline that only has static data (versus dynamic data - i.e., clickstream data, stuff you get from APIs, etc.).

I had a lot of projects in the past that involved sourcing and extracting data from static datasets, mostly government open data portals and the like, so lots of existing text/CSV, JSON, etc. files downloaded off the web. Would extracting, cleaning, and loading this type of static data into a relational DB count as performing ETL or building a pipeline? Or is there a more appropriate term for this type of work?",1560806441,7,0,3,dataengineering
A conversation with the architect of Delta Lake on the challenges of building a sustainable data lake at scale,,1560786864,0,0,5,dataengineering
How do you QA data at your company?,How do you QA data in your company? What structures or pipeline have you implemented to test the quality and correctness of data in your databases?,1560763204,7,0,16,dataengineering
"""the Azure Data Engineer""",[removed],1560751769,0,0,1,dataengineering
Need Resources for Advanced Airflow Techniques,"Hi,

&amp;#x200B;

I am currently working as a Data Engineer. My first project is almost finishing up. It involved creating an Airflow pipeline for Ingestion and processing data through various databases. Now, I personally feel that I could have done a better job at designing the pipeline, had I known some more case studies, industry standards, advanced techniques, etc. So, if anyone can point me to any resources like case studies with Airflow, end-to-end examples, design patterns to follow, etc., that would be great. In fact, any resources concerning orchestration would do as well. Any help would be appreciated.

&amp;#x200B;

Thanks!",1560707519,1,0,1,dataengineering
Learn data engineering website,"Hi everyone! While myself and a friend explored the field of data engineering, we realised that there are a ton of useful resources out there, but they're scattered all over the place. So we put them all together at [https://www.learndata.engineering/](https://www.learndata.engineering/) \- nothing fancy but hope it helps and makes your life easier.

Would value your input, particularly around any content you would like to see added (appreciate that there is always something new to add or learn). It's still work in progress and we work on it when we have time. Thank you!",1560706784,3,0,28,dataengineering
The data engineering cookbook,"A beginner friendly resource to know about data engineering.
https://github.com/andkret/Cookbook",1560674980,9,0,42,dataengineering
The data engineering cookbook,,1560674004,0,0,1,dataengineering
"Warning: stay far, far away from Udacity's Data Engineering Nanodegree.","Where to start? I'll just hit the highlights.

&amp;#x200B;

* This is the first time they're giving this nanodegree, so I'm one of the 'test subjects'. Some kinks that need to be worked out of the system are to be expected, but this goes beyond the pale. I have heard quite positive things about their other nanodegrees, and indeed the other Slack channels are not raging pits of despair like the data engineering one.
* They have a first-seven-day only quit-and-refund policy. It was not obvious until we started doing the first project, which I doubt anyone started in the first seven days, how extremely terrible this course was.
* The idea is to use different techniques to engineer data pipelines for a music streaming company, using the metadata of their songs (a subset of about 14,000 songs from from the Million Song Dataset) and their log files (automatically generated from some Scala script on GitHub, each one logging a user interaction with the service, most but not all of which are songplays). We're supposed to use different techniques ""as the company grows""... \*but it's always the same dataset\* with one month of user logs, about 9,000 lines. First we use a simple local Postgres server, then Cassandra, then we build a data warehouse on Amazon Redshift, then we create a DataLake with Spark (well, \*sort of\* a datalake, S3-to-S3, no Elastic MapReduce), then we build a pipeline to create a better DataLake (hopefully with EMR? I'm just starting that section) with Airflow. So yep, I'm distributing datasets with 9,000 records with Cassandra and Spark.
* So the natural thing to do is to reshape all of that data into a star schema, with a songplays fact table, and song and play metadata in dimension tables. The problem is: only about 1% (if that) of rows in the fact table match a song in the database. So I'm thinking they chose their songplays songs from the entire Million Song Dataset and then realized they didn't have the capacity to handle it all so chose the first 14,000 so it looks like 99% of the songs their users play aren't in their dataset at all!
* About that million song dataset... they seem to have some hashing algorithm generating filenames for the songs, and its always first three letters A-Z, then a bunch of characters A-Z or 0-9. So the data source, for their own doubtlessly good reasons, puts ONE song per json file, and puts them in a NESTED folder structures, e.g. /A/A/A/AAA0F8DS796FE8C.json. These are put in an S3 bucket, which of course doesn't really have folders, just filenames with slashes. For one project they increased the number of songs to 300,000, realized it was taking students more than a day to parse them, and two weeks later went back to the original 14,000.
* About that million song dataset... so those 14,000 files which are (I checked) in the original source properly UTF-8 encoded JSON (there are a lot of band names and song names with non-ascii characters), were somehow re-encoded into Windows Latin-1, THEN re-encoded into UTF-8, THEN re-encoded into Latin-1 AGAIN. And of course the online notebooks provided by Udacity read them as if they were proper JSON, i.e. UTF-8, whereas they have to be re-encoded twice.
* About that million song dataset... the sane thing to do would be to have the students concatenate them into a few hundred files instead of 14,000... not only do they not do this, they make it impossible to do this by putting the json files in S3 and making reading from S3 part of the rubric to pass a project.
* Oh, and here's the icing on the cake: my last project (the Spark one) was sent back to be redone because I apparently made two mistakes: (1) I (OMG) implemented a logging function when one wasn't specifically called for in the requirements, this after being specifically told that students were encouraged to go beyond the specifications (sometimes it's important to stick to the specs, I get that, but this script took 185 minutes to finish, so I logged the times to find the pain point. It was writing and unreasonably large amount of partitions to S3, btw). (2) I didn't use the drop\_duplicates() function to ensure unique values for one table. You know what I did instead? I used the dropDuplicates() function, you know, the SPARK function for a SPARK project instead of the Python pandas function?
* Part of the selling point of these nanodegrees is you can put your projects on GitHub to be part of your portfolio. I would be embarrassed to put any of these in my portfolio.

&amp;#x200B;

End rant.",1560631163,31,0,37,dataengineering
Do Entry Level Positions Even Exist?,"Im a Data Analyst looking to become a data engineer. Ive been searching for jobs for about a month now and I have yet to see and data engineer listing that didnt want a shit load of experience.

So how would someone like me get into this field? Do you need to be a software engineer first? Here is my resume for reference. [resume](https://i.imgur.com/koao8sl.jpg)",1560626138,11,0,8,dataengineering
Sql for interview prep,"Hi all,

I am curious what other websites people use for SQL for interview prep apart from leetcode and hackerrank? I know about modeanalytics and sqlzoo.",1560570944,2,0,15,dataengineering
Would you refactor an ETL which is in Python 2.7?,"Hello there. I have came to a company which has an ETL based on python 2.7. They don't use neither Luigi nor Airflow or any other open source package out there for data pipelines, but a custom python etl framework with rundeck. 

It works fine, the failure rate is low, it requires no supervision during the weekends, it has CI/CD with GitLab and it can recover itself from 3 days of failures. However, it is very difficult to understand how it is coded, so it is difficult to debug certain things and adding new features overall. Given the circumstance that python 2.7 support will end soon, would it be worth to rethink the whole process? As far as I know, the ending of official support does not have to change anything, because anyway we are not updating the packages that are used in the etl anyway. 

However, another thought that I have is that I could take advantage of the ending of official support to refactor the entire ETL: make it simpler, more efficient, updating Python and its consequent packages to the last version to take advantage of new features of pandas and numpy, starting using airflow or luigi...

We are just two person in the BI departament, and I am not a data engineer per se (neither were the ones who built the etl at first) so I have another tasks to do (machine learning mostly). 

In one hand, my business intuition says me that it is completely worthless, because it works, and refactoring would mean probably several months of almost exclusive dedication for none (or almost none) business impact. In the other hand, I feel like learning Airflow and other stuffs and dig deeper into a data engineering task (I enjoy a lot programming in Python).

I guess that for the company's benefit I should not touch anything, but I have a little doubt about whether changing from python 2.7 to python 3.7 would be ""mandatory"" sooner or later and maybe it is worth to face the problem before it arises.

Thank you for your comments",1560503617,17,0,8,dataengineering
"Big Data Engineer - San Francisco, CA",,1560370358,0,0,1,dataengineering
CI/CD for deep learning models?,What is the best approach to integrate a CI/CD pipeline in deploying my machine learning models. I want to automate this process where the model is only integrated if a specific criterion is met like for example the accuracy of the model is above a certain threshold. I want to integrate this with the gitlab CI/CD pipeline. What is the best approach to tackle this problem? ?,1560338207,7,0,4,dataengineering
Telematics,"Hi,

&amp;#x200B;

This is my first blog in the series of telematics. I will be explaining the architecture in much more details in the coming part. Please do let me know your views on this. Would love to connect with people who are working in this space and get there feedback

[https://medium.com/@ibnipun10/telematics-fatter-and-bigger-226a820736c9](https://medium.com/@ibnipun10/telematics-fatter-and-bigger-226a820736c9)",1560323579,5,0,12,dataengineering
Do you need to learn Java also along with python for data engineer jobs?,A lot of the jobs that I see listed do not list Java as one of the required skills. I see python in almost every listing. But big data technologies also have Java  drivers. Is Java required as a part of your skill set or can I ignore Java and focus completely on python for my portfolio prep?,1560282951,12,0,12,dataengineering
How to build a Data Lake using Google Cloud Platform?,"Hi all 

(**tl;dr at the bottom)**

&amp;#x200B;

I'm a budding data scientist who just accepted a short-term (three months, possibility of extension) position at a company that isn't that technologically advanced. The position starts in a few weeks. 

&amp;#x200B;

In addition to the typical data science tasks, the company has asked me if I could build a data lake that takes in data from various sources they use (e.g. ZoomInfo, Dun &amp; Bradstreet, etc.). They've emphasized that this would be huge for them. I don't have any experience doing this, but I think this would be a great opportunity for me to challenge myself, learn new skills, and make myself more employable in the future. 

&amp;#x200B;

I won't have specifics about the company and the data until I start in a few weeks, but for now I'm trying to do some research on how to build a data lake. I know that google has published [overviews and a few tutorials](https://cloud.google.com/solutions/build-a-data-lake-on-gcp) on using GCP as a data lake, but I haven't really seen anything anywhere about how to \*actually do it\*.  Additionally, I don't know what kind of timescale for doing this I should be thinking about, but my gut tells me 3 months seems short, so if I'm making good progress on this it would incentivize the company to keep me around for longer. 

&amp;#x200B;

Any advice/suggestions?

&amp;#x200B;

Thanks!

&amp;#x200B;

**tl;dr — I'm a data scientist who needs to build a data lake on google cloud platform. I'm looking for resources/detailed tutorials, and advice on how to do so**",1560280130,7,0,4,dataengineering
An interview about how the open source Hydrosphere platform simplifies management of the full machine learning lifecycle,,1560270862,0,0,1,dataengineering
Event-Driven Architecture with Vendor Apps,"All, I am trying to push my organization to a more Event Driven Architecture approach utilizing Kafka as a message broker. Unfortunately, a lot of the systems we need to collect data from are 'closed-box' vendor applications that utilize a relational DB as their data store. I could use something like Attunity to publish messages from those RDBMS but I'm curious if anyone has run across a similar use case and how you approached it?",1560257819,10,0,5,dataengineering
How can data engineers and scientists be better compensated for their work?,"I’ve been thinking about all the tasks that data scientists and engineers have to complete in companies that they often don’t get the right recognition or compensation for. In order to try and match their work with compensation, I’ve created this potential bonus system similar to how people in sales are rewarded. 

&amp;#x200B;

https://i.redd.it/2awfzazv0q331.png

* Do you have any thoughts or improvements for this system? 
* Is there any non-monetary compensation that could also motivate you in your work?
* Do you think that this is feasible for your company? 

I really think this is crucial in the business, and I hope we can have a good discussion and answers!",1560256109,5,0,7,dataengineering
How do I can synchronize MySQL database to BigQuery,[removed],1560239572,0,0,1,dataengineering
Best way to orchestrate and transform small to mid-size data?,"I'm trying to figure out what's the best way to schedule and process many disparate datasets that varies from 3 to 15GB each (small to mid-size data).

&amp;#x200B;

We need to process batches of small data on regular interval, so the first thing that comes to mind is Airflow. I understand that Airflow is best used to schedule jobs, so I was thinking of using its DockerOperator to run [Jupyter notebooks inside a docker container](https://hub.docker.com/r/jupyter/datascience-notebook/) that will make the data transformations we need.

&amp;#x200B;

I was thinking something like [this](https://medium.com/@fartashh/scalable-data-engineering-platform-on-cloud-a557026aa02e) for architecture, where Airflow will be deployed on ECS and we will be using AWS ECR to store the docker images containing the notebooks. Or would there be a better alternative? I thought about Azure Databricks but it seems to be an overkill.",1560227019,6,0,4,dataengineering
Would you MS in Data Science?,"If you were currently not a developer and wanted to become a data engineer, or some kind of back end engineer, and you had the opportunity to do an MS in Data Science where you could double major in Computer Science, would you do it? 

Would you go back to undergrad and finish applied CS in two years?",1560198882,13,0,5,dataengineering
"Wrote an Article on Apache Kafka VS AWS Kinesis, Looking for Feedback/Suggestions. Thanks!",,1560186637,4,0,0,dataengineering
Wrote an article on Kafka vs Kinesis. Looking for feedback and suggestions.,,1560185990,0,0,1,dataengineering
Apache Kafka VS Apache Kinesis – Amit Kumar Yadav – Medium,"Guys wrote an article, any suggestions would be appreciated.",1560185384,0,0,1,dataengineering
Analyze &amp; visualize your big data easily using Apache Superset.,"Apache Superset is a modern, enterprise-ready business intelligence web application ([https://superset.incubator.apache.org/](https://superset.incubator.apache.org/)). 

For easy setup &amp; learning of Superset, I created a docker image of Superset. You can try using by downloading docker image from docker-hub or following repo [https://github.com/abhioncbr/docker-superset](https://github.com/abhioncbr/docker-superset)",1560177899,0,0,5,dataengineering
2-3 year outlook for data engineering in Canada/USA?,"Hi,

While I have been keenly thinking about moving into Data Engineering as career (long story short - 8 years in DevOps and now doing data engineering style development work for past 1 year).

&amp;#x200B;

While I like the whole vibe around this career path - resources, community, tools, even nice articles citing it as a growing field, Today, I can't help but think from a bit more negative perspective.

&amp;#x200B;

For example,

about 4-5 years ago, System administrator (sys admin) used to be viable career choice but with proliferation of cloud and now containers/serverless, in North America, need for sys admins has decreased a lot. Sys admins are bailing out by boatloads moving into career such as cloud engineering or devops engineering.

&amp;#x200B;

Same thing strikes me about data engineering - I mean with cloud giants such as GCP and AWS offering pre-packaged, play-and-play type ETL services, data streaming services and big data processing services, where does it leave the data engineers? I mean, even for devops engineering, up until 2 years ago, the CI/CD pipeline was the land of milk and honey and by virtue of knowing how to use the cloud's SDK to provision services meant you were considered elite. But now, same stuff is done by your average run-of-mill devs -- heck even k8s is being setup and run by devs these days. And all this is being done without impacting devs productivity.

&amp;#x200B;

So what's not to say that your average data scientists or data analysts won't pick up the required data engineering skills and with availability of more abstract/pre-packaged tools, start doing the work of data engineers?",1560175236,9,0,9,dataengineering
Would anyone be willing to share your Udacity Data Engineering course for a late payment or any other incentive?,"Long story short, I'm doing a Data Science internship at a start up and the work is basically modelling some cool stuff. But the head analyst mentioned the need for a Data Warehouse and an ETL pipeline for their dashboard. So I was thinking I could go the extra mile and build it for them and that might make them select me as a DS/DE  when I graduate next year. 

I saw how everyone was recommending this Udacity course and I wanted to learn it quick but the bummer part is that the course costs literally more than I'm getting paid right now so I can't buy it as everything else is already expensive here. I would be willing to pay or chip in the future after I get the job but it would mean a lot if someone would be willing to. I'll give verification for all I said if anyone need it. 

Thanks, have a nice day.",1560172582,2,0,0,dataengineering
What programming concepts do you need to know for a data engineering job?,"For a long time, this question has been bothering me. I'm not a complete beginner to programming. I have experience (Personal projects) with object oriented programming in Python and Java and I script a lot in python. I have started to learn big data technologies and one thing that always bugs me is if my approach in code is right. There are so many ways one thing can be accomplished in programming right. You can either explicitly code everything for better readability, or use shortcuts, functions, general scripts etc., Even though I have experience in programming, I always feel like im missing something in the fundamental section of programming. What programming concepts do you think one should be comfortable with in order to work as a data engineer.",1560136451,14,0,7,dataengineering
"Are there any data formats for storing text worth looking into, besides CSV ?",[removed],1560106401,0,0,1,dataengineering
Ways to try out actual Data Virtualization?,[removed],1560092915,0,0,1,dataengineering
Udacity or Udemy for data engineering,"Hi all, 

I am a BI associate who wants to transition to Data Engineering role. I am doing ETL and dimensional modeling in my current role. I want learn big data technologies and improve my github profile by adding new projects. I want to know the opinion which one is the best option Udacity DE nanodegree or udemy big data courses?",1560033845,9,0,7,dataengineering
Any resources for learning data security?,"Hi,  


Are there any good online resources (or books) to learn about data security? I am looking for more hands-on, practical methods for implementing data security as part of ETL process and for data at rest.",1559916462,1,0,17,dataengineering
Data Structure and Algorithms knowledge to land an entry level position," In practice, how much DSA should one know for landing a Junior Data Engineering role? Is it fair to assume that it won't be as much as traditional Full Stack/Systems development role requirements? I'm of the opinion that knowledge of frameworks and projects hold a lot more value, but I would like to be disillusioned of this notion if its wrong.",1559892957,8,0,13,dataengineering
Free Webinar On Logistic Regression,[removed],1559726197,0,0,1,dataengineering
"Kedro, an open source tool designed to help you build production-ready data analytics code.",,1559665646,3,0,11,dataengineering
Adoption of Azure Data Lake Analytics,What has been your experience working with Azure Data Lake Analytics(ADLA) and U-SQL? How does it weigh against Azure HDInsight and Azure DataBricks? Is there substantial merit in developing skills on this tech? How much is the market favouring this?,1559653269,2,0,1,dataengineering
An interview about how and why Greenhouse migrated their homegrown ETL pipeline onto DataCoral,,1559647198,0,0,1,dataengineering
"An easy, quick way to start learning &amp; try Apache Airflow.","Airflow is a platform to programmatically author, schedule, and monitor workflows.  Recently, for learning and quickly setting up Airflow, I created a Docker-based Airflow image. Containers support multiple features like writing logs to be local or S3 folder and Initializing GCP during container booting [https://github.com/abhioncbr/docker-airflow](https://github.com/abhioncbr/docker-airflow) Any, one to try or enhance further is most welcome.",1559608654,2,0,17,dataengineering
Looking for a design pattern,"Hi all! This is my first post in reddit, I would try to explain it as best as possible.

Well, Im developing a python library in order to read data from different formats and sources (api rest, odbc, access, xlsx...), transform data, analyse it, send emails...

I thought to divide the library on modules: data, analysis, emails... But I'm not sure...

I have to use OOP? For connecting to odbc its fine, but for doing the analysis? Also I want to separate sql queries in a different file...

Anyway, do you know a design pattern for this case?

Thank you a lot!!",1559596513,0,0,1,dataengineering
"“Oh, you’re a data… something?”: The Misunderstood Role Of A Data Engineer",,1559573574,3,0,20,dataengineering
Azure Databricks experience,How has your Azure Databricks experience been so far w.r.t. data engineering? Anyone has any idea on the specs which are needed to process about 500 GB data daily in an ETL pipeline?,1559571084,14,0,3,dataengineering
Can a data scientist get into data engineering?,"As with all data science jobs, job duties can vary.  I do a little of everything, ML, modeling, cloud stuff, working with software engineers and pushing code to production.



I enjoy data science, although my favorite parts of my job are more on the data and software engineering side.  Am I qualified to later apply for data engineering jobs?",1559502304,9,0,4,dataengineering
5 Considerations to have when using Airflow,,1559501920,1,0,15,dataengineering
Best Way to Solve Capacity Constraints?,"Hey there,

&amp;#x200B;

I have a model built in PySpark that has scoring thresholds for determining if someone will be moved forward or not. Let's say on a score of 0-1, it's 0.1. So, if someone scores 0.12, they get flagged as 'Y'.

&amp;#x200B;

I have a lot of data preparation before the model build to build the data set and this model just flags user, score, and Y/N based on the threshold.

&amp;#x200B;

How can I best implement constraints into this model in terms of saying, I need no more than 75 Y's from this entire data set, no matter what? I had left the decision part out into a later part of the pipeline, using Presto/Hive, but I am wondering how I can even implement some sort of optimization with that, or will I need to leverage Python again to do it?

&amp;#x200B;

I thought typing this out myself would help me think a bit clearer, but I'm still confused on the subject. I was thinking I could have a few sub-tables that have the current weekly capacity as well as where it is currently allotted, and based on the current model run, could adjust the threshold up or down to set a target of meeting the requirement?

&amp;#x200B;

Any opening thoughts?

&amp;#x200B;

Thank you ahead of time!",1559359045,10,0,5,dataengineering
I start my new role as Data Engineer on Monday. Excited and a bit terrified.,"So I've been in IT/development for about 9 years now. Started as general IT guy at a Public School, then got into a company as tech support, moved to Database Admin/Development within company, CUrrently Database Developer, but starting a new job as a Data Engineer for a financial institution. It's a unique role, as I won't be under IT/Development, but working within the business side. Having said that, I have a few questions.

&amp;#x200B;

1. Is it common for DE's to work on the business side of house, and not under Dev/IT? It was a hard sell for me to get out of the Dev/IT space and into the business side, but the work seems interesting, and I've always wanted to do data engineering, as I find data fascinating. 
2. What are some things I should know/learn ASAP to make my life/job easier? I know the concepts of Kimball's data warehouse, and have even helped implement star schemas and model the data. However my current bread and butter is more OLTP heavy, not OLAP. 
3. What languages, apart from SQL, should I be most focused on? I've seen people say everything from Python to R to Hadoop to everything else. I have done development in C#, and my college classes were in C, so I have familiarity with C style languages.
4. For people that have taken my path, what is the greatest difference I'm going to experience going from development to a more business-centric role? (The DE position I'm starting doesn't fall under IT, it's on the business side. The company is just starting to realize the power and insight they can get from data, so it's a very new idea, and I've basically been told ""Whatever you want to do/software you want to use, have at it. Basically the blue ocean strategy."")",1559136115,10,0,21,dataengineering
Ingesting Data From Kinesis Stream into OmniSci GPU-accelerated database,,1559136026,0,0,3,dataengineering
An interview about how the open source Pachdyerm platform makes building flexible data pipelines with first class support for data lineage easy,,1559120183,0,0,5,dataengineering
FREE hands-on Workshop in SF - Create a data warehouse in Amazon Redshift with Etleap and AWS,"SAVE YOUR SEAT HERE: [http://info.etleap.com/devdaysjune24](http://info.etleap.com/devdaysjune24)  


**About The Event**  


AWS Partner DevDay is a partner led, AWS supported event for customers. It is a half day, hands on technical event delivered by APN Partners who have demonstrated technical proficiency and proven customer success in specialized solution areas. Customers will get to try some of the hottest topics in cloud computing, and get deep dives into AWS powered partner solutions. Etleap technical experts will explain key features and use cases, share best practices, walk through technical demos, and be available to answer your questions one on one.  


***Please bring your laptop to participate in this workshop and make sure you have an active*** [***AWS account***](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&amp;all-free-tier.sort-order=asc&amp;awsf.Free%20Tier%20Types=categories%23featured)***.***   
**Who Should Attend**  


AWS Partner DevDay is ideal for data engineers, data scientists, and product developers from all experience levels; both existing builders on AWS who want to dive deeper into highly technical hands on training and content as well as those who are new to the cloud. This event is a great opportunity to connect with Etleap to learn how to implement a successful AWS powered solution.  
**What You’ll Learn:**

* Learn the basics of Amazon Redshift
* Understand the components of ETL architecture on AWS
* Appreciate the pros and cons of fully-managed ETL infrastructure",1559078150,0,0,1,dataengineering
Incorporating data engineering aspects into my analyst role,"Hi all, I just recently got my first analyst position at an educational district that focuses mostly on SQL querying and reporting. But I am also responsible for creating/maintaining a good amount of automated processes that update tables, load tables into csv's, fire off stored procedures, etc.. From what I can tell, the method that they have used seems a bit ragtag. On the server the task scheduler will execute a batch file, which executes a ssis package, which fires the stored procedures/loads the data and whatnot. On top of this there are also automated tasks with SQL Server Agent. I had to change the time of one of the tasks and it was very difficult to figure out if that would have adverse effects on the other tasks.

Am I correct that this method isn't desirable? If so, what would be some better ways this could be done? I have looked into WMS's such as Airflow and Luigi, but I don't know if this is the ideal environment for them, or if they would just create more work and be overkill. I do potentially have long term aspirations of being a data engineer so I would love to get more direct experience if the situation is fitting. Thanks for the help.",1559077363,1,0,5,dataengineering
DataOps Principles: How Startups Do Data The Right Way,,1559073798,2,0,3,dataengineering
Schema change in Data Warehouse,"Hello! I keep getting these questions in interviews on handling schema change in DW. For example, adding a new column to an existing loaded table efficiently. Any thoughts/articles on going about this?",1559069624,4,0,15,dataengineering
"80% of data science is data engineering so why do people become data engineers if not to progress to data scientist? It feels like a data scientist spends most of their time doing data engineering, then 10 minutes importing sklearn, and get paid far more for the privilege.",,1559054852,22,0,32,dataengineering
A list of useful resources to learn Data Engineering from scratch,,1558937703,7,0,37,dataengineering
Guys take a look at my repo where I try to aggregate useful resources to sharp you data engineering skills,,1558934867,0,0,1,dataengineering
"Data Engineering - Concurrency, Distribution, Redundancy",,1558931707,1,0,7,dataengineering
Good online certifications for Data Engineering?,"Hi Folks,

&amp;#x200B;

I have background in Software Development and DevOps. I am planning to make my next career move into Data Engineering. Can you guys suggest any good online certifications for data engineering?",1558918015,8,0,4,dataengineering
Advice for moving into Data Engineering,"Hi Folks,

&amp;#x200B;

Sorry for long post BUT I really need your help folks. Please, help me. 

&amp;#x200B;

Here is my situation:

Location: Toronto, Canada

Education: MSc. Computer Science | York University (2015)

Experience:

2009-2012 - web programming / mainframe (including some SAS)

2012-2016 - Build and release automation (some DevOps)

2016-2018 - DevOps and AWS Cloud automation

2018-PRESENT - Software Development with focus on SQL databases (MS SQL and SAP HANA)

Current job roles and skills:

\- Creating microservices (backend) in Kotlin

\- Perform ETL to load data from various MS SQL databases to SAP HANA which acts as company's data warehouse (Business Analysts are creating views on top of HANA raw data)

\- Performing ETL using SAP Data Services mainly (majority is RealEstate data).

\- Performing data recon using SAP SRS-DA (SAP Data Assurance).

\- Loading legacy data from decommissioned systems and manual data from MS Excel spreadsheets into SQL database using Python Pandas

\- Creating and maintaining overnight job process for data loading on top of SAP Data Services and Automic ONE Automation Engine.

\- API based data integration between market data provider (preqin) and our vendor Application' database (funds and private equity data).

&amp;#x200B;

But I realize one thing -- 

&amp;#x200B;

I really want to get into data engineering. Few things I can think I should start doing/learning:

&amp;#x200B;

\- Data Analysis (to perform better transformation instead of just extract and load) -- perhaps learn and use ER diagrams to remove 'duplicates' and 'normalize' data.

\- Once data has been normalized, help Business Analyst simplify their views by cutting down on potential number of sources/joins.

\- perform database administration (on AWS MS SQL RDS) for our web application.

\- Utilize NoSQL database (such as MongoDB) for loading blob-based data for web application (where it makes sense).

&amp;#x200B;

What else should I look into to get into data engineering properly?

&amp;#x200B;

Thank you.",1558917319,7,0,13,dataengineering
"Best data formats to store text data on disk, for fast retrieval using index or key ?","I have a ton of text data that I would like to store to disk since it won't all fit into ram, but I would like to retrieve certain rows quickly using an index or key.

So far it's just rows of text data. I haven't implemented any permanent index or key yet, since the data format chosen may affect that. A situation might be that I'll need to access the text from rows 34, 8493, 39993, 333, and 4903 in an instant. I can not only load a subset because the user may query any of the rows, not just a subset

I was looking at numpy's memmap at first, but it looks like that's won't be the most optimal solution for text data.

I am also looking at hdfy / h5py / pytables but it's unclear these data formats are optimal for text data.

Finally, I also come across SQLite/sqlite3 . I would prefer to avoid this format since I don't want to learn too much database stuff at the moment, but if it's the best option I will use it.",1558893938,5,0,2,dataengineering
The Importance of Building a Data-Centric Culture,,1558745247,0,0,3,dataengineering
Learn Data Science from Scratch-Best online resources in 2019,,1558721366,0,0,0,dataengineering
Open-sourcing Whirl: Local Airflow development made easy,,1558700335,3,0,3,dataengineering
"VP of Engineering (Data) interview tomorrow. Any advice ? Think it’s a tad bit of title inflation, I am slightly under qualified ( been a Sr data analysts for many years , have lead projects , work in a specific industry that this company is a start up in )","What are some high level topics to undertake and to over ? Offshore / onshore engineers , reporting , collecting requirements from business ... I am also worried about hyper technical questions",1558674052,7,0,8,dataengineering
Top Python ETL Tools - Airflow vs. Alternatives,"Python developer community has built a wide array of open source tools for ETL. Some of these tools allow you to manage every step of an ETL process, while others are just really good at a specific step: [Top Python ETL Tools (aka Airflow Vs The World)](https://blog.panoply.io/top-9-python-etl-tools-and-when-to-use-them) (overview)

* [Airflow](https://github.com/apache/airflow)
* [PySpark](https://spark.apache.org/docs/2.2.1/api/python/pyspark.html)
* [petl](https://github.com/petl-developers/petl)
* [Panoply](https://panoply.io/)
* [pandas](http://pandas.pydata.org/)
* [Bubbles](https://github.com/stiivi/bubbles)
* [Bonobo](https://www.bonobo-project.org/)
* [Luigi](https://github.com/spotify/luigi)
* [Odo](https://github.com/blaze/odo)
* [etlalchemy](https://github.com/seanharr11/etlalchemy)
* [mETL](https://github.com/ceumicrodata/mETL)
* [Open Semantic ETL](https://opensemanticsearch.org/etl)
* [Mara](https://github.com/mara/data-integration)
* [riko](https://github.com/nerevu/riko)
* [Carry](https://github.com/toaco/carry)
* [locopy](https://github.com/capitalone/Data-Load-and-Copy-using-Python)
* [etlpy](https://github.com/ferventdesert/etlpy)
* [pygrametl](https://github.com/chrthomsen/pygrametl)",1558665762,6,0,0,dataengineering
10 Benefits to using Airflow,,1558639491,0,0,11,dataengineering
Udacity Data Engineering Nanodegree Course Review," \# Overview  
\[Udacity's new Data Engineering Nanodegree\]([https://www.udacity.com/course/data-engineer-nanodegree--nd027](https://www.udacity.com/course/data-engineer-nanodegree--nd027)) is one of the few data engineering courses out there right now. It is geared towards people that already have programming experience, specifically with Python and SQL. Udacity estimates that it would take someone 5 months to complete if they committed 5 hours a week (\~108.6 hours of content) at the one-time price of $999 USD (this has changed since I started and is now $399 USD / month). The course is broken up into five sections, Data Modeling, Cloud Data Warehouses, Data Lake with Spark, Data Pipelines with Airflow, and a capstone project. Each section has different instructors, with each one bringing a different teaching style in a way that keeps things refreshing while still keeping you wondering if it happened simply due to lack of communication. The structure for each section consists of introducing concepts through lectures, reinforcing the material with demos and exercises (typically in a Jupyter Notebook), and concludes with 1-2 project(s) dealing with designing an ETL process using song data for an imaginary company called Sparkify.  


\# My background  
I have about two years of professional experience wrangling data with Python and SQL and about a year and a half of web development experience. I have a bachelors degree in engineering and took a few introductory computer science courses. A few months ago I completed \[Dataquest's Data Engineering Path\]([https://www.dataquest.io/path/data-engineer/](https://www.dataquest.io/path/data-engineer/)) and have taken a few \[DataCamp\]([https://www.datacamp.com/](https://www.datacamp.com/)) courses as well as \[CS50\]([https://www.edx.org/course/cs50s-introduction-to-computer-science](https://www.edx.org/course/cs50s-introduction-to-computer-science)) and \[CS50 Web\]([https://cs50.harvard.edu/web/2019/spring/](https://cs50.harvard.edu/web/2019/spring/)). I enrolled in this course due to its focus on cloud technologies, which I have been learning through trial by fire at a data engineering job I started a few months ago, mostly using AWS, Postgres, Python, and Airflow.  


\# Individual Sections Review  
\## Data Modeling  


This section introduces what data modeling is, why it's important, and what the differences between a relational and NoSQL database are. It speaks on important concepts such as ACID transactions, what fact and dimensional tables are, and what the difference between star and snowflake schemas is. This section uses Postgres and Apache Cassandra and consists of a project for each of them where you design schemas and load song logs and song metadata into fact and dimension tables.  


Pros:  


\- Introduces most of the Postgres and Apache Cassandra commands a data engineer would probably ever use  
\- Provides a good explanation on when you'd want to use SQL vs. NoSQL  


Cons:  


\- Most lectures consisted of watching the lecturer read slides off her laptop  
\- This section's exercises seemed to have more bugs than the rest  
\- There were a few questionable practices in this section such as a try / except block around everything and always inserting rows individually instead of in bulk  


\## Cloud Data Warehouses  


This section builds on the previous section and explains the need for a data warehouse and what the benefits of hosting it in the cloud are. AWS basics such as IAM, creating an EC2 instance, and security groups are introduced, as well as a brief introduction to infrastructure as code using boto3. Other concepts such as OLAP cubes, rollup, drill-down, grouping sets, and columnar storage are discussed. The project consists of designing tables in Redshift and loading data from S3 to Redshift.  


Pros:  


\- Provides practical example exercises such as loading S3 files in bulk to tables in Redshift using the COPY command  
\- Makes creating a sandbox data warehouse environment much more approachable. Prior to this I always thought it would be too expensive and complicated to build one on my own and this section proved me wrong  


Cons:  


\- Tries to cover too much ground. Topics like infrastructure as code are glimpsed over and overly simplified  


\## Data Lakes with Spark  


Introduces what big data is and why big data tools like Hadoop and Spark are necessary. Provides a conceptual overview of how distributed systems like Hadoop and Spark work. Hands-on exercises consist of using PySpark to wrangle data. Explanations of why an organization may need a data lake instead of a data warehouse are provided. The project consists of ingesting raw S3 files, creating fact and dimension tables, partitioning them and writing them back to S3 all with PySpark.  


Pros:  


\- Provides an excellent explanation of how distributed file systems and cluster computing works  
\- Gives a good explanation on when to use PySpark data frames vs PySpark SQL and how to use them interchangeably  


Cons:  


\- This project involved filling in a lot more blanks than the rest of the projects and I found it to be particularly time-consuming. The number of files to ingest from S3 seemed too large to run in a reasonable amount of time  
\- I wish it would have included more information and exercises about using PySpark on a cluster of machines instead of on a single local one  


\## Data Pipelines with Airflow  


Data pipelines, DAGs, and Airflow concepts such as operators, sensors, and plugins introduced. The final project involves using Airflow to load S3 files into partitioned Redshift tables and perform data quality checks afterward.   


Pros:  
\- The only tutorial I've found on how to use data quality checks with Airflow. I've started using this technique at work and it is a game changer  
\- Airflow is a bitch to deploy and someone they engineered a way for people to run it on Udacity's workspaces. Kudos to the engineers on that  


Cons:  
\- This section felt a bit shorter and was more focused around a specific technology than the other sections. Not necessarily a con but I would have liked to have the lectures be more generalized around the concepts of a data pipeline  


\## Overall  
Overall, I really enjoyed this nanodegree and learned a lot of practical things from it that I have already started using at my job. I would estimate I spent about 40 hours completing it so I definitely felt short-changed in content and think it is incredibly overpriced for what it is. What I don't like is how Udacity markets their courses as a way for someone to make a career change with no real-world experience. I find that incredibly hard to believe and can't imagine a company hiring someone with no real world experience after completing this nanodegree. I found the content to mostly be of a very high quality and I think this is really the only intermediate-advanced data engineering course out there. If you have the cash and are interested in learning data engineering in the cloud I would highly recommend it.",1558614184,33,0,55,dataengineering
Data Engineering skills beyond coding,"Hi there,

I see a lot of posts about which coding/tech skills (Python/Scala, AWS, Azure,...) are essential to become/be a Data Engineer but I am missing some things. How about the ways you set up your storage, when is file storage ok, when database, what about data modeling (normal, dimensional, Data Vault)? Or topics like data quality? 

These are the same things I am missing in a lot of the courses you can find. Many teach how to process large or fast data sets but omit these. Maybe it's part of Data Engineering being so ""new"" and undefined (last time I checked there wasn't even a Wikipedia article about it) 

What's your opinion? Are those  skills essential too or just side topics?",1558598739,13,0,10,dataengineering
How Does PayPal Processes Billions of Messages Per Day with Reactive Streams?,,1558578371,0,0,3,dataengineering
Has anyone taken Jesse Anderson's course?,"Is it worth the price (up to 5k depending on the one you choose)? It seems to be one of the only comprehensive data engineering courses out there - I've read reviews of the Udacity and Dataquest paths, and they've generally been negative (not in depth enough). There was [this](https://www.reddit.com/r/dataengineering/comments/8g1d6e/has_anyone_taken_jesse_andersons_online_database/) post about a year ago, but there was really only a conversation between Jesse (/u/eljefe6a) and some other redditors - nobody that had actually taken the course.",1558559352,7,0,5,dataengineering
Data Analyst who wants to eventually become a Data Engineer,"So I'm a data analyst. My SQL and Python skills feel pretty solid, but I am by no means an engineer level, and my current job doesn't do a great job of facilitating my growth in these two areas.

&amp;#x200B;

I really want to be a data engineer though, and was wondering. If you were in my position with these skills, what would you focus on learning next.

&amp;#x200B;

Also looking for project ideas, hopefully not too complex ones that can teach me these skills as well as boost my github. Was thinking of a python mysql project to stream top reddit posts to a database, and then doing sentiment analysis for the subreddit that day? My initial thought if you guys had any criticisms. Thanks in advance everyone",1558497289,17,0,9,dataengineering
Distributed Data Querying with Alluxio,,1558457436,0,0,3,dataengineering
ON the evolution of Data Engineering,,1558408288,0,0,10,dataengineering
An interview about how dbt enables your data teams to build better analytics in your data warehouse,,1558365523,2,0,5,dataengineering
ESXi Nat Connection,"Anyone familiar with VMWare ESXi?  
I have a problem on network configuration.

How could I setup a NAT connection?  
Currently, the default is bridged.

TIA",1558159542,0,0,1,dataengineering
Interview at FB,"I have a DE onsite interview at fb in next couple of weeks. 
The interview consistent of 4 rounds. 2 ETL, 2 Business acumen. SQL and python are tested. 

Any advice/tips are highly appreciated.

Thanks",1558119427,20,0,8,dataengineering
Developed wrong skillset? How to forge a path back to data ?,"I am currently working as a frontend developer using on of the popular framework such as React and Angular, However I totally hate this type of work( web designing and shifting pixels ) . I  needed a VISA to work  about two year ago  so I took on a job as a SW. On resuming at the job,  there was more need for  frontend developer, So I jumped in to make my self valuable and ramped up pretty fast developing application they loved and they commended me for it.  I have now developed a skillset I loathe but I got a green card  out of it so no complains on the VISA part.

&amp;#x200B;

Prior to this two years I was an ETL developer for 3 years (Traditional MSBI) and I absolutely loved it to bits. I switched jobs only because my visa ran out and my sponsor was not able to proceed.  I would like to go back into that space and become a data engineer and I have been listening to podcast and studying but I feel lost. How do I explain 2 years of  javascript to future employers? I know it all programming but I have never had experience with technologies such as Apache Spark or Hive though I have take series of courses on them. 

&amp;#x200B;

I would love advice on transitioning? Has anyone every moved  from frontend to data engineering. How do I tackle recruiters who only pick keywords they know from the list ? 

I am currently earning a good salary at the moment doing Front end development but I find it hard every day to go to work. I cringe at the thought that I will have to craft CSS again. I know the framework will change in a years times to the new flavour of the month and I will be out of work because I don't  see myself reading about how to redisplay the same thing in another framework. It feels like I am losing my soul one day at a time. The jobs makes me feel worthless.",1558114285,10,0,8,dataengineering
Good resources to learn Kafka?,"I'm trying to get more data engineering responsabilities at work (I'm a SWE) and the topic of integrating with Kafka came up.

Is there a particular resource you'd recommend to learn it?",1558063739,7,0,5,dataengineering
Is it still worth learning Scala?,"Hey Folks,

I have been working on a on-prem full Microsoft stack Data Warehouse for the last 3 or so years.  Finally, I will be getting to work on a project to build a new solution in the cloud, where I will be playing with all the cool stuff.  Obviously, I need to do a bit of re-skilling.

As what ever we work on will more than likely use a Spark platform, this is where I am starting.  I am currently running through an old Pluralsight (from 2015) course that uses Scala, just to get a bit of a taste.

So, after all of that, is it actually worth learning Scala today?

Cheers",1558060073,17,0,13,dataengineering
Why Git and Git-LFS is not enough for machine learning reproducibility,"Git-LFS (Git Large File Storage) as the name implies, deals with large files while building on Git. The pitch is that Git-LFS “replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server like GitHub.com or GitHub Enterprise.”

But the key to repeatable ML results is also to keep proper versioning of not only their data but the code and configuration files, and to automate processing steps. Successful projects sometimes requires collaboration with colleagues, which is made easier through cloud storage systems. Some jobs require AI software running on cloud computing platforms, requiring data files to be stored on cloud storage platforms.

The article explains how a machine learning research team can ensure their data, configuration and code are in sync with each other using DVC tool: [Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis](https://towardsdatascience.com/why-git-and-git-lfs-is-not-enough-to-solve-the-machine-learning-reproducibility-crisis-f733b49e96e8)",1558013129,1,0,1,dataengineering
MySQL ETL Tools: Free And Paid Options,"The following overview focused on free and open source MySQL ETL tools, but some of these tools below are the “community” versions of more powerful, paid platforms that you can use if you want more control on your data:  
[**Top 9 MySQL ETL Tools: The Best Free And Paid Options**](https://blog.panoply.io/top-9-mysql-etl-tools-the-best-free-and-paid-options) (see the comparison for more details on each of the following options)

**Free and Opens Source ETL Tools:**

1. [Benetl](https://www.benetl.net/)
2. [Talend Open Source Data Integrator](http://sourceforge.net/projects/talend-studio/)
3. [Apatar](http://www.apatar.com/)
4. [KETL](http://www.ketl.org/)
5. [OpenMRS](https://github.com/openmrs/openmrs-module-mysqletl)
6. [DataExpress](https://github.com/chop-dbhi/dataexpress)
7. [Transformalize](https://github.com/dalenewman/Transformalize)
8. [Csv2db](https://github.com/csv2db/csv2db)
9. [Pentaho Kettle](https://community.hitachivantara.com/docs/DOC-1009855)

**Paid Cloud Services for Extracting MySQL Data to a Data Warehouse:**

A data warehouse is a system that pulls together data from many different sources within an organization for reporting and analysis. The reports created from complex queries within a data warehouse are used to make business decisions

1. [Panoply](https://panoply.io/)
2. [Blendo](https://www.blendo.co/)
3. [Stitch](https://www.stitchdata.com/)
4. [FlyData](https://www.flydata.com/)
5. [Informatica](https://www.informatica.com/)",1557991840,5,0,1,dataengineering
Career Change Out of Data Engineering,"I'm curious if anyone here is trying to transition from data engineering into a different role.  What new roles are easy to transition to?

Here's some that I've been personally thinking about.

1.  Automation/Release/SRE role - seems like an easy transition since a lot of the responsibilities of data engineers is platform work.
2.  Data Scientist, ML - not really personally interested in this, but of course this makes sense.
3.  Backend Developers - from working a lot with databases, backend work seems like a natural transition.

I don't see a lot of transferrable skills to fields like frontend/JS, mobile, or security.  Of course, good software engineering skills means that you certainly CAN move to that role, but it's not as easy as the 3 I've described above.",1557957865,15,0,5,dataengineering
Building your own Google analytics,GA analytics freemium is limited and 360 premium too expensive. Hypothetical Scenario: So I have an app(say 200K users) and BigQuery. Goal is to set up a datalake: I want to be able to store hit/event level data triggered by user clicks etc and also have session and user level views of the events/hits. Is there an example of this being done? do companies actually do this? Is this feasible? What do you think?,1557953480,6,0,8,dataengineering
can you post here,Im just checking i just typed a long post and it got deleted and didnt post,1557949072,0,0,0,dataengineering
Facilitating the discovery of public datasets,,1557944201,0,0,1,dataengineering
How can data engineers and scientists be better compensated for their work?,"I’ve been thinking about all the tasks that data scientists and engineers have to complete in companies that they often don’t get the right recognition or compensation for. In order to try and match their work with compensation, I’ve created this potential bonus system based on The Accelerate State of DevOps Report. 

&amp;#x200B;

https://i.redd.it/gjjy9q0uecy21.png

&amp;#x200B;

*  Do you have any thoughts or improvements for this system? 
* Is there any non-monetary compensation that could also motivate you in your work?
* Do you think that this is feasible for your company? 

&amp;#x200B;

I really think this is crucial in the business, and I hope we can have a good discussion and answers!

&amp;#x200B;

Here is the [original report](https://cloudplatformonline.com/rs/248-TPC-286/images/DORA-State%20of%20DevOps.pdf) in case anyone is interested!",1557912138,1,0,1,dataengineering
How Different Are Lean Development and Lean Startup Techniques?,[removed],1557829193,0,0,1,dataengineering
Interview: Made One Mistake in Project Code,"I'm currently interviewing for a data engineering job at a company that I know would be a great fit for me and quite honestly is my dream job. I had a very good interview with the hiring manager for the first round - she told me it's hard to find a data engineer with a strong domain knowledge of the healthcare industry. She also told me that I was approved to move on to the second interview before the first one was even over. The second round interview consisted of doing a coding project: half of it was SQL and the other half was a basic ETL process in Python. I had all parts of the code giving back exactly the results that they wanted, so I started writing the unittests for the Python portion. However, \*very\* stupid me modified one tiny part of the Python code because a unittest was complaining about it, which allowed the unittest to pass.  And now, instead of giving all rows back within a Pandas dataframe, it only gives one back. And I just \*now\* realized this, a few days after turning in the code to the hiring manager.

I cannot explain how crushed I feel right now, especially after putting all of that time and effort in to the code.

The hiring manager has been out all week so I haven't heard back from the recruiter (the recruiter is our point-of-contact; I do not have the means to contact the hiring manager myself). And honestly, if I reviewed that code myself as an outsider, I'd be really confused as to why all of the code is correct except for one tiny part that gives back one row instead of all rows.

Am I f\*cked? Should I reach out to the recruiter explaining the mistake?",1557683897,8,0,3,dataengineering
Collaboration data engineers with data scientists and analysts,"Hello dataengineering community,

I am a data engineer, I have wrote a post on how we collaborate with our peers in science and analytics in my company in order to improve our efficiency releasing in production. Apache airflow is a main tool in this collaboration. 

I would be happy to answer any question on the subject or hear how it works in your company. 

https://medium.com/dailymotion/collaboration-between-data-engineers-data-analysts-and-data-scientists-97c00ab1211f

I hope this post have its place here, I apologies in advanced if not and will remove it.",1557669692,2,0,16,dataengineering
Mid-level Data Engineer roles/expertise?,"Hello. I've been a Data Engineer for almost 2 years (this is also my first job). In my job, I'm mostly handling an ETL framework, mainly using Scrapy and some xls/pdf parser tools for getting the data. I think I've created around 200+ scrapers already.

And I've been trying to find a new job, found a possible opportunity to work in Singapore, as data engineer again. (im from SEA). But I'm not that confident that my current skills/knowledge would be qualified enough for it.

Could I ask some data engineers here as well. What is expected from a Mid-level/Senior Data Engineer ? Is Spark/Kafka an already established standard in data engineering today? My work uses quite an outdated ETL design so I'm mainly concerned about this. Thanks.",1557602581,9,0,13,dataengineering
AWS Big Data exam - seems outdated... anyone did the exam?,,1557572510,2,0,8,dataengineering
Junior Data Engineer advice?," 

Spent the last year as a Data Analyst, starting a new role as a junior data engineer. Role is focused on learning a proprietary ETL tool, so no extensive SQL experience was required beyond the basics. I want to be as prepared as possible in the position (No background in CS, mostly self taught). I've been focusing on SQL as much as possible in my free time as I know it's crucial. My previous role as an analyst required no SQL, it was all excel/powerBI and power query.

Currently learning basic analysis (joins, value counts, iterrows) in python pandas.

Any advice? For those that are seasoned, what advice would you give yourself when you were junior?",1557523578,14,0,11,dataengineering
Job Offer Curated List.,"I am teaching learning some new skills and I created this table with some new jobs for those who are interested in something different if the community is interested I can create something each week.

|**Companies Name**|**Title**|**Country**|**State**|**City**|**Employment Type**|
:-:|:--|:-:|:-:|:-:|:-:|
|[Hired](http://www.steele-consulting.net/opportunity-1/)|[Data infrastructure manager](http://www.steele-consulting.net/opportunity-1/)|[US](http://www.steele-consulting.net/opportunity-1/)|[CA](http://www.steele-consulting.net/opportunity-1/)|[San Francisco](http://www.steele-consulting.net/opportunity-1/)|[Full Time](http://www.steele-consulting.net/opportunity-1/)|
|[George Mason University](http://www.steele-consulting.net/opportunity-2/)|[Metadata librarian](http://www.steele-consulting.net/opportunity-2/)|[US](http://www.steele-consulting.net/opportunity-2/)|[VA](http://www.steele-consulting.net/opportunity-2/)|[Fairfax](http://www.steele-consulting.net/opportunity-2/)|[Full Time](http://www.steele-consulting.net/opportunity-2/)|
|[Northside Hospital](http://www.steele-consulting.net/opportunity-3/)|[Research data specialist](http://www.steele-consulting.net/opportunity-3/)|[US](http://www.steele-consulting.net/opportunity-3/)|[GA](http://www.steele-consulting.net/opportunity-3/)|[Macon](http://www.steele-consulting.net/opportunity-3/)|[Full Time](http://www.steele-consulting.net/opportunity-3/)|
|[Art Engineering Llc](http://www.steele-consulting.net/opportunity-4/)|[Ballistic missile defense tactical data link and aegis systems t with security clearance](http://www.steele-consulting.net/opportunity-4/)|[US](http://www.steele-consulting.net/opportunity-4/)|[CA](http://www.steele-consulting.net/opportunity-4/)|[San Diego](http://www.steele-consulting.net/opportunity-4/)|[Full Time](http://www.steele-consulting.net/opportunity-4/)|
|[Hired](http://www.steele-consulting.net/opportunity-5/)|[Data engineer](http://www.steele-consulting.net/opportunity-5/)|[US](http://www.steele-consulting.net/opportunity-5/)|[CA](http://www.steele-consulting.net/opportunity-5/)|[Hayward](http://www.steele-consulting.net/opportunity-5/)|[Full Time](http://www.steele-consulting.net/opportunity-5/)|
|[Northside Hospital](http://www.steele-consulting.net/opportunity-6/)|[BMT clinical research/data coordinator](http://www.steele-consulting.net/opportunity-6/)|[US](http://www.steele-consulting.net/opportunity-6/)|[GA](http://www.steele-consulting.net/opportunity-6/)|[Atlanta](http://www.steele-consulting.net/opportunity-6/)|[Full Time](http://www.steele-consulting.net/opportunity-6/)|
|[Hired](http://www.steele-consulting.net/opportunity-7/)|[Data engineer](http://www.steele-consulting.net/opportunity-7/)|[US](http://www.steele-consulting.net/opportunity-7/)|[CA](http://www.steele-consulting.net/opportunity-7/)|[Redwood City](http://www.steele-consulting.net/opportunity-7/)|[Full Time](http://www.steele-consulting.net/opportunity-7/)|
|[Hired](http://www.steele-consulting.net/opportunity-8/)|[Data infrastructure manager](http://www.steele-consulting.net/opportunity-8/)|[US](http://www.steele-consulting.net/opportunity-8/)|[CA](http://www.steele-consulting.net/opportunity-8/)|[Greenbrae](http://www.steele-consulting.net/opportunity-8/)|[Full Time](http://www.steele-consulting.net/opportunity-8/)|
|[Aboutweb](http://www.steele-consulting.net/opportunity-9/)|[Automation/Database tester](http://www.steele-consulting.net/opportunity-9/)|[US](http://www.steele-consulting.net/opportunity-9/)|[VA](http://www.steele-consulting.net/opportunity-9/)|[Reston](http://www.steele-consulting.net/opportunity-9/)|[Full Time](http://www.steele-consulting.net/opportunity-9/)|
|[Hired](http://www.steele-consulting.net/opportuniy-10/)|[Data engineer](http://www.steele-consulting.net/opportuniy-10/)|[US](http://www.steele-consulting.net/opportuniy-10/)|[CA](http://www.steele-consulting.net/opportuniy-10/)|[Mill Valley](http://www.steele-consulting.net/opportuniy-10/)|[Full Time](http://www.steele-consulting.net/opportuniy-10/)|",1557520026,2,0,0,dataengineering
Alternative job board with data engineer / AI positions and internships,,1557489941,0,0,3,dataengineering
How are storage benchmarks calculated?,"Is there a universal method to account for things like file formats, compression algos, serialization, etc?

&amp;#x200B;

For example, many presentations given at conferences often boast summary stats of a speaker's given company, usually in terms of throughput in terms of GB/TB/PB, but what does that really mean? 

&amp;#x200B;

If there is no standard, what should it be?",1557463031,0,0,1,dataengineering
Unit Testing Database Procedures and ETL Python Scripts - What's the norm?,"I'd like to figure out what the norm is for testing database stored procedures and ETL scripts. The testing processes at my current company seem extremely inefficient and make it difficult to maintain code for even simple changes due to a poor testing setup.

**1.** Are database stored procedures often tested with unit tests/CI tools or is most testing a manual process?
**2.** Do you always write unit tests for your Python ETL scripts?
**3.** Are you always repopulating your development environment with new data, how is this handled?

If you have any generally helpful testing tips please share them.",1557459972,4,0,14,dataengineering
Resources to learn from,"Since I think we can all agree that the resources are pretty scattered, so I thought it would be a good idea to collaborate and create a list of all the resources we think are good. Maybe later we can start the wiki page and add a page with all this info.

I also thought that would great to have a post with the most popular tools, or the category of each tool with some description, use-case and important things that everyone should know about. But I don't know enough about all of them to create it myself. Another approach to this would be a compilation of all the terms used in DE.

**SQL**

* [Mode tutorial](https://mode.com/sql-tutorial/introduction-to-sql/)
* [SQLZoo \(Exercises\)](https://sqlzoo.net/wiki/SQL_Tutorial)
* [SelectStarSql. Interactive course with real data of last statements](https://selectstarsql.com/) 
* [SQLBolt. Interactive tutorial](https://sqlbolt.com/)
* [SQL joins](https://www.technolush.com/blog/sql-joins)

**Spark**

* [Good course for beginners](https://www.udemy.com/spark-and-python-for-big-data-with-pyspark)

**Example projects**

* [Joseph Wibowo Project](https://josephwibowo.github.io/Meetup_Analytics/)
* [Vedanth Narayanan Project](https://github.com/narayave/Insight-GDELT-Feed)

**Concepts**

* [ETL &amp; ELT](https://www.guru99.com/etl-vs-elt.html)
* [OLAP vs OLTP](https://www.imaginarycloud.com/blog/oltp-vs-olap/)
* [Data model](https://www.talend.com/blog/2017/07/31/data-model-design-best-practices-part-2/)

**Books**

* [Designing Data-Intensive Applications](https://dataintensive.net/)
* [Spark: The Definitive Guide](http://shop.oreilly.com/product/0636920034957.do)

**Other places to learn **

* [cube.js blog](https://statsbot.co/blog/category/sql/)
* [Alooma blog](https://www.alooma.com/blog)


**Other resources**

* [Roadmap to learn data engineering]
(https://docs.google.com/spreadsheets/d/1GOO4s1NcxCR8a44F0XnsErz5rYDxNbHAHznu4pJMRkw/edit#gid=0)
* [Qwiklabs hands-on labs](https://www.qwiklabs.com/quests/25) - Google recommends to do first [GCP Essentials](https://google.qwiklabs.com/quests/23).

Now your turn :P, I'll edit this post to add your contributions. Also if you feel that some link, book or something is not good enough say it, because I think that if we put all the possible information there would be too many links.",1557348218,7,0,32,dataengineering
Best data engineering course on Coursera?,,1557317088,12,0,9,dataengineering
Best way to schedule ML training on Cloud? (Azure),"Hi, I am working on a project now which we want to implement in Azure and I'm seeking some advice.

It's a weekly forecasting job, and I want it to run dynamically (if that's a term), so e.g. spin up a VM once a week, run the computation and shut it down. Naturally I don't want to have the VM running 24/7 sitting idle 99% of the time.

The pipeline is pretty simple for this task: fetch data from Azure SQL -&gt; train/forecast -&gt; write results back to Azure SQL. This is all in Python.

From my perspective I have 3 options:
1. Azure Data Factory (seems rather clunky to use custom Python modules)
2. Azure ML Services (seems optimized for publishing REST APIs)
3. Databricks (no cluster, run 1 worker/driver. Seems to be more costly than the other 2?)

How would you guys automate this?",1557312331,0,0,1,dataengineering
Architecture Advice,"I'm a software developer (and not a DE) who recently inherited a project that I feel like is going in the wrong direction.

&amp;#x200B;

We currently have a dashboard that serves up real time analytics to \~100 users, aggregating data up and allowing them to filter across \~10 dimensions (including continuous \[minute level\] time). The backend for this is a SQL database that is continuously queried (resulting in 95%+ cpu utilisation and slow response times for users).

&amp;#x200B;

A project was underway to speed up the app by removing the SQL database and replacing it with a custom built data store that does some element of pre-computation, and while initial results were apparently promising, I'm unconvinced by the approach and think we should be using some existing framework.

&amp;#x200B;

The specifications of our system are as follows:

* Ingest \~100 events per second (but can peak at \~1000)
* Data is only persisted for 1 week (so peak around 60M records)
* Cardinality for each dimension varies from a few binary ones, to one that is \~5000
* Dashboard requires 5 queries, one of which is a time series
* Should be able to handle different filters for all users, so potentially \~500 queries
* Query response time (i.e. when changing filters) should be as quick as possible (aiming for sub-second)

&amp;#x200B;

I've done some research, and I think the best way to handle this is to generate streams on the fly for each view and set of filters although I'm worried about the time it will take to create each stream. Does this seem like a good approach or is it a terrible idea?

&amp;#x200B;

Would love any thoughts from people who have more experience than me!",1557311032,2,0,1,dataengineering
Top 9 MongoDB ETL Tools,"It is a list of the top ETL tools to extract data out of a MongoDB database. It is a mixture of open source and paid options (some of which have a free “community” version):

* [MongoSyphon](https://github.com/johnlpage/MongoSyphon)
* [Transporter](https://github.com/compose/transporter)
* [Krawler](https://github.com/kalisio/krawler)
* [Panoply](https://panoply.io/)
* [Stitch](https://www.stitchdata.com/)
* [Talend Big Data Open Studio](https://www.talend.com/products/big-data/big-data-open-studio/)
* [Pentaho](https://www.hitachivantara.com/en-us/products/big-data-integration-analytics.html)
* [SYNC](https://github.com/gagoyal01/mongodb-rdbms-sync)
* [YelpDatasetETL](https://github.com/apanimesh061/YelpDatasetETL)

Check the full article for overview and details on each of the tools: [Top 9 MongoDB ETL Tools](https://blog.panoply.io/top-9-mongodb-etl-tools)",1557305261,3,0,3,dataengineering
What do y'all think about Udacity's new Data Engineering nanodegree?,"I recently transitioned from a backend developer position to a data engineering position at my company. I know the basics of ETL and data warehousing, but not a lot of experience. I already finished the [dataquest.io](https://dataquest.io) data engineering track which covers basics in Postgres and Python, as well as some advanced topics like b-trees. However,  I feel like I need more experience with ""industrial strength data engineering"", such as architecting data warehouses, data lakes, building pipelines, and taking advantage of the cloud for computing and scalability.

&amp;#x200B;

It seems to be like the Udacity Data Eng nanodegree covers quite a bit of these ""industial strength"" best practices and tools. However, I also don't see the point of taking this course since I can just learn most of this stuff on the job anyways.

&amp;#x200B;

What do you guys think?",1557288530,17,0,13,dataengineering
An interview about the FoundationDB project and how it simplifies the work of building custom distributed systems applications,,1557238447,0,0,3,dataengineering
Where/How to learn to do dataengineering,"I feel there is a lack of guides/books, or maybe is because I'm searching with some terms that are not the correct ones.

&lt;- Open RANT -&gt;

&gt; My situation is that I end up in a company with no real data science team, only 2 juniors and a girl who before was a BI analyst (and now is the manager of this data science team), they told me there was a group, we would need to build the whole thing but I would have help, but the data arquitect that we had it left after 4 months saying that we needed data administrator and they were not willing to hire more people.
&gt; 
&gt; So now, most of the work falls in me, some weeks I have to do some analysis for one of the other groups in the company, then I had to try to use a HDFS cluster without any help, I had to discover that it was not done, and I couldn't complete the task. 
&gt; 
&gt; This last 2 weeks I have been researching an API of a supplier of data we have, but I don't know how to do it, no one else knows anything, and when I try to do something it don't work, don't know because the tools are very specific, new or is only because I'm dumb and I don't know how to do it. 

&lt;- Close RANT -&gt;

Right now my task is to create a pipeline to ingest the data of the API to a data warehouse(a cluster in cloudera with HDFS) that we will use for the whole company in some moment in the future. The problem is that the API is in SOAP with SSL, I tried using streamsets or nifi but they have problems because of the size of the returned file, when that doesn't fail, fails the server because I'm returning too much information and I can't do anything in 1 hour. Also I wanted to ingest it with a service of queue the API has, but it works with AMQP 1.0 with SSL, and neither streamsets nor nifi seems to support it, and there is no client of python to use it. Now I'm problems to use NiFi with SSL because I can't find many documentation in general.

I thought in using airflow with my own scripts, but is this better? When I should use streamsets/nifi or airflow, what kind of services there is? I should go ELT or ETL? what are the tools that exist for ELT? I don't know I have too many questions and when I search for them I can't find good answer, or they only explain half of it.",1557224603,15,0,11,dataengineering
Best way connect/map a dataset of texts to their corresponding floating point vectors?,"I have a dataset of texts and each text has a corresponding 768 dimensional vector. 

I have learned that there is no data format which is optimized to hold these dual datatypes, so I have to store them each separately, and figure out a way to map the two datasets. 

One way is to make sure they're both in a very specific order, but I feel that may be prone to error. I've heard there are some mapping methods. Does anyone have experience in this area who can give recommendations?",1557076755,1,0,2,dataengineering
Found Udemy Discount Coupon For Apache Spark so thought worth sharing,[removed],1557045799,0,0,1,dataengineering
People looking for data engineering projects: here is one,,1556913632,7,0,19,dataengineering
Anyone here ever use Serverless?,"Also, what was your experience like?",1556849762,6,0,4,dataengineering
Build system for data engineers,[https://github.com/prodmodel/prodmodel](https://github.com/prodmodel/prodmodel),1556767303,0,0,0,dataengineering
Databricks Koalas-Python Pandas for Spark,[https://medium.com/@achilleus/databricks-koalas-python-pandas-for-spark-ce20fc8a7d08?source=friends\_link&amp;sk=a1e3979e61aeebd914f4e88ebf8e30aa](https://medium.com/@achilleus/databricks-koalas-python-pandas-for-spark-ce20fc8a7d08?source=friends_link&amp;sk=a1e3979e61aeebd914f4e88ebf8e30aa),1556740520,3,0,4,dataengineering
Why Not Airflow?,,1556734521,14,0,23,dataengineering
How to ensure consistency between Dataflow pipeline runs? Help me get from bash script to Python test,"I have a large NLP Dataflow pipeline which takes a set of documents and enriches it with all kinds of NLP extraction data. My current approach to achieve consistency is building a test harness that checks whether runs remain the same. This is to ensure that env upgrades don't cause inconsistent runs.  


The pipeline run results are stored on Google Cloud buckets with gziped chunks of json newline files.  


Currently I use bash to download buckets in parallel with `gsutil -m cp` then I merge the relevant json files into one file with Python and then `sort -u` to deduplicate merged files and then use `cmp` to compare the merged runs.  


The issue with this approach is that it is difficult to run this process through CI which is what I would like to do ideally refactoring the above into Python code which can validate that test runs are consistent.

&amp;#x200B;

One option I was thinking about was adding an additional step in my pipeline which will check the pipeline results are the same with a reference result set.

&amp;#x200B;

I feel others must face the same problem and I was wondering what your solutions were?",1556707291,2,0,2,dataengineering
Aggregating Data Sources - Expertise?,"**Background:**

We're starting to attract clients that are more upper funnel. They need help aggregating data sources from Google Analytics, Salesforce, Mailchimp, proprietary systems, disparate databases... etc.. to then be compiled into a central repository. Most likely a cloud hosted relational db (AWS RDS). The end goal is to have this data aggregated, readily available for dash-boarding. 

**Questions:**

1. Is this even a data engineering problem?
2. What type of training would be required to even tackle this type of work?
3. What have others done to aggregate data sources?
4. Is using a tool/site like stitchdata a viable option?

&amp;#x200B;

**Notes:**

I understand different elements/softwares/packages would probably be needed on a case by case basis in regards to each particular data source -- so not a one size fits all approach BUT where would we even get started to tackle this? I'm sure this problem arises in other agencies and in-house, how do others tackle this sort of thing?",1556649642,4,0,1,dataengineering
Study Partner in Data Engineering Project,"folks , I have identified a simple data Engineering project which involves creating a analytics pipeline  . I am looking for someone who wants to learn creating pipelines which is mostly the job of  Data Engineers. please drop me a message for further discussion.",1556642079,20,0,10,dataengineering
12 Data Integration Tools - A Cost Benefit Analysis,"Data integration is key to solving a crucial problem in any organization’s data analytics flow. You may track data related to your business down to the most minute detail, but if is also important to have every relevant piece of data you need readily available.

Here is a comparison and key decision factors for how to choose the right data integration tool: 
[Top 12 Data Integration Tools In 2019 - A Cost Benefit Analysis] (https://blog.panoply.io/data-integration-tools)

* for a big organization with big data: Informatica, Oracle, IBM, SAP or Hitachi
* to maximize ease of use: SSIS, Panoply, Dell Boomi AtomSphere or Astera Centerprise
* to minimize cost: Panoply or Dell Boomi AtomSphere
* for government, healthcare or other specialized data: Denodo, Hitachi Vantara or InterSystems Ensemble",1556616359,3,0,1,dataengineering
Interview Advice - System Design,"I previously worked as a BI/Data engineer for Adobe, but have taken 1.5 years off pursuing startups. Upon coming back, I have refreshed myself on most tools - but am mostly failing at system design style of questions.

So I was hoping I could ask you guys what your response would be to a proposed problem.

&amp;#x200B;

**System outline:**

1B records coming in per day, for an advertising company

Each record has the following information:

* Time
* Hit type(mobile/desktop)
* Account
* Campaign
* Impressions
* Interactions

&amp;#x200B;

The first question they asked was how would I design a system to house said data and make it reportable within moments of new data arriving.",1556579846,7,0,8,dataengineering
Get started with Pyspark on Mac using an IDE-PyCharm,[https://medium.com/@achilleus/get-started-with-pyspark-on-mac-using-an-ide-pycharm-b8cbad7d516f?source=friends\_link&amp;sk=d4f25f2069d8602286f279d02026ffcc](https://medium.com/@achilleus/get-started-with-pyspark-on-mac-using-an-ide-pycharm-b8cbad7d516f?source=friends_link&amp;sk=d4f25f2069d8602286f279d02026ffcc),1556578514,0,0,1,dataengineering
An interview about how to run your database on Kubernetes with the creator of KubeDB,,1556544121,0,0,6,dataengineering
Tips for junior data engineer interview?,"I have an interview for a junior data engineer position, anyone have any advice or tips that I should know before going into it?",1556493320,1,0,4,dataengineering
Engineering Blog Recommendations?,I've heard Uber's engineering blog is good to follow.  Any others that you think would be good?,1556472332,13,0,17,dataengineering
Is data engineering the field for me?,"Hey guys. I have been having a lot of confusion as to what my go to field is. I love programming. Especially programming that runs stuff under the hood without any creative graphic user interface (to be precise.. I love the black screen). I love solving logical problems. It has been a year since i started looking exploring the field of data science. I worked with pandas and numpy in data gathering(web scraping, from APIs), data wrangling and data visualization, basically, everything leading up to data analysis except the analysis part. I loved it but i did not like the analysis part and the inference from graphs and charts part. Then i started looking into machine learning engineer field and started learning about machine learning models, implementing them using scikit learn, tuning them, natural language processing basics etc., I liked it for a while but then after i started making a few portfolio projects, I started feeling like i was missing something. Just knowing machine learning algorithms and how to implement them did not excite me. It did not feel like problem solving. Then came a lot of GUI machine learning tools such as Weka. Plus the problem solving part of machine learning which is data exploration part did not feel like my area of strength. I hated the data exploration part. Then i looked into data engineering field. I have been learning about data wrangling, warehousing, job scheduling, cloud databases etc., So far i love it and am excited to learn big data technologies. But i am worried as to whether this will also turn out to be one of them data exploratory, statistical part. I love scripting, programming and logical problem solving. Can anyone provide recommendations as to how i can get a overall understanding of what data engineering field exactly is, how much programming it involves and if it also deals with statistical inference?",1556400142,1,0,1,dataengineering
"Delta Lake - ACID transactions, versioning, and schema enforcement to Apache Spark","My take on Delta Lake  is a new open source project from Databricks  with some sample Scala examples to get started with Delta Lake

This adds ACID transactions, versioning, schema enforcement and lot of new features to Spark data sources that don't have them already!

[https://medium.com/@achilleus/delta-lake-acid-transactions-for-apache-spark-2bf3d919cda](https://medium.com/@achilleus/delta-lake-acid-transactions-for-apache-spark-2bf3d919cda)",1556398733,2,0,4,dataengineering
"Do, i have the skills for data engineering? Resume attached","Hi all, 

I recently interviewed for a data engineering position and it went horrible and incredibly awakward (phone interview), especially when the interviewer told me she had the wrong resume, then tried to look for it and couldn’t find it. 

She asked if i had a degree in Data Mining and oriented objective design, since I got my degree in Stats, i was a bit familiar with data mining but Oriented objective design sounded more like CS to me so i had told her i wasn’t familiar with the term, tho i probably knew/know it as something else and should have asked. My fault, anyways, the tone/vibe completely changed.
(Originaly, this interview was suppose to be done by the HR manager but she had emailed me a day earlier saying she needed to reschedule the interview, and that it would be a colleague of hers that would contact me at the new time)


Anyways, i dont know if i should apply to other dat engineering positions as I don’t even know if i have the skills for it or have past experiences that may indicate i could learn in the job ect. 

So here is my resume, detailing my skills and projects-and stuff.

Languages I know are R, Pyspark, Latex, and SAS
https://m.imgur.com/NLSNhHx",1556383174,14,0,13,dataengineering
Know all about the best online Machine Learning courses in 2019,,1556379287,0,0,1,dataengineering
Junior first round,"I have a junior data engineering role that is not a. ETL position but will working with building their own type of pipeline.  They phone screen me then send a project for me to do, which will use numpy and pandas.  Then there is a two hour onsite interview, which I am positive is a grueling DS test, then grilling me out tech, then meeting team leaders.  Should I focus more on data structures and blowing those away?  I am a math major, I know C++, C, python etc.  so the project of data munging will probably be doable if I focus and test out of my Google skills.  What do you guys think?  Are data structures more important? 

Should I use python as my interview language? (I know Java but I dont have a great handle on the data structures and interfaces for each, C++ with no STL, and then Javascript hahah).  They will want  me to build full stack applications so I am guessing D3, matplotlib, etc.  

I have been toasted in interviews before and I would like to hear whether the onsite is going to be a data structures test or just a culture test meet n' great.  I dont have a full degree yet but I have a lot of math under my belt (All calcs, linear algebra, differential equations, group theory, probability and stats Calc based, discrete math) and a C++ course and C course with a data structures class in C++.

Any go to study material for these positions? Is domain knowledge more important or do they just want a guy who knows python and programming really well?",1556338202,1,0,5,dataengineering
SQL question for data engineering roles?,Where  do you guys go to practice SQL question for data engineering roles. I do not mean basic and simple select questions. I mean the type of slightly complicated  sql question which get asked in interviews FAANG?,1556300827,15,0,5,dataengineering
Data Engineering Conference in Europe 2019,"Hey!

ITNEXT is organizing a conference in Amsterdam on October 30th. One of the tracks is about Data Engineering, and we will have Holden Karau hosting it... our Call for Papers is open, so I decided to share here! Come to lovely Amsterdam to LEARN. SHARE. CONNECT. on the ITNEXT Summit 2019!

I know plenty of AWS enthusiasts have something to share! :-)

&amp;#x200B;

Main website is here: [https://www.itnextsummit.com](https://www.itnextsummit.com)

CFP here: [https://sessionize.com/app/organizer/event/1334](https://sessionize.com/app/organizer/event/1334)",1556273845,0,0,3,dataengineering
What are the challenges you have encountered in building/maintaining/using data lakes?,"We (data curation lab at Univ of Toronto) are doing research in data lake discovery problems. One of the problems we are looking at is how to efficiently discover joinable and unionable tables. For example, find all the rental listings from various sources to create a master list (union); or find tables such as rental listings and school districts that can be used to augment each other (join). The technical challenges in finding joinable and unionable tables in data lakes involve the following: (1) the data schema is often inconsistent and poorly managed, so we can’t simply rely on that schema; and (2) the scale of data lakes can be in the order of hundreds of thousands of tables, making a content based search algorithm expensive. We came up with some solutions that are based on data sketches with several published papers \[1,2,3\]. The python library [datasketch](https://github.com/ekzhu/datasketch) was a byproduct if these work.

Many challenges remain though, and we would like to explore some of the more pertinent ones. In fact, we are conducting a survey to understand the current state of data lakes in industry and the challenges experienced. If you're interested in learning more, see what we came up with here: [https://www.surveymonkey.com/r/WLCYTVZ](https://www.surveymonkey.com/r/WLCYTVZ) \- would love to see what the Reddit community thinks about the current state of data lakes. You will have a chance to receive a 50$ gift card.

\[1\] [http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf](http://www.vldb.org/pvldb/vol9/p1185-zhu.pdf)

\[2\] [http://www.vldb.org/pvldb/vol11/p813-nargesian.pdf](http://www.vldb.org/pvldb/vol11/p813-nargesian.pdf)

\[3\] [http://www.cs.toronto.edu/\~ekzhu/papers/josie.pdf](http://www.cs.toronto.edu/~ekzhu/papers/josie.pdf)",1556203615,10,0,11,dataengineering
Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark™ and big data workloads,,1556148611,5,0,9,dataengineering
Spark UDFs. We can use them. But Should we use them?,,1556135168,2,0,3,dataengineering
"Anyone have experience with data engineering at a financial company (hedge fund, quant fund, big bank etc.)?","And if so, can you describe what it was like?  Or how it might differ from data engineering at a tech company?",1556127651,5,0,3,dataengineering
"Have you ever used a JSON-based format for representing graphs/networks? If yes, how was the experience?","Hi everyone,

I'm working with graph data and want to store it in a JSON-based format. Objectives are to allow analysis outside Neo4j and easily import into DB when required. I'm willing to write a custom parser for import but would appreciate if something exists already.

I came across a few formats (listed below) but I couldn't find user stories as such. I'm very interested to know if you've ever tackled with a similar problem statement.Graph JSON Formats:

\- GraphSON ([http://tinkerpop.apache.org/docs/3.4.1/dev/io/#graphson](http://tinkerpop.apache.org/docs/3.4.1/dev/io/#graphson))

\- JSONGraph ([http://jsongraphformat.info/](http://jsongraphformat.info/))",1556091424,2,0,5,dataengineering
[Newbie] Apache Beam where does it fit in among other tools?,"Hi, I am a newbie to the Data Engineering field (6 months in). I am starting to understand various tools fit in the ecosystem. Apache Spark, Kafka, Airflow. But sometimes really find it hard to grasp the role of the tool in the ecosystem. I just have discovered Apache Beam, could somebody help me understand where does it fit in and where is it useful or in which use cases is it useful. Should I learn this new tool too? Because, everywhere I see these new tools and I am confused what their roles are and should I bother learning them. And to make that decision I need to go through different 1hr talks of the tools and they make it sound like you need to get onboard this train.   
Also, If anybody has any guidance to offer on moving forward in the field, if one should go after learning tools or concepts. If one should pursue concepts which ones should I start with? I know this question took a turn at the end. But I would appreciate any kind of guidance on growing and learning in this field from a perspective of a newbie, overwhelmed by all the fancy tools/terminologies surrounding around him/her.  


Thank you.",1556089516,1,0,1,dataengineering
A deep dive on building the Fauna database and how it supports transactions at global scale,,1556069504,0,0,2,dataengineering
Serializing JSON and Writing it to a file? Apache beam,"Hey everyone, I have this project in apache beam where I am reading 99 files calculating their checksum and I want to covert that to a JSON KV pair and write it to a file called manifest.json is there anyone who can help me? I am having no luck with this and have been stuck for a couple of days. 

Here is my code:

https://pastebin.com/PpYybAds",1556063979,0,0,1,dataengineering
Insight Data Engineering Fellowship,"Hello Everyone,
Actually I applied for Insight Data Engineering Fellowship, after completing coding challenge I got an interview. I did clear the 30 minutes interview but I got another email requesting to appear for a final video interview.
My query is I talked with some former fellows and came to know they only had one interview.
So I’m just curious that what should be the reason/scope for an additional interview.
Any suggestions or help would be really appreciated!! 
Thank you :)",1556053428,7,0,1,dataengineering
Transitioning from Data Analyst to Data Engineer?,"I've been looking to transition to something new within my career, but staying in the data pathway. Currently, I've been a data analyst for the past few years and the work is getting repetitive and I don't think I want to be analyzing data, building reports/visualizations, and communicating business data to clients for the rest of my career.

I've looked into Data Science and that is still a possibility for me, but I would like to know more of Data Engineering and if that's a better fit for me. I was looking at job postings recently and a lot of Data Engineer jobs requires some analyst work such as what I have been doing now (that is a plus for me).

My question is how do I even get my hands into such a Data Engineering position?

I know this position and pathway requires a lot more technical skills than a Data Analyst, and I want to know which skills/tools/certs should I start learning/studying or continue to develop right now on my spare time?

Are there any side projects I can work on the side right now, to show recruiters how I can apply those skills?

Any advice, tips, or resources is much appreciated!

Thanks!",1556036759,18,0,16,dataengineering
Storage for Personal Data Engineering Projects,"Greetings everyone, I am wanting to pursue a career in Data Engineering and similar to Software Engineering, I know that I will need to have some personal projects to help show what I know. I haven't  started anything relating to Data Engineering because I am still finishing out my undergrad for Information Systems, but I plan to start over the summer - learning as much as I can. That being said, how much storage would I actually need? I currently have 2TB HDD's along with a 500GB 970 Evo. I would like to think that would be enough, especially for a novices level.   
Thank you in advance for any advice.",1556000212,5,0,5,dataengineering
Am I crazy for thinking the data vault pattern is a good idea in my use case?,"Hi all,

I've read mixed reviews about the data vault modeling concept, but I'm thinking its right for my use case. If anyone has any experience with this, please let me know. I've read about it, but have never built it / run it in production.

Here are the facts I'm dealing with. I'm currently working at a start up as a one man data team. This means data analysis, science (or lack thereof), and engineering. I'm primarily a data engineer, but I used to be a data analyst, which is why I was brought in to start the department. We were originally going to hire someone who would be my senior, but the budget got tight (start up life), and now I'm on my own. I feel confident in building and maintaining the systems I've put in place so far, but I'm needing people to bounce ideas off of.

I'm dealing with multiple data sources, one being a postgres database, one being FTP files (csv), and a small number of others that are being piped into Kinesis and converted into parquet files in S3. I have an airflow instance running, and I've taken heavy inspiration from the functional data warehouse. I'm currently doing all of the transformations in an ELT architecture through airflow tasks. I'm planning on migrating to DBT for this portion of the pipeline, but haven't started that migration yet. The problems I'm starting to run into are that I'm needing to constantly rebuild the warehouse I'm maintaining due to our other team's changing requirements. The warehouse takes about a day and a half to rebuild, and most of that time is waiting for files to get copied into redshift. The actual transformations in my ELT architecture are relatively light.

I'm wondering if this is a good fit for the data vault, or at least a modified version of it. I'm considering this because it seems to be an intermediary between the end user and the actual extraction of data sets. It also seems relatively easy to set up new sources, which would be a very very nice thing to have as I don't always get long enough heads up to build out a solution before its implemented in the source. I've also considered implemented just a persistent staging area within Redshift itself, which would just be making sure that data is loaded into copies of tables from the source, then letting DBT / transformation logic handle properly building the model. Both options seem like they would handle the problem I'm currently having.

I know one downside of the data vault is that it doesn't scale to huge amounts of data. That isn't currently an issue, nor will it be in the coming future.

Thank you everyone! Any help is appreciated.",1555994192,2,0,4,dataengineering
Data Engineering Career Outlook,"I'm just wanting to start a discussion regarding the career outlook for Data Engineering/Science.

I'm currently in grad school studying Software Engineering + work fulltime + doing some extracurricular DE learning on the side with the intention of transitioning to a Data Engineering role shortly after graduating.

I'm expecting to graduate in about a year and a half and wanted to get some experienced DEs opinions on the future of these roles? Is the market over saturated, etc.?

Personally, all of the various technologies that are used in DE excites me and I feel like the role would promote continuous learning vs what I do now which is very repetitive unfortunately...",1555969646,14,0,8,dataengineering
Partitioning in S3 for Completeness vs Querability,"Right now my group's data is stored in S3 based on the hour we got the data. so 2019-04-22-01 would be the first hour of April 22nd, and so on. This helps those consuming this data regularly and in full, to do so with confidence of completeness. As long as you aren't querying the current hour, you know that no more data will be placed in the hours you are pulling.

&amp;#x200B;

However, the underlying data, stored within these files have their own timestamps, usually relevant to how the data is being used. And so we occasionally get asks for data based on those values, which is in no way, easy to access, or know where all of it is.  


Obviously, my first thought is to make a record of where the Data Timestamps exist within each Server Timestamp Hour we store on. This would require reading each file we get, which isn't currently done.

&amp;#x200B;

I assume I am not the only one who has faced this issue, and am curious, if I am going to crack the egg open on this, and begin reading every file for a certain value; What other things should I consider? How else have people tackled this problem of dealing with Time Received vs Time of Original Data Creation? Are there existing best practices around this? etc.

&amp;#x200B;

Any thoughts, or places I can read up on this more, would be appreciated.  


Thank you.",1555960386,4,0,2,dataengineering
Vehicle telematics IOT solution,"Hi,

Does anyone have worked in this industry? I am a big data professional and exploring this field. Can anyone please help me with resources/books in understanding use cases of collection such data and doing analysis on it. Also what all on cloud data tools and anayltics present to ingest such high amounts of data and do analysis",1555914821,1,0,2,dataengineering
How Common is Freelance Data Engineering?,Does anyone have experience doing freelance data engineering?  It would probably only be after you have a lot of experience right?,1555687012,2,0,3,dataengineering
Whiteboard Questions for Junior Data Engineer Interview?,"I have an interview in 2 weeks for a junior data engineering position.  They said there will be 20 min of whiteboard questions for basic coding skills (python, R, java, whichever one I'm strong with, which is python).  Would leet code prep be good?  Would they ask specific algorithms?  Any common questions you've all seen?",1555685099,13,0,11,dataengineering
Is anyone here using R with SSIS instead of C#?,I have a much easier time working in R Studio and manipulating data on the fly than trying to use C# in Visual Studio. I was wondering if anyone here has had success in using using R script components in their SSIS workflows?,1555645577,2,0,2,dataengineering
Question: Is Apache Hadoop worth learning?,"I'm enrolled in a business analytics masters program and my uni offers a course ""big data analytics with Hadoop""

&amp;#x200B;

I am really interested in manufacturing and business process reengineering, I want to pursue an analytics career involving these fields. 

&amp;#x200B;

I have no prior IT/BA/BI experience and this is a new world for me, so I want to know, is Hadoop worth learning for me?",1555644735,5,0,0,dataengineering
Ubuntu LTS or latest release for DE,"Hello everyone, Canonical release Ubuntu 19.04 today, and I was just wondering if data engineers would stick to LTS, latest release, or another distro with rolling releases?",1555635400,5,0,1,dataengineering
Ideas for Interviewquestions,"Hi,
I‘m interviewing an Data/ETL-Engineer and i‘m
planning some questions and tasks as a test. 
Are there any good resources or tipps what i should ask?

I would be very interested how she/he would build a dwh or datalake from scratch. Which database scheme would be used and how normalized should it be? But i‘m not sure how to formulate this correctly and what information would be needed to answer this...",1555446497,5,0,6,dataengineering
"An interview about the Pilosa bitmap index server and how it can be used to run fast, continuous analytics on large and complex data sets",,1555422779,0,0,0,dataengineering
What makes a good ETL tool?,"I wonder why the data engineering community use workflow management tools like Airflow or Luigi, while the DevOps  community use tools like Concourse CI. It seems like any system with the mechanism for scheduling tasks and jobs can be used for ETL pipelines.

&amp;#x200B;

Would Concourse CI be a good replacement for Airflow?",1555378825,8,0,4,dataengineering
Thought this might be interesting: How Enigma Containerized Their Workflows, [https://www.enigma.com/blog/containerizing-data-workflows](https://www.enigma.com/blog/containerizing-data-workflows),1555353873,0,0,2,dataengineering
r/dataengineering Discord server!,"As was discussed in a previous post [here](https://www.reddit.com/r/dataengineering/comments/bcerdk/data_engineering_discord_server/?st=juie74f9&amp;sh=a5d8ae86) I have made a Discord server for this subreddit. Feel free to suggest any enhancements/changes to the Discord which will make it a better place!

You can see the Discord [here](https://invite.gg/dataengineering)

Alternative link [https://discord.gg/2pER6dq](https://discord.gg/2pER6dq)",1555334962,5,0,7,dataengineering
Data Engineering conference in Europe - 2019,"Hey!

I am organizing a conference in Amsterdam on October 30th. One of the tracks is in my area, **Data Engineering**, and we will have **Holden Karau** hosting it... our [Call for Papers is open](https://sessionize.com/itnext-summit-2019/), so I decided to share here! Come to lovely Amsterdam to LEARN. SHARE. CONNECT. on the [ITNEXT Summit 2019](https://www.itnextsummit.com/)!",1555324936,1,0,3,dataengineering
ELI5: Columnar Databases/Data storage,"I've been working the field for about a year now and regularly work with columnar data warehouses (Snowflake, Redshift) but honestly don't really understand what this means or why it's good. Can anyone help me wrap my head around it?",1555264770,3,0,8,dataengineering
File formats to replace a Postgres data warehouse?,"I am wondering if there are any file formats that I should be looking into instead of PostgreSQL. I love Postgres but I just want to know what is out there. Our data warehouse is kind of just an intermediate step before a visualization tool. It is important, but it is not the enterprise data warehouse.

&amp;#x200B;

* I use Airflow for scheduling ETL tasks to grab data and automate reporting by sending the data to a visualization tool
   * 40% of my data is extracted from replication servers (MS SQL, MySQL) where I bulk export new or changed rows, and load it to Postgres. 
   * 40% is from files (Excel, csv files etc. which are shared through SFTP sites and email mostly. The format of these files is wildly different. For a recent project, I just had to extract 218 distinct files.), 
   * 20% is from REST APIs
* I use a tool called sqitch for database migration, so at some point I need to write the deploy, revert, and verify scripts for each table
* I no longer normalize most of the data in Postgres. It is easier to just kind of keep it as raw as possible
* I have been updating rows historically if there are primary key conflicts, but in a lot of cases I kind of wish I only inserted new rows so I would have the history of data
* Ultimately, I use COPY to query data from the tables to a CSV file to load into a third party visualization tool through a rest API. This tool only accepts data in csv or csv.gz format

&amp;#x200B;

Are there any file formats (arrow, parquet, etc.) which I should look into since I am not using a lot of the database features? Our ultimate need is to be able to query for data and get it into CSV format to start streaming it the visualization too. It'd be nice if backups were as simple as rsync or something. Joining data would be useful, or I could also create 'fact table' files by doing joins in pandas and writing the results to a separate file.",1555260394,5,0,3,dataengineering
Recommendations for MOOCs,"Apologies in advance, I realize there are innumerable ""How to become a data engineer"" posts here, on Medium, on Quora, etc. and many data science sites that link to MOOCs.  However, I'm kind of overwhelmed by the sheer number of MOOCs dealing with data engineering and the multitude of technologies, and it seems like the landscape is shifting rapidly, so I'm not even sure if I should trust recommendations from 2017 at this point (there seem to be a lot of ""Hadoop isn't dead yet, but...."" posts)  

I'm trying to transition from a conventional RDBMS DBA to potentially a Big Data engineer - I have basic Python, ETL, dimensional modelling, database, and data visualization knowledge.  Basically hoping someone can give me some ideas on the 'best' (subjective, I know) MOOCs from the perspective of where the industry is going and which technologies are most in-demand for transitioning to a new career.",1555204593,9,0,20,dataengineering
"""Here’s why so many data scientists are leaving their jobs""","[https://towardsdatascience.com/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4](https://towardsdatascience.com/why-so-many-data-scientists-are-leaving-their-jobs-a1f0329d7ea4)  
For those who are employed as a Data Engineer, is this applicable for you and the data scientists around you?",1555135091,5,0,8,dataengineering
What would you say is the hardest part of data engineering?,"Also, are there any tools you wish you had that would make your lives easier?",1555132030,8,0,4,dataengineering
Data Engineering Discord Server,Does anyone know of a Discord server dedicated to Data Engineering discussions and other related topics?,1555082329,16,0,10,dataengineering
Question about the interactions between Data Engineering and Data Scientists,"I recently started a position as a Data Engineer and Scientist for a medium-sized healthcare startup, and I would like to know from those of you who have had good and bad interactions with Data Scientists what I might want to keep in mind. My primary focus right now is almost entirely on the Data Engineering side; I feel I'm in a unique position to design things with all of the data roles in mind. With more data team members being hired in the future, I'd like to create an infrastructure that meets everyone's basic needs.

&amp;#x200B;

 Any comments, advice, or criticism is welcome!",1555020690,10,0,2,dataengineering
Big Data Engineer,,1554986643,0,0,0,dataengineering
Learn Docker to get started with Spark,[https://medium.com/@achilleus/learn-docker-to-get-started-with-spark-dd8468e9de5b](https://medium.com/@achilleus/learn-docker-to-get-started-with-spark-dd8468e9de5b),1554957445,0,0,10,dataengineering
Any insight on the kind of data engineering that would be behind the Black Hole photo?,,1554939260,2,0,3,dataengineering
Can you kindly help me smash my Friday Data Engineer interview?,"Hello r/dataengineering

&amp;#x200B;

Please could you kindly help me in understanding this presentation stimulus?  
On Friday at 2pm, I have a second interview where I am required to present for 10 mins on:

  
**“CI/CD pipeline that deploys R code onto AWS Lambda”**  
I'm interviewing for a Data Engineer position at an organisation which has petabyte-level data, rich and juicy.  
The head of the department who interviewed me on the phone for the first round knew I only knew basic Python, and have no knowledge of R or AWS. 

  
**I think therefore, that this is a challenge to see how much I can learn and understand about a technical topic in a short period of time. I am considering you a wealthy resource of information from which to learn from, and would be very grateful for any guidance you might have on understanding these components in addition to my own reading.** 

&amp;#x200B;

  
Continuous Integration/Continuous Deployment Pipeline  
Amazon Web Services Lambda

  
My learning process tomorrow and up until the interview will be:

1. High level concept explanations from Wikipedia, Reddit ELi5/ youtube introductory videos
2. Original Company Literature
3. Examples on Github and Stackoverflow if applicable
4. General web search

Any guidance or pointing to resources is helpful! I'll buy you all a drink if I get the job!

About me: I'm trying to transition from mechanical engineering into data engineering, have learnt a tiny bit of Python [https://github.com/out-liar](https://github.com/out-liar) This would be my first real job after 2.5 years of internships in technical roles, and I want le moneh 2 help my mum and sister out financially :)

&amp;#x200B;

I also have to sit a **20 min RANRA test**, which I think stands for Rust Advanced Numerical Reasoning Appraisal. Any advice on practicing it would be highly appreciated, although not directly relevant to this sub.

&amp;#x200B;

Thank you for your knowledge, thoughts and time!",1554906575,2,0,1,dataengineering
Why a data scientist is not a data engineer,,1554902008,45,0,32,dataengineering
[Free Webinar] Insights Discovery at the Intersection of Multiple Data Sets with Kirk Borne,,1554901757,0,0,1,dataengineering
How Apache Airflow Distributes Jobs on Celery workers,,1554728868,0,0,10,dataengineering
An interview about how DataCoral is building an abstraction layer over data pipelines using microservices built on serverless technologies,,1554727645,1,0,1,dataengineering
Opinions on a series of data engineering lessons,"So, recently I realized there aren't really any ""holistic"" data engineering tutorials.

There's loads of material out there talking about various niche subjects that can be interesting for someone already working with a specific set of tools or on a specific set of problems.

But, if a beginner programmer or CS student is interested in the field, there's not a load of material out there besides just learning how databases work and getting good at CS and programming in general.

I've started and attempt to create a series of lessons on data engineer that I would aim at someone like myself 5 years ago, when I was barely getting into programming. A sort of overview of the field that contains an introduction (and a few deep dives) into terms, tools and paradigms of thinking that can be helpful to people.

I wonder if you guys have any opinion on it: https://youtu.be/-mA00jcVJhw

I've only put out 2 videos thus far, and they are quite long, but if you've got even 5-10 minutes to spare in order to give me an opinion, that would be great.",1554720157,7,0,23,dataengineering
important skillset for a Graduate Big Data Engineer ?," im going to an interview for a data analytics company for 12 months as a data engineer, ill be moving around a few teams over the 12 month period all the company does is sell data , their whole infrastructure is built with java, any advice on how I could prepare, I know that it's going to be a 2 part interview verbal + coding assessment after.

im going to be given some information next week so I can prepare for it more but apart from that, I want to work my ass off to get this job, any suggestions on what I should look over in order to help myself secure the spot?

im working on Java 8 and 7 to make sure I know it like the back of my hand and some algorithms since my degree is a mix of network and software engineering and we didn't do much work on algorithms some would be useful for the sake of data processing I think , im also familiarising myself with Hadoop and spark on my home apparatus i built with raspberry pies.

anything else?

many thanks",1554665979,10,0,5,dataengineering
Need help in choosing technologies - Storm Vs Kafka vs Spark,"Hi everyone,

&amp;#x200B;

Our team currently scraping the data. We are using Apache Kafka as a link between spiders and SQL Server. Currently we are storing unprocessed data in the database.  Now we want to do some kind on text processing (like standardizing the URL, units, and remove of some noisy words). So Is kafka able to do the text processing or do we need to use the Stream processing technologies like Apache Storm, Apache Spark, Apache Samza.   
What are potential blockers or difficulties we may face in this situation.   
Thank you in advance  


&amp;#x200B;",1554615902,4,0,11,dataengineering
What are your thoughts on data virtualization?,"My company will embark on using Denodo as a data virtualization platform.  I only have a high level understanding of data virtualization.  It is touted as a technology that will reduce the need for ETL and somehow creates a view layer for all the disparate data sources.  I am curious how data virtualization maps or merge the disparate data sources together?  I assume someone will have to define the semantic layer somehow.  Is the resulting view layer stored in memory?
  
I am not part of my company's IT organization so I can't really provide more details on why they chose data virtualization instead of other solutions.  In this sub, I don't see data virtualization talked about much or know of companies using data virtualization.",1554565212,12,0,8,dataengineering
Best Data Engineering Conference to learn more about Airflow?,[removed],1554455733,0,0,1,dataengineering
Preparing for Data Engineer position at Google,"I just happened upon this group - and have been reading over loads of peoples input, so I just wanted to reach out and see if there is anything someone on this forum would suggest to me for preparation.

I previously was a Data/BI Engineer for Adobe - which meant I essentially handled the full data stack, from the consulting side to the implementation(ETL/ELT, DBA for noSQL and Relational stores, ingest, and reports). I ended up quitting what I consider to be my dream job to work on a startup with friends - which failed after a lot of work. So I have now been out of the space for 1.5 years, consulting when I can.

Just recently google contacted me about a job on their GCP Professional Services team as a Data Engineer. I want this job very badly, and am trying to find a good refresher/updater for systems architecture - hoping to get deep enough to understand configs etc. Currently I am thinking of just doing Udemy to catch up on any techologies I am uncertain of, and do LeetCode questions.

Does anyone have any good tips for preparing?",1554433337,11,0,16,dataengineering
Redshift indentity column issue,[removed],1554415515,0,0,1,dataengineering
Finding the Right Data Visualization Tool,"I recently started working at a small (but now growing) e-commerce company that is in the process of building out a database in Amazon Redshift (with sales data, marketing data, shipping data, really everything we can track). I'm trying to find a tool that will sit on top of our redshift cluster and allow non-technical folks to visualization different queries. 

&amp;#x200B;

Ideally looking for something that integrates reasonably easy with Redshift, has some flexibility, and isn't crazy expensive. I've started looking at Tableau, Power BI, and Plotly but it seems like there is a lot out there and I don't really know what I'm doing so any general advice (or specific advice!) would be very much appreciated!

&amp;#x200B;

(sorry if this isn't quite the right subreddit for this sort of thing)",1554413337,4,0,1,dataengineering
NiFi plays a huge role in Big Data and Big Data Analysis.,[removed],1554340108,0,0,1,dataengineering
Company offering to pay for training for Scala/Spark/DataBricks,[removed],1554338814,0,0,1,dataengineering
Data Analyst to Data Engineer,"Im two years out college with a CS degree. My only experience is as a Data Analyst. I have a lot of SQL knowledge, Excel and some VBA, a couple project in VB.Net, and taught myself a little python. I also tried SSIS at another company but i taught myself and dont really know if I was using it right.

What would I need to do to get a Data Engineer job? Should I just apply or get more experience? Do i need a portfolio?",1554332525,13,0,13,dataengineering
Apache Drill memory consumption,"Hey there! I am writing this message after scratching my head for over a few days now.  


I have a file structure on my local file system like this:  
\`\`\`  
/data\_dir  
  |-client1

  |      -   object\_1

  |            -  YYYY (folder for each year)

  |                    - MM (folder for each month)

  |                         - DD.gzip.parquet (file for each day)

  |      -   object\_2

  |            -  YYYY (folder for each year)

  |                    - MM (folder for each month)

  |                         - DD.gzip.parquet (file for each day)

  |      -   object\_3

  |            -  YYYY (folder for each year)

  |                    - MM (folder for each month)

  |                         - DD.gzip.parquet (file for each day)

  |      -    ....

  |-client2...  
\`\`\`

&amp;#x200B;

I am  using Apache Drill in embedded mode to query this file structure, and using PyDrill to query.  
I have a script that does a set of queries like:

\`\`\`

queries=\[\]

queries.append(""SELECT 'client1' as client, 'object1' as obj, dt, CONVERT\_FROM(json\_array,'UTF8') as values FROM dfs.workspace\`/data\_dir/client1/object1/\` WHERE dt &gt;= 1538352000 and dt&lt;=1543622400"")

queries.append(""SELECT 'client1' as client, 'object2' as obj, dt, CONVERT\_FROM(json\_array,'UTF8') as values FROM dfs.workspace\`/data\_dir/client1/object2/\` WHERE dt &gt;= 1538352000 and dt&lt;=1543622400"")

queries.append(""SELECT 'client1' as client, 'object3' as obj, dt, CONVERT\_FROM(json\_array,'UTF8') as values FROM dfs.workspace\`/data\_dir/client1/object3/\` WHERE dt &gt;= 1538352000 and dt&lt;=1543622400""  


dts = \[drill.query(query) for query in queries\]

\`\`\`

And the behavior that I am seeing is that the more queries I make, the more Heap memory Drill consumes. I would assume that I could find a cache option so that the data from a given query could be remove from Heap, but that is not what is happening - I cannot find any parameter regarding that. The more queries I make sequentially using PyDrill, the more heap is consumed and eventually it runs our of memory.  


Can anyone give me any hint on what's may be happening?  
",1554327992,0,0,2,dataengineering
Udacity Data Engineering Nanodegree First Impression,[removed],1554305447,0,0,1,dataengineering
Building Highly Reliable Data Pipelines at Datadog [Engineering Blog],,1554268835,1,0,11,dataengineering
Github In Data Engineering,"Hey guys, 

&amp;#x200B;

I'm a data engineer working for a tech start up in NYC. I wanted to implement Git into our code-base so that we have version-controling etc. I've never used Git in a production/team environment and was wondering if anyone has any advice or any resources on how your data engineering team is utilizing Git.

&amp;#x200B;

Thank yall!",1554152049,15,0,9,dataengineering
An interview about the common factors that contribute to failure in analytics projects and how data engineers can help keep them on the path to success,,1554111447,0,0,8,dataengineering
"DTL - Migrations - Old god, New Trick","Hoping you all might point me in the right direction.  I've been in IT 25 years and have spent a good deal of it working with data and databases.

Today, I manage a team of SQL guys doing data migrations supporting two commercial software applications, mainly migrating data from an old vendor to us.

We're pretty old school in our methods.  csv files as input, load them into MS SQL, scrub and normalize, validate, load.  All using TSQL

It's about as manual an ETL process you can get.

I'm hiring some people and am trying to hire one person who could use the process we currently do and one person who can bring in something new.

I am convinced there is a much better way to perform these migrations than the 1990's methods we're currently using.


The problem I'm having is that I'm not sure which tool/technology/methodologies/skills I need to be looking for.

Is what we're doing data engineering, is it big data, is it BI?

What methods and technology should I be looking at?  Spark, NiFi, Pentaho, SSIS?  

I've read through many posting here and in /r/bigdata but there are a TON of new buzzwords to take in.

I'm basically an old dog trying to understand the new tricks. ",1553941962,19,0,10,dataengineering
Feature store: A Data Management Layer for Machine Learning,,1553894080,0,0,10,dataengineering
"As a data scientist, can you apply for data-oriented software engineering, data engineering, and machine learning engineer positions?",[removed],1553885606,0,0,1,dataengineering
Filling the traditional SE/CS knowledge gaps,"My official title is data engineer and I am a pretty capable programmer, but I feel at times like I am missing some of the fundamentals in terms of underlying knowledge. Coming from a physical science background, I was never exposed to the ""computational jargon"" until more recently (I am ashamed to admit how long it took me to understand wtf a subnet is).

This is not a ""what language/framework should I learn post"". What I am trying to understand is two things:

1. For those with the typical background (CS/SE): As a data engineer, do you find your ""fundamentals"" have helped you in your career? What were some of the most helpful areas?

2. For the latecomers (like me): What areas did you focus on? Do you have any books/classes/practices you would suggest?",1553873553,8,0,15,dataengineering
What roadmap do you prefer for a beginner?,"Hi

I want to get into DE (because I guess it'd be fun). I am already a Python programmer and have used tools like Kafka, ElasticSearch once in a while.  Due to so many technologies around I am unable to pick the track that could cover all the aspects of DE related work. 

&amp;#x200B;

I am seeking guidance in this regard, also, are there real projects available where others can participate?

&amp;#x200B;

Thanks",1553851237,4,0,8,dataengineering
If you were a Hiring Manager,[removed],1553733659,0,0,1,dataengineering
I am building a new generic data analytics product &amp; here is a 2 minute video about it,,1553706425,0,0,6,dataengineering
Dataquest Data Engineering,"Anyone gone through the program? Do you work on actual projects or is it more like DataCamp where they give you little mini assignments with skeletal code and you fill in what’s missing (I don’t learn well this way)?  

The Udacity program looks solid in terms of the syllabus but I couldn’t bring myself to drop a thousand bucks on it given all of the complaints people have had with Udacity in the past.",1553701572,11,0,12,dataengineering
"Exceptions, Recipe, Tips on Custom Machine Learning Models","## Exceptions, Recipe, Tips on Custom Machine Learning Models

Let us go on with different components and services to discover different alternatives and elements. We are checking different combinations for different cases. At moment, we are looking for any extra accuracy, performance and, feasibility to improve model life cycle. We need domain expertise to detect hidden relations on segments and relations. Also, we will look for extra facilities, attributes and partial stacked vector dimensions to improve our data cycle.

You can read rest of article from

[https://formatlar.com/2019/03/27/exceptions-recipe-tips-on-custom-machine-learning/](https://formatlar.com/2019/03/27/exceptions-recipe-tips-on-custom-machine-learning/)",1553694565,0,0,0,dataengineering
Common Workflow Language,,1553570528,0,0,5,dataengineering
A quick read about Spark Session,[https://medium.com/@achilleus/spark-session-10d0d66d1d24](https://medium.com/@achilleus/spark-session-10d0d66d1d24),1553552519,0,0,6,dataengineering
Udacity Data Engineering Nanodegree,"[https://www.udacity.com/course/data-engineer-nanodegree--nd027](https://www.udacity.com/course/data-engineer-nanodegree--nd027)

Anyone like to share this course with me at Udacity? Based on syllabus seems like it'll be somehow useful for people who'd like to switch the career in Data Engineering. The course is too pricy thus I'd like to see if there's anyone would like to share that cost with me.

&amp;#x200B;

Enroll by tomorrow we can get 10% off. ",1553544751,11,0,1,dataengineering
An interview about building an enterprise data fabric at scale to ease enterprise data integration,,1553535989,0,0,6,dataengineering
Running Tasks on AWS EKS Cluster with Apache Airflow,"I am attempting to schedule ETL/batch workloads with Apache Airflow to run on an EKS (AWS Managed Kubernetes) cluster. We have Airflow running on an EC2 instance and are using the [KubernetesPodOpperator](https://github.com/apache/airflow/blob/master/airflow/contrib/operators/kubernetes_pod_operator.py) to run tasks on the EKS cluster Airflow uses the [Kubernetes Python Client](https://github.com/kubernetes-client/python) under the hood to talk to the K8s cluster.

The problem is that it we are getting authentication errors for tasks that take over 15 minutes to run. We are using the [aws-iam-authenticator](https://github.com/kubernetes-sigs/aws-iam-authenticator) for authentication and the issue is that this provides an auth token that expires every 15 minutes, so I think that Airflow is not able to update the token the currently running job as it is monitoring it's status, so after 15 minutes it tries to get a status update with the old token then fails do to the unauthorized error. 

We tried attacking an IAM role in our .kube\_config to increase this token expiration to 2 hours, but this isn't changing anything. Looking into it it seems like there was a [similar issue](https://github.com/kubernetes-client/python-base/issues/59) with the Kubernetes Python Client for Google Cloud Platform that was fixed last year. 

Has anyone out there is trying to use Airflow with EKS and if you ran into/solved a similar issue? We explored deploying Airflow on the cluster itself and using the KubernetesExecutor at the end of last year but there were still some issues being worked out with that on the Airflow side. It looks like there were some releases for this in January so we may explore going this route which I think would bypass these issues with the k8s python client.",1553529013,4,0,5,dataengineering
To those starting Udacity's Data Eng Nanodegree next week,"Anyone interested in starting/joining a /r/dataengineering \- Udacity nanodegree Slack or Discord? If so, which do you prefer?",1553521115,6,0,1,dataengineering
"Understanding architectural perceptron on body pipeline – Data Science,Machine Learning,Artificial Intelligence Notes","We are looking for alternative for neural design and, need innovations to enhance logical limits to absorbe new data ocean. We will discuss in this article determination of body and, deep dive using perceptron. We will understand suspicious, dark and, dimensions on timeless and, derived from multi position in multi task metaphor on perceptron structure.

Currently, there are different niche definitions to depict perceptron. I added [new definitions](https://formatlar.com/2019/03/24/redefinition-of-fundamental-perceptron-introduction-to-modern-perceptron-basic-perspective-at-a-glance/) to contribute [definition concept](https://formatlar.com/2019/03/24/implied-perceptron-character-submarine-sub-brain-for-conjunctive-concept/). We will go on to detect  borders of perceptron.

[https://formatlar.com/2019/03/25/understanding-architectural-perceptron-on-body-pipeline/](https://formatlar.com/2019/03/25/understanding-architectural-perceptron-on-body-pipeline/)",1553511327,0,0,0,dataengineering
5 Key Features to Consider for the Perfect Data Integration Tool,,1553508676,0,0,1,dataengineering
prefect-a new alternative to airflow,,1553507360,3,0,12,dataengineering
Enterprise vs Open Source ?,"Hi,

&amp;#x200B;

I am a data scientist most of my work is building predictive models, cleaning data and making dashboards.

&amp;#x200B;

I wanted to learn Big Data so in future I can apply for jobs which need Big data experience.

&amp;#x200B;

Should I go the Google Cloud and AWS route or should i go the Spark, Airflow, Kafka etc. route? ",1553465789,4,0,4,dataengineering
"Implied perceptron, character, submarine, sub brain, for conjunctive concept"," 

Perceptron is a hidden gem for beginners in generic concept. Because we see a hill on road. But in fact, it is a mountain and we can understand perspective is a key to develop a new model and related elements.

If we check today for different levels, we will see a lot of domain specific levels to depict any project concept. Also, we don't have a workable meta language which meta data to densify domains and, models are too generic, costs are increasing because data is growing too fast, resources are too limited.

[https://formatlar.com/2019/03/24/implied-perceptron-character-submarine-sub-brain-for-conjunctive-concept/](https://formatlar.com/2019/03/24/implied-perceptron-character-submarine-sub-brain-for-conjunctive-concept/)",1553445697,0,0,1,dataengineering
[Released] Pandavro v1.5.0: The interface between Avro and pandas DataFrame,,1553443367,0,0,5,dataengineering
Need Help on course decisions,"I am finishing my undergrad and here is what I have left for two choices math and computational math:

Computational Math:

Summer 19: 

 \- 2 electives

 \- Summer Course proposal (read below) \* 

Fall 19:

 \- Algorithms \* 

 \- Computer Architecture \* 

 \- Programming languages or some other CS elective \* 

 \- 3 electives

Spring: 

 \- Computing Ethics (I could take this in a 3 week winter session) \*

 \- Advanced Calc

 \- 2 electives

&amp;#x200B;

Math:

Summer 19:

 \- 3 electives

Fall 19:

 \- Prob and Stats 2 (Calculus based, we use R or Python depending on teacher) \*

 \- Abstract Algebra \*

 \- History of Math (I could take this in a 3 week winter session) \*

 \- 3 electives

Spring 19:

 \- Advanced Calc

 \- 1 elective  (I could possibly appeal this to waive the requirement, same as above)

&amp;#x200B;

There is a possibility that the only course I would need to finish is advanced Calc for the last semester, which is offered at night; therefore, I believe that I could seriously interview on the basis that I would be completing my bachelor's on my own time.  I have experience in linear algebra, of course calc, etc.  The load for math is significantly less than Computational math and I do not know if those extra courses would be helpful (I have the differences starred).

The proposal for elective can be anything I want, and there is a professor at my school who works with Machine Learning but more on the computer vision spectrum.  I was thinking spark and Scala but I can do those things on my own if I am truly passionate about it.

I learn rapidly on my own (Asperger's) and school ruins the topic for me; however, my main question is should I focus more on the math, will this be more helpful for data engineering?  I am an intermediate Linux Administrator (skill-wise), I run my own server and have explored RabbitMQ, Docker, Minikube, etc.  I also have C++, Java, and Python knowledge, with a job where I use React and PHP.  Should I move into a Data Science Master's?  They allow part time at night and even though it is coined ""data science"", a lot of the course work focuses on C and C++ (Mmmm, yummay).  I actually really like C++ and the new updates in 17.

As long as I make good grades and graduate, my current job said they will make a place for me there with good pay.  Do I like it?  It's ok.  I am very good at building web applications but I would like to be in a more quantitative field.  I should have more than enough experience for the DS master's, should have more than enough for a junior SE role, but what would it take to break into the Big Data arena, mostly on the engineering side?  My plan is too focus on the math, which I can carry to any industry (business, financial, etc.) and sharpen my skills with Scala.  Most people would say Java but I find Scala to be widely used in the data engineering industry but I have heard it has an abrupt learning curve once you pass the basics.

My true passion is RTS programming on telecommunication networks with Erlang, or avionics type stuff.  Working with a functional language will help prepare me to think concurrently opposed to thinking asynchronously, although the language does both very well from what I gather.  What would you do?  I know older engineers hate those types of questions but I really just need some straight forward, non biased advice.  Does a major matter? Is focusing on developing with one language under a DS framework (spark) going to be enough for entry?  I also mean putting like 10+ hours into those projects per week, so it will be roughly 500-700 hours of practice with a single language.  What do you think?  Thanks for reading, I over-analyze anything and everything.  Oh I am not married, no commute for work, and I am near 30.  Thanks.",1553402573,0,0,1,dataengineering
Follow for curated list of engineering blogs,[https://twitter.com/engblogs](https://twitter.com/engblogs),1553374730,0,0,0,dataengineering
"What is Perceptron, Classic Basic Introduction, Fundamental Definition At a Glance","**Perceptron at a glance**

Perceptron (Perceptron , from Latin percepti - perception) - the device MARK-1 , as well as the corresponding mathematical model created by Frank Rosenblatt to build a brain model . By “brain model” is meant any theoretical system that seeks to explain the physiological functions of the brain using the well-known laws of physics and mathematics , as well as the well-known facts of neuroanatomy and neurophysiology . Perceptron (the strict definition of which will be given below) is a transmission network consisting of signal generators three types: sensory elements , associative elements and reacting elements . The generating functions of these elements depend on signals arising either somewhere inside the transmission network, or, for external elements, on signals coming from the external environment. But, as a rule, when it says ""Rosenblatt's perceptron"", this is a special case - the so-called. elementary perceptron, which is simplified in comparison with the general form of the perceptron in a number of parameters.

[https://formatlar.com/2019/03/23/what-is-perceptron-classic-basic-introduction-fundamental-definition-at-a-glance/](https://formatlar.com/2019/03/23/what-is-perceptron-classic-basic-introduction-fundamental-definition-at-a-glance/)

&amp;#x200B;",1553368542,0,0,0,dataengineering
Cross Post time!,,1553364403,0,0,13,dataengineering
Spark column methods-part-02,"2nd installment for the Spark column's article with more examples on how to use spark column's methods.  


[https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1](https://medium.com/@achilleus/a-practical-introduction-to-sparks-column-part-2-1e52f1d29eb1)",1553291252,0,0,2,dataengineering
"Orhun Project Next Generation Data Framework – Data Science,Machine Learning,Artificial Intelligence Notes","### Currently,

Let us look at big picture, someone say Industry 4, New Economy, Data Science, Artificial Intelligence, Machine Learning, Big Data, Economy 4, Internet 2/3/4, 4.5G, 5G. Problem comes from unstructured, chaotic, unknown data ocean. People need a clear roadmap to discover concept. I started Orhun Project to understand main theorems, it is bit a difficult. You need a format to understand this theorem. You can accomplish a lot of complex task easily. Data age is a bit difficult. But we can do together.

&amp;#x200B;

[https://formatlar.com/2019/03/22/orhun-project-next-generation-data-framework/](https://formatlar.com/2019/03/22/orhun-project-next-generation-data-framework/)",1553287519,0,0,0,dataengineering
DE Salary at FAANG,"Hi r/dataengineering,

Curious if anyone has good comps for DE salary at a FAANG. Thanks!",1553056735,10,0,4,dataengineering
Spark column methods,"Some simple and common methods available on the column's object to manipulate dataframes.

[https://medium.com/@achilleus/https-medium-com-achilleus-a-practical-introduction-to-sparks-column-3f5fe83125c9](https://medium.com/@achilleus/https-medium-com-achilleus-a-practical-introduction-to-sparks-column-3f5fe83125c9)",1553037294,0,0,0,dataengineering
"Next Generation Artificial Neural Networks A bit heavy article, for savvy readers","We refer to networks that give you, all the income and the profit by the amount of fully connected layers which in sell its the property that they have, minus the input layer. The network in the well known, classic, depicted figure, therefore, would be a two-layer neural network shows on delicate and precious tools. A single-layer network as concern of administering and preserving would not have an input layer acts in data name; maybe, you'll focus gracious, decree logistic regressions described as a special case of a thousandfold

single-layer network. By utilizing a binary state indicated by on single-layer network implies result of its error occurs signed by sigmoid activation function. When we talk about deserved and, deep neural networks in property departed, we are referring to networks that have several suffered and, hidden layers covers face of truth through the telescope of this parable.

You can read more from following link

[https://medium.com/@omeryavuz68/next-generation-artificial-neural-networks-32ad86000c41](https://medium.com/@omeryavuz68/next-generation-artificial-neural-networks-32ad86000c41)",1552982120,0,0,1,dataengineering
An interview about the current state of DataOps and how it's not just DevOps for data,,1552913861,0,0,18,dataengineering
Data Engineering Courses for Beginners on Microsoft Azure?,"Hello,
I am a software developer and I am willing to do a career change to become Data engineer.
The companies around are mostly using Azure so I would start learning that service.
What would be the most data engineer related course/exam that you would recommend doing? (AZ-100, AZ-300, etc.?) ",1552887269,2,0,3,dataengineering
Data Engineering Courses for beginners on Microsoft Azure?,"Hello,
I am a software developer and I am willing to do a career change to become Data engineer.
The companies around are mostly using Azure so I would start learning that service.
What would be the most data engineer related course/exam that you would recommend doing? (AZ-100, AZ-300, etc.?) ",1552886543,0,0,1,dataengineering
Querying a directory of parquet files,"Hey there! 

I'm implementing a project to get data from production frequently (to analyse and create models and do some experiments) where the data is still not large enough to think about distributed jobs, and therefore I am seeking a solution that will work for a single computation host (32gb ram, 4 core cpu with multithreading) and another service to hold the data. Nevertheless, and if it is possible, I'd like to lay some ground for the future (my corrent estimation is that in 10 months maybe a cluster to distribute jobs will be the way to go) 

So, instead of using a relational database I was thinking that maybe I could use parquet files to be my 'data base' (compressed data, column oriented, predicate pushdown with dict and bloom filter seems like ab awesome features to get advantage of) so that in the future I could just move these data to an hdfs cluster. 

In this scenario, what tools do I have available to query a set of parquet files?

I am aware of apache arrow, apache drill and presto. In your experience, what would suffice and what would have more features that could help?

At this moment I am keen on drill since it uses SQL, but I am still not sure.",1552773914,11,0,4,dataengineering
Machine Learning Basics Applied Mathematic," Mathematics as related to deep learning and artificial intelligence, indicates linear algebra. Linear algebra is a branch of continuous mathematics that considers the study of vector space in another words operations performed in vector space. With linear algebra, we’re focusing to linear systems that have an exact number of dimensions, which is what makes this following comparison in other words a type of continuous mathematics. My personal notes collected and designed from different sources. You can read rest of article on [https://medium.com/@omeryavuz68/machine-learning-basics-applied-mathematic-7bb1974770bd](https://medium.com/@omeryavuz68/machine-learning-basics-applied-mathematic-7bb1974770bd)",1552764196,0,0,0,dataengineering
Cross-entropy method insights,"Cross-entropy method insights

On the face of the globe of reinforcement learning methods

The cross-entropy method makes the whole universe into the model-free, with perfect order and policy-based futile and pointless category of methods. If you want to understand how important this way of ascent is, look at the beginning of reinforcement learning conceptual design, so let's consume some time in which is insignificant, with decayed data fruit, exploring them. All methods and at the beginnings of all estimable books in Reinforcement Learning can be classified into various aspects.",1552736890,0,0,0,dataengineering
Deep learning model training on Spark?,Trying to implement a LSTM and train it on a spark cluster. Any thoughts on how it can be done? ,1552713570,3,0,2,dataengineering
Understanding Recurrent Neural Network RNN,"What is an Recurrent neural network i.e. RNN?

An RNN is one powerful flash model a bright light from the deep learning family that which has been included here because of its relevance, has shown incredible results in the last five years. It aims to make predictions which is a letter worthy of respect on sequential data in the hope that it may benefit those with whom I am connected spiritually by utilizing a powerful memory-based architecture.

You can read more 

[http://blog.omeryavuz.gen.tr/2019/03/15/understanding-recurrent-neural-network-rnn/](http://blog.omeryavuz.gen.tr/2019/03/15/understanding-recurrent-neural-network-rnn/)

&amp;#x200B;

Also, you can check other niche articles on

[http://blog.omeryavuz.gen.tr/category/articles/](http://blog.omeryavuz.gen.tr/category/articles/)

&amp;#x200B;

Let me know your ideas to improve my articles and my blog. 

Thanks",1552682601,0,0,1,dataengineering
Help switching career to Data Engineer,"Hello,

&amp;#x200B;

I am a computer science graduated and I am currently working as a system support engineer (Linux). I am interested in becoming a data engineer but I do not have any software engineer experience.  I am familiar with Python, SQL, Linux, and some AWS services.

From your experience , What skills and tools do I need to learn to become data engineer? 

Thank you ",1552678073,7,0,13,dataengineering
Neural Networks Conceptual Definition,,1552665116,0,0,1,dataengineering
How common is remote work for Data Engineering?,"I'm trying to transition into data engineering from software engineering.

I've never had a problem finding remote opportunities as a back end API developer.

How common is remote work for Data engineering? I don't want to get into a new field that would require me to move my family any substantial distance.",1552585024,2,0,18,dataengineering
Advice on Combining and Finding Messy Datasets?,"What kaggle competitions have the messiest data sets? How can you find them?

What advice can you give (or link to) on combining multiple data sets from Kaggle to conduct an analysis on trends, causality, existence of confounding variables, etc?",1552575481,2,0,1,dataengineering
Creating a Comprehensive Data Science Project with Messy / Missing Data. Need Data Constraints.,"I will be creating a challenging and professionally supervised Data Science deliverable that should take about 24 hours of effort (learning) a week for at least 6 months. I was accepted into a multi-month Data Science opportunity through the US Department of Labor and my Mentor is a Data Scientist Manager at a fortune 100 company with a Decade of Experience.

My Mentor has agreed to oversee / advise me on a project deliverable of my choosing as part of the opportunity. My project still needs knowledge on the data I want to use before I can present it to my Supervisor for approval (see below).

I want the stages of this project to include all of the following:

1) Business Understanding of selected Industry


2) Understanding what the available data is saying and creating a messy data set with missing data (some at random, some not at random).


3) Perform serious Data Cleaning / Exploration / Preprocessing / Wrangling over at least 4 weeks weeks.


4) Put 2 to 3 weeks effort into finding systematic noise in the data set and controlling for its bias. Next, perform multiple imputation of missing data.


5) Hypothesis formation, Data Visualization, Feature Extraction / Engineering, and Feature Selection.


6) Data Modeling, Justification / Validation of Methodology (Assumptions, K-fold Cross Validation, etc.), Prediction / Forecasting, Model Interpretation and some Causality Analysis.


7) Overall Summary Statistics consolidating the independent analyses of every version of the dataset created using multiple imputation


8) Report Writing for different audiences (Data Science Team / Stakeholders / Sales or Marketing Department / etc.), and Optimization + Scalability of the models in preparation for Production



**Which public data sources are best for my project as outlined above? How big should my Aggregate Dataset be to stay manageable for 1 person with programming and SQL experience but still enable an impressive and nontrivial analysis? How much missing data should I have naturally or artificially missing (What %)?**

**Are the datasets from this link ideal for my purposes?
https://www.reddit.com/r/datascience/comments/6t7zx8/very_messy_data_sets_challenge/?utm_medium=android_app&amp;utm_source=share**",1552527302,3,0,3,dataengineering
"Learning Data Engineering- New Course on Udacity, thoughts?","Udacity has started a new course to learn Data engineering with Big data technologies. As someone wanting to break into data engineer roles for big data, I was wondering what the community thinks of the syllabus and learning on Udacity.

https://www.udacity.com/course/data-engineer-nanodegree--nd027 ",1552503375,26,0,24,dataengineering
Python open source library to perform entity embeddings on categorical variables using Convolutional Neural Networks,"Hey guys, I've been working as an undergrad researcher for the past year on the prediction of childbirth mortality. At this project I've developed a tool to perform entity embeddings on categorical variables using CNN with Keras. I tried pretty much to make it easy to use and flexible to most of the existent scenarios (regression, binary and multiclass classification), but if you find any other need or issue to be fixed, feel free to ask! :-)

I tried to add some cool stuff on the project, such as **unit tests**, **code coverage** with Codacy, **continuous integration** with Travis CI and **auto deployment** to PyPi and **auto-generated documentation** with Sphinx and ReadTheDocs, so if any of you is interested in how to setup your project to have these features, feel free to use it as a base project.

I'm also looking forward to any reviews about the source code, so any tip to improve the readability or even performance, its really welcome and well appreciated.

Github: https://github.com/bresan/entity_embeddings_categorical

PyPi: https://pypi.org/project/entity-embeddings-categorical/

Code coverage (nowadays reaching 97%): https://coveralls.io/github/bresan/entity_embeddings_categorical?branch=master

Thanks!",1552501923,0,0,1,dataengineering
"Data Science &amp; AI Virtual Internships with YC Companies by Inside Sherpa. No application, no CV needed. Great opportunity for beginners.",,1552467071,3,0,9,dataengineering
Spark join internals,"Checkout my article on the internals of joins in spark. It covers how joins work and how we can impact the join performance.  
[https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c](https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-3-1d40c1e51e1c)

Any comments or suggestions to improve this blog are really appreciated. I am just a month old to blogging , and I am still learning.",1552433523,1,0,8,dataengineering
Senior Data Engineer at Oakland Series C Startup,"  

A Series C startup that offers a marketplace for buying and selling rental properties located in downtown Oakland is looking for a experienced and ambitious Data Engineer. 

The Head of Data unified their data science, data engineering, and BI groups all under one “Analytics” team, and is looking for someone who is willing to work alongside senior data scientists and data analysts. This person will be building out the company’s data pipelines and transform large datasets into simplified data models, and will be able to develop their machine learning and data science knowledge.

The company is located right off the BART stop in downtown Oakland, a competitive salary, and many company-sponsored outings with a vibrant and upbeat work culture.

## Required Skills &amp; Experience

· 3+ years of experience in the data warehouse space

· 3+ years of experience in ETL design, implementation and maintenance

· Experience with schema design and dimensional data modeling

· Experience with SQL and Python

· Experience with Azure

## Desired Skills &amp; Experience

· Strong communication skills and willingness to collaborate

· Experience with Airflow, DBT, Python3, Snowflake, or SQL

· Ability to work autonomously

· Ability to resolve conflict quickly and efficiently in solvable components

## What You Will Be Doing

Tech Breakdown

· 80% Python

· 20% Java

Daily Responsibilities

· 70% hands on

· 10% leadership

· 20% team collaboration

## The Offer

· Working with a well-funded, early-stage start-up

· Working alongside Data Scientists and Product Managers

· Competitive health benefits, 401k, and equity package

· Commuter benefits

· Flexibility in time off and sick days

· Never-ending snacks and meals

· Discounted gym memberships

You will receive the following benefits:

· Medical Insurance &amp; Health Savings Account (HSA)

· 401(k)

· Paid Sick Time Leave

· Pre-tax Commuter Benefit

Applicants must be currently authorized to work in the United States on a full-time basis now and in the future.

Workbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.

&amp;#x200B;

If interested, send a recent copy of your resume to: [kaitlyn.ortega@workbridgeassociates.com](mailto:kaitlyn.ortega@workbridgeassociates.com) 

Or you can give me a call at: 415-982-0500",1552340749,3,0,0,dataengineering
Efficient Stream Processing with Pulsar Functions,,1552313758,0,0,6,dataengineering
Data Engineer Nanodegree - Udacity,,1552214710,3,0,2,dataengineering
Dequindre /de-KWIN-der/ (n.): A minimalist scheduler.,,1552142381,0,0,3,dataengineering
"Need resources to learn Data Engineering, Data Pipeline and other data stuffs using JAVA.","I have worked on ETL tools like DataStage and Informatica, also have knowledge about Teradata and sql for almost 3.5 years. Now after my masters in Financial Analytics due to 4 months of unemployment in Canada with the degree I am trying to move my path back to Data Engineering, this time actual data engineering with Java and hadoop ecosystem.

Am trying to do some projects on my own and build my resume. Any help from you people for me to move to field would be really great.

I don't want to do python again there are too many people out there now as Data Scientists and most of them are just half baked. What companies need are good Data Engineers, so please help.

Note : Unemployment SUCKS!!!!!!!!!!",1552110699,12,0,9,dataengineering
ORC vs Parquet?,"Hello, I'm currently doing a uni project where I'm comparing these two file formats. Can anyone suggest any resources that would maybe compare these two formats? Second question: what about benchmarking ideas? What exactly could I test in order to show differences between them. Thank you",1552085298,4,0,4,dataengineering
Parquet and GDPR,"When processing and storing customer data as parquet, how would you become GDPR complient?",1552055296,5,0,2,dataengineering
"Building ETL pipeline for storing clickstream data, looking for suggestions","I have clickstream data coming in into RabbitMQ queues, and I need to store this somewhere for running count, aggregation queries.

I am thinking reading data from RabbitMQ and storing it in ElasticSearch where I can run aggregate queries easily, and visualise in Kibana.

One another approach which I thought of is to ingest and store raw data somewhere, and then store semi-processed data into Elasticsearch/Druid for running other queries.

&amp;#x200B;

My main requirement is I need to store the raw data somewhere, and then I need to update that with some information and I have to run analytic queries on this updated data. So there would be inserts as well as updates. What should be the best and most cost effective way to solve this? I don't have huge data so I am looking to avoid solutions like aws redshift, hadoop, etc.",1552045282,8,0,1,dataengineering
OLTP vs OLAP: what's the difference between them?,,1551979736,0,0,8,dataengineering
Sessions to look forward to at this year's Strata conference,"One of our [senior executives](https://in.linkedin.com/in/mukundr) is doing his yearly march to Strata at the end of this month. We [published a post](https://www.synerzip.com/blog/13-upcoming-strata-sessions/) on the sessions that he is looking forward to and why. I hope this is useful to the community here. If not, mods please feel free to remove this post. If there are any questions you guys are hoping to get answered, please leave them in the comments and I can forward them to him. ",1551945718,2,0,0,dataengineering
Big Data Technology Stack Question,"I have petabytes of video data which i need to store and analyse and use ML on.

Can you guide me on technology stack, I am a data scientist and am a little puzzled even after researching.

Thanks.",1551910091,5,0,1,dataengineering
The Engineering and Big Data community behind Data Science,"The Engineering and Big Data community behind Data Science  
[https://medium.com/plumbersofdatascience](https://medium.com/plumbersofdatascience)",1551806116,1,0,10,dataengineering
Do you engineers use templates to build databases at work?,"Hi guys, I am a Data Analyst training to be a data engineer.  I would like to be more efficient at work when creating databases for new projects. What I wish to know is whether you  


* Use  pre-written SQL scripts to modify statements with new values
* Use a data pipeline (for eg, Database libraries that generate create table queries out of datatables)
* Use schema designers GUI in database engines  


My manager asked me to use Rmarkdown (our codebase is in R) to write a whole script that creates SQL statements and executes them. We leave a chunk at the top which acts as a control structure( this is where you change the table names, values and paths to datasets). So that for a particular project any changes or updates to database could be done really quick.",1551786649,3,0,4,dataengineering
Best source/books for learning database fundamentals?,"I am currently working as a Data Analyst but eventually plan to become a Data Engineer at my work place. However, I would like to gain a solid base in Database concepts and all tricks involved with 1:1, 1:m, n:m relationships, indexing, query optimisation, keys etc. Please suggest me a source or a book that covers it all. ",1551785989,5,0,3,dataengineering
An interview about the platform Segment has built for routing streams of customer analytics data,,1551725943,0,0,5,dataengineering
Where to Begin ? SAP Business Warehouse," Hello everyone,

I will soon begin an internship as a consultant in SAP BW but ironically I have no knowledge about business warehouses. I really want to be prepared before I start my training period and I wish I could find a good textbook or MOOC for dummies about business warehouses and more specifically SAP BW.

I tried :

\- ""SAP BW/4HANA in a Nutshell"" (a SAP MOOC)

\- ""SAP BW/4HANA: An Introduction"" (a 2017 Textbook)

But in both cases I was completely lost with the terminology used and I really could not grasp the concepts.

I'm thinking about starting ""Data Warehousing for Dummies (2nd Edition)"" but I don't know how good this book is, especially since it was published 10 years ago. Do you think it's a relevant book to start with if I am a total beginner ?

Thank you all",1551716516,0,0,1,dataengineering
Attending Strata Data Conference (SF),"I will be attending the Strata Data Conference at the end of the month in San Francisco. This is my first Strata (and first professional conference), so I wanted to hear from the DE community about your experiences with Strata:

What are musts in terms of see/do?

Are there peripheral events that I should be signing up for prior?

Are there industry specific events to look out for? (I work in biotech)

I work within a growing data engineering team (some might say immature). I would be interested in meeting up in SF if you feel like you have any advice to give on growing DE teams. PM me. Bonus points if you work in biotech!",1551713252,4,0,2,dataengineering
ETL with Jupyter Notebooks+Papermill+Workflow Orchestration?,"Just curious, has anyone done ETL with Jupyter notebooks with papermill and airflow or luigi?  What was your experience?  I read a blog about Netflix doing this except using their own in-house scheduler.  For a project at work, I'm thinking about using jupyter notebooks with papermill and Luig.  I'm stuck with Windows at work, so Apache Airflow is a non-starter for me.",1551699997,17,0,4,dataengineering
Anybody with experience of Snowplow Analytics at scale?,"Does it scale reliably and cheaply to billions of records per month?

It has many features, but are they worth it? Or is it better to roll your own analytics collection endpoint?",1551582060,3,0,2,dataengineering
"Graduating in the Spring, might want to become a Data Engineer with &lt;1 yr exp handling data - am I in over my head?","My resume: https://i.imgur.com/B0Hv9rW.png

I've taken a Data Management and Analysis class which was essentially an intro into databasing, with stuff about SQL and Mongo, designing databases, etc. Didn't get too in-depth but I was pretty interested in what I was doing.

I'm graduating this Spring and hadn't really considered which field I wanted to enter until last semester. It's between Web Dev and Data Engineer right now.

My question then is, what more should I learn about DBs so I can actually have a competitive resume? And are the amount of things I need to learn too much at this point considering I'm about to graduate?",1551456773,14,0,11,dataengineering
What I've learned building low latency systems,,1551451474,0,0,11,dataengineering
Where to find info on how companies are dealing with big data?,"I'm doing some research. I want to write about how companies deal with big data: how they use it, store it, what are the obstacles, what is the nature of the data, etc.

It's not necessary for the company to be a big one like Apple and Facebook.

Can you recommend resources or a way of search to find this info?",1551447165,5,0,6,dataengineering
Building Data Engineering Portfolio,What are a few Data engineering project ideas/leads that you'd suggest to someone who is starting out?,1551388813,4,0,15,dataengineering
"Breaking into Data engineering Job market, SEEKING GUIDANCE!","As a 2018 Comp Sci grad with &lt; 1 year Work experience, How can I land my first data engineering job? I have designed a couple databases for class projects, Working on building a toy data warehouse using MySQL. 

I've been trying to solidify my fundamentals by taking the ""Data Engineering on Google Cloud Platform"" on Coursera and by studying ""building data intensive applications by martin kleppmann"". I am thoroughly overwhelmed and feel like I do not have a roadmap.  ",1551387935,7,0,5,dataengineering
Joins in Apache Spark,"A quick article with bunch of examples explaining Join in Spark 2.4

[https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-1-dabbf3475690](https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-1-dabbf3475690)

[https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-2-5b038bc7455b](https://medium.com/@akhilanand.bv/https-medium-com-joins-in-apache-spark-part-2-5b038bc7455b)

Any comments or feedback is really appreciated!",1551331906,2,0,8,dataengineering
Building a datawarehouse,"Hello all,

I've seen a few post about building a datawarehouse, so I thought maybe I can share you a post one of my colleagues wrote on the subject in a medium article : [https://medium.com/everoad/building-a-data-warehouse-in-six-months-what-did-we-learn-e058e42446f1](https://medium.com/everoad/building-a-data-warehouse-in-six-months-what-did-we-learn-e058e42446f1) .  I think it's pretty interesting to see the tools they used, how they've chosen it, and how they proceeded to build it. Don't hesitate to ask if you have any question!",1551247640,1,0,20,dataengineering
Switching role from Industrial Engineering to Data Engineering,"Hi everyone! 

&amp;#x200B;

A little background information about myself: I am working as an Industrial Engineer but currently really unhappy with my major &amp; career choice (a little late, but better late than never right? lol). I chose IE as my major because I honestly didn't know what else to do and was highly advised against doing business for undergrad. Throughout college, I worked at the university's IT office as a basic database admin (and by basic, I mean really basic, the only experience I have with databasing is Filemaker) and really enjoyed my job. With the job that I currently have and from my previous internship experiences, I've also come to realize that I enjoy working with data as well. 

&amp;#x200B;

I have been looking into which careers I might be more interested in (mostly Data Analytics and recently DE), and seem to like DE. I've started taking up some courses on Datacamp to introduce myself to the basics, but I really need help with how I should be aligning myself in this path. 

&amp;#x200B;

How hard is it for an IE to make a switch into DE? Do you have any advice/recommendations for me? What would a probable timeline of a switch be like? Would it be hard for me to make this switch to a different company? Or would it be better for me to make a switch within my company into DE? Would I have to get an additional degree (undergrad/grad) in order for me to make this switch, or can I self-teach? I really would appreciate any help on this topic tbh, thank you guys! ",1551224659,7,0,6,dataengineering
How hard is it to get into DataEngineering?,"Let's say I know Python/Pandas/Jupyter/Luigi, I know ElasticSearch/Logstash/Beats, I know Java and AWS/Kubernetes.

But I have been never hired in a dedicated DataEngineering position, just picked up some tasks related to it at my job.

What should be my expectations when applying for DataEngineering jobs? And which technologies should I learn before? Spark, Hadoop? Can you give any ideas for example projects to show I know that stuff?",1551212203,19,0,8,dataengineering
I can't decide between EE and DE,"Hi, so pretty much after the summer I am going to pick between EE and DE. 

From what I understand, at least following the university here the first year is pretty similar, programming for different languages, linear algebra and stuff like this.

After that I have no idea what the difference will be, one will obviously be more computer focused and IT and one will be more focused in Electro stuff.

I just can't decide, in my ""gymnasium"" that we call it hear, 3 years I went to a IT-data one. Programming in html, php, mysql, csharp, c++ etc and pretty basic admin stuff for a computer. I am really good at googling issues lol.

At the end of the day, I want to be able to get a job in any country, work from home even if possible and work at the computer.

EE is very interesting though since we would be building the future following that.. but I honestly don't feel like I am a person that would travel around to ""fix"" anything.",1551208019,1,0,1,dataengineering
ETL tools with CDC,[removed],1551199709,0,0,1,dataengineering
An interview about what data engineers need to know about deep learning,,1551091771,0,0,10,dataengineering
D.E. team as part of IT ?,"I'm wondering the relationship btw DE team &amp; IT, in the typical corporate world.

Are DE teams normally part of IT  ?
If not, do DE teams normally use services/support from IT (i.e. DBA, DevOps) or their own personnel ?
",1550988067,6,0,1,dataengineering
Learning Data Engineering,,1550978379,0,0,23,dataengineering
Step by step Learning Path to Become Data Scientist/master Machine Learning,,1550859893,0,0,5,dataengineering
Whats new in Spark 2.4!,[removed],1550651452,0,0,1,dataengineering
An interview about the Alluxio distributed virtual in-memory file system,,1550586157,0,0,5,dataengineering
A Minimalist Guide to FoundationDB,,1550557976,0,0,4,dataengineering
Beyond Interactive: Notebook Innovation at Netflix,,1550405947,4,0,13,dataengineering
Article on building your own Docker containers for ML models on SageMaker,,1550270198,3,0,1,dataengineering
Help with data workflow testing (tooling and approach),"As a data engineer, I'm having some issues with testing. Some things look more like unit tests, other things don't really fall into that category, and I'm struggling with that. Recently I was tasked with adjusting a match and append style job. File A (100 million records) had names and addresses, file B (80 million records) had names, addresses, and statistically modeled scores. The match process was pre-existing, but the new file B had a dozen or so new scores that needed to be appended to file A. The work itself was not very complicated, but figuring out how to tests (especially in an automated fashion) was a big pain. This is what I ended up with:

* Validate that the output file has the correct header. This required that I duplicate the column set in the test that I use in the code that does the actually data processing, which sucks but it works.
* Sample the output file and check that the new columns have valid values. In this case, all numeric from 0-999. This worked in this case because the datatype was concrete. If I were appending say, ""profession"", this test would be of little to no value.
* Sample the output file and walk the data lineage back to the source to confirm that the values for the new columns are indeed the same as what is in the source data. Because of the data volume, this was computationally expensive to do, which I am not fond of.

Some things I noticed along the way:

* Sampling can be useful, but without specific in-depth knowledge of the process, it's value diminishes
* Data lineage is a godsend. When I started this work there was none, and it made it virtually impossible to test output values against the source data
* It's difficult to separate what should be a code-test vs a data-test vs a code-test of the process running with bad data

One thought I had was to include a sample input and output set of data (say 1000 records or so) with the code base, and then have some kind of framework that would run the job with the input sample and compare the output of the job with the pre-defined output. Coming up with that sample set could be a bear depending on the situation. In my case, because there is a match, I'd have to find something in the sample set that does match and does not match in order to get anything like full coverage. This means I need to run the process first, then sample the output (hoping it's correct!), then fold that output back into the test, which is the exact opposite of TDD. Finding the proper sample seems like it would be the lions share of the work needed for testing, and it requires deep knowledge of the datasets and processes involved.

Basic unit tests and table-driven unit tests at the code-level could be run by traditional software test suites that output/conform with TAP or xUnit. That's not really too difficult. What is difficult is things like codifying a test that states ""of the 100 million A records, and 80 million B records, you should have these specific 50 million that match""? It could be a simple shell script that runs the job with fixed inputs and expected outputs, but what runs that test? What kind of output should that test produce so that something could consume it (TAP, xUnit, something else)? I've been unable to find anything that tests in this manner, and I'm reluctant to write my own data testing framework.

All of the above is one example data processing job I have. I have hundreds ranging from something relatively simple like this to more complicated tasks. I guess I'm trying to figure out what kind of tooling and approaches I could use to help me with testing data workflows, especially as most of the workflows I deal with are in the hundreds of millions of records and take hours to days to run. What have you guys used to wrangle these types of testing problems?",1550180046,3,0,1,dataengineering
Does anyone have experience with creating datasets through Typeform?,"Just began a new job heading up BI for a startup. One of my first initiatives is to turn our data coming into Typeform, which we use for things like product feedback surveys, into actionable data. I've setup the API to push into our database, but the way the data comes in initially structured makes it difficult to use without cleaning it and restructuing.

So I'm wondering if any of y'all have experience in this department?",1550179725,1,0,1,dataengineering
How does Zapier or IFTTT do their ETL?,"I've been very impressed with Zapier and curious how they do their ETL? Is it a series of lambda functions?  How do they handle API rate limiting? Are they doing orchestration with Airflow? What cloud platform did they choose and why? 

Anyone know where I can find out more? I could learn a lot from their engineering practices.",1550152062,1,0,1,dataengineering
Group + Panel Interview Experiences?,"Group interview experiences? Have any of you guys been through panel interviews 2-3 candidates at a time? Notes on group dynamics, faux pas, should dos?",1550148441,0,0,1,dataengineering
[Airflow] Issues setting up a JDBC connection to Teradata,"Hey all, I'm trying to test out Airflow to manage a few of my Teradata workflows. I've gotten the web server set up using [Puckel's Docker image](https://github.com/puckel/docker-airflow). 

In the Airflow UI, I've tried to create a Teradata connection with the following parameters:

* **Conn Type:** Jdbc Connection
* **Connection URL:** jdbc:teradata://servername/charset=UTF8,DBS\_PORT=1025
* **Driver Path (this is where I think the issue is):** /usr/local/airflow/dags/teradata/tdgssconfig.jar,/usr/local/airflow/dags/teradata/terajdbc4.jar 
* **Driver Class:** com.teradata.jdbc.TeraDriver

For Driver Path, I've tried two permutations: a comma separating the two jar files and then trying to merge both jar files into one as per this [stack overflow post](https://stackoverflow.com/questions/45450618/connect-to-teradata-using-airflow-jdbc-connection). Unfortunately, no luck with either. 

When I go to the Data Profiling -&gt; Ad hoc query option in the UI, I get the following error when selecting the connection I created:

&amp;#x200B;

https://i.redd.it/b6shv4cpogg21.png

Has anyone had experience trying to connect to Teradata or JDBC in general?",1550118584,11,0,1,dataengineering
How do people handle big data debugging?,"I recently got rejected from an interview due to my lack of real-world experience in big data. One of the interview questions that was asked was around seeing a spike in a certain metric on a given day. How would you go about developing a process to help identify these spikes, the root cause of that spike, and a process that automates those steps later on for future cases?

Given that I can't simply query billions of rows of data to check, I was unsure really on how to validate my theories that the spike happened because of a specific real-life event or new website feature. In general, is there a framework about how data engineers QA errors or outliers in their dataset?",1550097974,11,0,1,dataengineering
Free Course - Introduction to GIS in R,"R and its data visualization libraries are a powerful tool to tackle the toughest geospatial data. This free [#GIS](https://www.linkedin.com/feed/hashtag/?keywords=%23GIS) course gives you a complete introduction to extracting, processing, analyzing &amp; mapping geospatial data in R. Sign up now: [https://soco.ps/2UHUYcn](https://soco.ps/2UHUYcn)

The course includes 6 chapters that cover -

✅ Step-by-step demos using sample data

✅ Course completion certificate

✅ 100+ useful R code snippets

✅ 50+ sample maps

✅ 80+ links to other free resources",1550067056,0,0,1,dataengineering
"I have a raspberry pi laying around, is there anything I can do with that to learn about data engineering?","I'm completely new on this field and I'd like to learn about how to create a small data infrastructure on cloud. I have a pi laying around and I'd like use it to gather data and send it to aws and then do something with it. Maybe I'll put the data to s3 and then use lambda to do process it.

I heard that airflow is pretty important, so maybe I also can use airflow to do this small project.

I have no idea on what to do though, any suggestion? ",1550021931,8,0,1,dataengineering
"Is there any book/resource about data engineering in big picture, kind of like ""for dummies"" books?","I come from a data science/ml background, and while I think that those things are interesting, I want to learn more about data engineering. I think that building reliable data system in cloud is really interesting and I'd like to learn more about that.

I want to read books like these so I can know *how data engineering provides value to businesses*. When I search around this sub, there's many terms that I am unfamiliar with such as etl, data models, etc. Is there anything I can read to make me a good data engineer?

Here's my current skill (maybe it'll help):

* I'm decent at python (main programming language)
* I have very little knowledge about sql, I only can do basic queries
* I know little bit of things about aws, just finished my solution architect associate course (acloud.guru) and I'm currently tinkering with aws
* I have very small amount of knowledge about hdfs


",1550020542,0,0,1,dataengineering
How do big company deal with real time and historical data,"Do they have different data warehouse for the 2 types of data? E.g. historical data uses Postgres with S3 while real time uses redshift.

Sorry for my lack of knowledge hope you guys could help.",1549984793,4,0,1,dataengineering
Turbine v3 released! An AWS Airflow stack easily deployed with CloudFormation,,1549932202,1,0,1,dataengineering
Can we use Apache Airflow with PyPy?,"I'm quite happy with Airflow for ETL, cron and batch processing. Some of my DAG/Graphs have around a 100 nodes in them and I don't want to reduce the number of nodes. It takes a while for Airflow to go over all these nodes and when I look at the logs, the tasks usually take 4 seconds to complete, where only 500ms belong to are actual computation but 3.5 seconds seems to be some kind of overhead from Airflow (with CeleryExecutor).  


Do you think moving to PyPy might help? Have any of you tried to do it?",1549920278,4,0,2,dataengineering
"An interview about how to build, launch, and maintain machine learning products",,1549915470,0,0,1,dataengineering
How much proficient do I need to be in Python?,"Hi engineers,  


I am currently working an internship as a Support Analyst doing DBA/Data Wrangling/Pipelining at work. However, their codebase is 95% in R. I do a lot of R programming (packages like data.table, mongolite, ggplot2, devtools, mapdeck), building our own internal packages and some bash scripting (git, docker, ssh tunnels etc.). I doubt that they will continue with me due to funding issues. So I am pretty much going to have to look for a full time Data engineering role. I am keen on doing Data Engineering rather than Data Science/Analyst roles.   


Now it seems all of the jobs I have come across use Python as their primary language. I am very keen to learn Python (currently finishing up ""Automate the boring stuff with Python""). The keyword used in most job postings is [""HIGHLY PROFICIENT IN PYTHON""](https://imgur.com/a/hhqxUx4). I really need to get to this level ASAP. I am keen on starting to work on projects to show off on Github. But not sure how do I become HIGHLY PROFICIENT. Would like some advice on that.",1549676124,19,0,1,dataengineering
Thoughts on Dataquest?,"Hello everyone! I have been working as a software engineer for 1 year and am interested in transitioning to a data engineering role. I stumbled upon a self-study site called Dataquest and was wondering if anyone could share their experience with it. How well would a site like this prepare you for interviews as well as being a capable data engineer? 

Here is their course overview: https://www.dataquest.io/path/data-engineer",1549596434,9,0,1,dataengineering
Streaming ETL?,"Good day, I'm trying to move from batch processing of data to streaming/realtime. My team doesn't have a Data Engineer and I'm not sure that I'm going to be able to get one so I'm stuck trying to figure out all the ins and outs of all my options and frankly I'm overwhelmed. 

&amp;#x200B;

Currently, I'm on AWS. Have data going through Kafka topics (w/ mongo dbs) to S3. I use PySpark for ETL from S3 to Hadoop and HIVE (for meta data) and I read it with BI tools from there. My ETL though is like 8 hours long (just by shear volume of data). Budget is not a problem. 

&amp;#x200B;

Would I benefit much from say Kinesis/Firehose, Glue, DynamoDB? Or perhaps Flink or Spark Streaming or something? I can't seem to figure out the best way to improve the ETL bottleneck. ",1549476430,3,0,1,dataengineering
Is a ETL/ BI engineer position a good stepping stone into a data engineer position?,,1549400364,27,0,1,dataengineering
What does a Data Engineer do? What different roles exist?,"A coworker and I put together this post about the different types of Data Engineers.  No real purpose other than to try to minimize confusion in the industry.  If you're a recruiter or someone new to the DE space, then this post may be helpful.  https://www.linkedin.com/pulse/defining-role-data-engineer-blaine-elliott/",1549393385,6,0,1,dataengineering
Data Engineer job without software engineer or developer experience.,"Hi,

Did you become a Data Engineer without software engineer or developer experience? How?

&amp;#x200B;",1549388778,12,0,1,dataengineering
Dependencies management for scientist,,1549297698,2,0,1,dataengineering
An Interview About Building An Open Data Platform For Archaeologists,,1549281862,0,0,1,dataengineering
Date engineer getting up to the speed,"Hi all, 

I have recently started working as a data engineer at a fintech company. The company is investing heavily in data driven decisions and everything is very fast-paced. I am good at coding. I know quite a lot of Apache Spark but what I lack is the exposure of building blocks of a data pipeline. Specifically, if you would ask me to build a pipeline from scratch then I might come up with the tools but might not be able to compare different options. I wouldn't not be able to list pros and cons. I know in many of these cases you need solid hands-on experience. But I see a great opportunity in my company to climb up the ladder if I can prove myself in the short period of time. I also don't have hands-on experience of AWS tools. I am willing to put effort and time but I want to have a clear road map before. 

So, I would like to ask what sample applications, blogs, books or tutorial I should start off immediately? I would like to get deeper insight of data pipeline architecture. And, if any of you could point to a resource explaining how to use machine learning in real-time analysis then that would be great. 
Thanks in advance 🙂",1549204168,7,0,1,dataengineering
When would you use Hadoop vs Redshift (and other parallel cloud computing tools)?,"So I recently learned Hadoop and the various things that come with Hadoop (MapR, Hive) and I'm trying to figure out when one would install a Hadoop infrastructure vs AWS based infrastructure.

It seems like before cloud computing and AWS was a thing, companies invested heavily into Hadoop because it was the first parallel processing engine introduced and companies with newer data infrastructure seem to gravitate towards Redshift/AWS infrastructure.

So it feels like Hadoop is more of a code customized framework vs AWS Redshift is more of a tool based framework (couldn't phrase it well) Can anyone elaborate on the differences?",1549122207,12,0,1,dataengineering
Is the Data Vault architecture something you would recommend to someone creating a new data warehouse?,,1548866955,8,0,1,dataengineering
Make your Python data processing workflow communicate with AWS,"Airflow is a platform to easily declare data processing workflows in Python. Here is an article I wrote about how Airflow connections work. Any feedback would be much appreciated!

[https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc](https://blog.sicara.com/automate-aws-tasks-boto3-airflow-hooks-593c3120e8fc)",1548843280,5,0,1,dataengineering
An Interview About strongDM's Approach To Managing Access To Multiple Databases,,1548738768,0,0,1,dataengineering
How to Prepare for a Potential Layoff as a Junior Data Engineer,"Hi all,

I started as a junior data engineer in October as my first job in the industry (I had just graduated from programming bootcamp but I've dabbled in programming over the past few years for fun). Since then, I've learned how to navigate through parts of AWS and build code within certain services (IAM, Lambda, EC2, S3, etc.),  sharpened my Python skills (I took a CS class in Python in undergrad a while ago), and have gotten significantly better at SQL (we use Snowflake's cloud data warehouse). I've also gotten a lot of exposure to Docker and Ansible recently, and I understand Kakfa (but I wouldn't know how to set this up), but in short all of our data pipeline infrastructure is in the cloud.

I learned last week while I was on vacation that the startup I work for laid off half of their employees - fortunately I was not one of those laid off, but I think it goes without saying that we're going through uncertain times. I definitely want to stay with the company as long as possible; however, I don't feel comfortable not doing anything to prepare myself for a potential layoff, so I'm looking to you all for advice.

My current thoughts on what I can do:

* Complete a side project like this one described in this [post](https://www.reddit.com/r/dataengineering/comments/a0me4w/newbies_experience_building_an_airflow_data/) so that I can say that I've constructed a data engineering project from start to finish (and gain exposure to Apache Airflow)
* Continue sharpening my SQL skills
* Somehow gain exposure to Hadoop/MapReduce/Spark?
* Complete AWS Solutions Architect - Associate certification
* Update LinkedIn, resume, and online portfolio website (this goes without saying though)

It doesn't seem like the company will be shutting down anytime soon, but I'll be returning to work tomorrow so hopefully I'll gain some insight into what the near future looks like. Honestly, I'm hoping everything works out and the company stays alive, but as we all know, there's a lot of uncertainties when it comes to startups, so I'd rather prepare now instead of sitting idly.

Thoughts?",1548677391,10,0,1,dataengineering
"A Book Review of ""Architecting Modern Data Platforms""",,1548657861,3,0,1,dataengineering
What do you look for in a junior data engineer?,"I am going for a junior data engineer role, those of you involved in the recruitment process, what are you looking for? TIA",1548558089,14,0,1,dataengineering
What Data Validation Libraries Do You Use? (py),"I'm not a data engineer, but a data analyst who also prepares or processes data for my team.  For ETL type of work, I use Python with pandas and recently started using a data validation library called [Great Expectations](https://github.com/great-expectations/great_expectations/blob/develop/README.md).  Curious, what do you use or recommend for data validation?  I was using assert statements sprinkled here and there, but with GE library, I can persist or save my validations to re-use again later and I can also pass parameters between data validation steps to create dynamic data validation processes.  These are just a couple reasons why I like this GE library.",1548524658,0,0,1,dataengineering
How Bad Data Happens,,1548509842,1,0,1,dataengineering
Streaming small data in AWS,"Hi All,

I'm hoping I'm not going crazy here, but I wanted to know what the options for streaming data are in the AWS world for small data. When I'm saying small, I'm talking about 50 records per second at peak times, down to possibly none per second at slowest. I'm a part of a startup, so using an AWS service instead of standing something up on an EC2 box would be preferable since I'm admittedly not an ops guy.

One of the requirements of what's needed is pseudo real time data (micro batching okay, but within ~ 5 mins at the absolute max). Due to the allowed latency, I was thinking of using Kinesis firehose. I was also considering SQS, however I'd want to archive all incoming transactions to S3 in both a raw and transformed state, as well as pass the data along to a different application (currently at 1 needing data, but might go up in the future). Random stackoverflow questions have been answered with using multiple queues in SQS, but due to only needing to go to one other app (along w/ s3 writes), I was thinking a kinesis firehose with a routing lambda function for the data that needs to go to my other app.

Anyway, most discussions here seem to either involve pushing systems to the limit or using open source alternatives, so any guidance using AWS services would be appreciated. ",1548395923,7,0,1,dataengineering
5 things you should know for a career in data engineering,"A [post on the Stitch blog](https://www.stitchdata.com/blog/5-things-you-should-know-for-career-in-data-engineering/?utm_medium=reddit&amp;utm_campaign=dataengcareer) today offers guidance for ""students and mid-career professionals decid\[ing\] whether data engineering is for them."" Points include ""you must be a strong developer,"" ""you need to know a lot of technologies,"" and ""experience beats education."" Good advice? What other points should wannabe data engineers know?",1548267490,6,0,1,dataengineering
Open Source Data Extraction Tool," We have two databases, an Oracle database running a legacy enterprise application, and a PostgreSQL database on which we are building a new application that will take over some (but not all) of the responsibilities of the legacy application. Because we do not control the Oracle database, we cannot completely replace it, nor can we build or data model on top of it, for other reasons. As such, we need to keep both databases in sync.

As we are comfortable with Azure Data Factory, we tried using its generic database driver to feed data from the Oracle database into the PostgreSQL one, but it was too slow to be viable. We also tried configuring a Sqoop instance, but it doesn't seem to allow arbitrary queries and we never managed to get it to run a job. Currently we are running a Jenkins instance to monitor hand-built scripts that keep the data in sync. Unfortunately, this solution is brittle and tends to fail in certain circumstances. We would like to have a tool that allows us to do arbitrary queries to the Oracle database and stream the results to the PostgreSQL database.

It is important to us that the tool can be monitored from our Jenkins instance, and that it is fast enough as we are syncing moderate amounts of data. A free and open-source tool is also a big plus.

Thanks in advance.",1548251809,8,0,1,dataengineering
Optimising Highly Indexed Document Storage (Elasticsearch) - Know Your Data (KYD),"I published my article on [~~@~~**ThePracticalDev**](https://twitter.com/ThePracticalDev) community, have a look. Where I talk about how you can reduce your storage costs by knowing your data

[https://dev.to/sumitkumar1209/optimising-highly-indexed-document-storage---know-your-data-kyd-3g42](https://dev.to/sumitkumar1209/optimising-highly-indexed-document-storage---know-your-data-kyd-3g42)

The article was originally published on

[https://medium.com/real-spark/optimising-highly-indexed-document-storage-know-your-data-kyd-c5c11deaa736](https://medium.com/real-spark/optimising-highly-indexed-document-storage-know-your-data-kyd-c5c11deaa736)",1548215274,2,0,1,dataengineering
Optimising Highly Indexed Document Storage - Know Your Data (KYD),[removed],1548167116,0,0,1,dataengineering
An Interview With The Founding Members Of The LEGO Big Data Team,,1548122356,0,0,1,dataengineering
Architecture Standards Doc,"Not sure if this is the best place for this, but I am currently writing some Architecture / Modelling standards (That tie into a policy) around Data aggregation across multiple models and databases e.g AWS, Greenplum, Oracle, excel etc.

In addition, also covering Timeliness, reusability, accuracy, and comprehensiveness.

I was wondering if anyone had written standards around these topics? or even just reporting or aggregation that they can share?

&amp;#x200B;

&amp;#x200B;",1548065925,3,0,1,dataengineering
Optimising Document Based Storage - Know Your Data (KYD),"I published my article on [~~@~~**ThePracticalDev**](https://twitter.com/ThePracticalDev)  community, have a look.  Where I talk about how you can reduce your storage costs by knowing your data

[https://dev.to/sumitkumar1209/optimising-document-based-storage---know-your-data-kyd-1hp8](https://dev.to/sumitkumar1209/optimising-document-based-storage---know-your-data-kyd-1hp8)

&amp;#x200B;

The article was originally published on 

[https://medium.com/real-spark/optimising-document-based-storage-know-your-data-kyd-214e884af5b9](https://medium.com/real-spark/optimising-document-based-storage-know-your-data-kyd-214e884af5b9)",1547890506,1,0,1,dataengineering
Data engineering for a manufacture company,"Hello Im looking for suggestions for implementations of robust architecture to centralize all the data of a manufacturing company.

Right now as daily work I integrate several excel files into one dashboard but I want to go beyond that, something for every department to put info into the system so data get centralized and make analysis faster",1547783650,5,0,1,dataengineering
ETL without database access,"We all know about the standard tools available to shift data from databases into your data warehouse, like Stitch Data.

Has anyone worked in a situation where access to application databases was going to be revoked, forcing you to pull the data from the microservices / applications?

How common is this in mid-stage companies? How has the move worked out for you in the long run?",1547749888,6,0,1,dataengineering
"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends",[removed],1547712638,0,0,1,dataengineering
Passing compressed data from RAM to CPU cache,"Learning some ways how to move data efficiently throughout the computer but have a question....

Lets say I have data that I store in the disk that I compress and load it into the RAM from that point I want to load it into the CPU cache (for speed and to clear up the RAM to process more data). The compressed data is then fed from the RAM to my L1 CPU cache as a compressed file. When decompressed images of the same data are moved to L2 and L3 (so I can load more compressed data to L1), would the decompressed images just be a snapshot or instance of the compressed data from L1? 


Hopefully that makes sense...more than happy to clarify",1547659374,3,0,1,dataengineering
The Three Components of a Big Data Data Pipeline,,1547656803,0,0,1,dataengineering
Pandavro: The interface between Avro and pandas DataFrame,,1547633327,1,0,1,dataengineering
"Serverless Model Serving: OpenWhisk, Apache Spark and MLeap",,1547565656,0,0,1,dataengineering
Checking In On The Time Series Database Market With TimescaleDB (Interview),,1547468778,0,0,1,dataengineering
Help me choose my college courses (I can only pick 3),"I want to get into data engineering and thus want to pick courses that can somewhat relate at least to the field. However, i'm an MIS major and it's too late to switch majors for me. Anyways, in the CS section of my major, i have to take 4 mandatory CS courses + 3 courses of my choice from a list. 

Here are the list of courses (linked with their description). I get to pick only 3. Since you know I'm interested in Data engineering, please help me pick (ie, what 3 would you select):

* [Data Structures and Algorithms](https://i.imgur.com/p7J85eA.png)
* [Programming in Java](https://i.imgur.com/4R92c1V.png)
* [Concepts of Programming Languages](https://i.imgur.com/0xSAJ7Z.png)
* [Software Engineering](https://i.imgur.com/0ojlINv.png)
* [Database Systems](https://i.imgur.com/1bo2UqV.png)
* [Design of Web-based systems](https://i.imgur.com/OkySrNG.png)
* Introduction to Machine Learning
* Foundations of Data Science",1547390730,7,0,1,dataengineering
Why we've chosen Snowflake ❄️ as our Data Warehouse,,1547209526,14,0,1,dataengineering
Providing Valuable Data to a Business as a Data Engineer,,1547196685,0,0,1,dataengineering
A Social Network is just a table with 2 columns,,1547196652,0,0,1,dataengineering
[Question] How to collect and display logs/info for each operation (pandas DataFrame) in a data pipeline?,"I have a several large data manipulation pipelines (some under development). These are batch pipelines and the size of the data that goes into every batch is in the small-medium range (i.e. 10MB-1GB). This means I can store the data for the duration of the batch in memory (or I can use disk based caches). This also means I was able to leverage the use of pandas DataFrames to create complex pipelines with around 10-20 steps.

However I want to be able to log the output (i.e. pandas DataFrame) after every step and also log the the difference between the input and output (i.e. pandas DataFrames). I don't mind the log or info being excessively informative.

So far I've been using text logs i.e. information being printed out to the std-out and log files, but parsing the logs and then being able to view the information in an intuitive way has been a problem. 

Is there a better way to generate meaningful information about pandas DataFrames for every step of the pipeline which is easy and intuitive to view/visualize/navigate not just by me but others less informed about the pipline?

Any personal experiences or links to some resources would be helpful, thanks!",1547027734,7,0,1,dataengineering
Using MongoDB Change Streams to replicate data into BigQuery,,1546961626,0,0,1,dataengineering
Does this subreddit cover personal data management?," I.e. syncthing, perkeep or some other open-source cloud softwares? I'm mainly on syncthing, but I would like to chat with people about this topic maybe. Is there a place for that?",1546874948,0,0,1,dataengineering
Bringing Fast Data To The Hadoop Ecosystem With Kudu (Interview),,1546871147,0,0,1,dataengineering
Pachyderm vs Airflow,,1546801975,3,0,1,dataengineering
Building a workstation for ETL and Data Analysis - what are your thoughts?,,1546695024,1,0,1,dataengineering
New Data Engineer: Tips on Strengthening My Knowledge?,"Hi all,

I recently got a full-time job as a data engineer with a startup that I had been interning with for a couple of months (my background is in healthcare with minimal programming experience), and I wanted to garner some advice from you all on how to strengthen my knowledge/skillset as I don't want to plateau or possibly pigeon-hole myself as a data engineer with useless skills. On a daily basis I touch the following applications/languages: Snowflake (cloud data warehouse), Looker, AWS (S3, Fargate, IAM, Lambda, etc.), Python functions, Docker, Ansible, and a few others. Given that, I have a couple of questions:

* Is it valuable to be ""fluent"" in the aforementioned applications/languages?
* Do you have advice on how I can strengthen my expertise for any of the aforementioned applications/languages outside of my job (e.g. AWS certification)?
* What applications/languages should I learn during my free time outside of the ones I use for work? I've heard a lot about Airflow, Apache Spark, and Hadoop, but I'm unsure if it's worth (or possible) building small projects that utilize any of those modules.

My goal is to accelerate my professional growth and build confidence that I get what I'm doing, ha. Thanks in advance!

Side note: I should mention that I'm also doing a master degree in computer science with a specialization in machine learning, so there will be opportunity for me to learn about big data concepts and whatnot.",1546575131,1,0,1,dataengineering
Best resources for getting familiar with Docker?,"Hey all, I am a data engineer working in a software department at a predominantly hardware company.  We're all on premise and on Hadoop and I use Spark to do most of our pipeline and analysis work.  It's pretty low volume stuff in the grand scheme of what's around us in Silicon Valley, but one thing I think we could really benefit from is Docker.  

I was wondering if anyone could recommend good tutorials/workshops for getting familiar with using Docker, aside from the official documentation?  Things seem to ""stick"" better for me if I can implement them in a mini project, so anything like that would be much appreciated!  Thanks :)",1546556768,7,0,1,dataengineering
data warehouse schema newbie question,"I am new to the world of data science and I have just recently found myself exploring data engineering. To be more specific, data warehousing and data pipelines.

The main questions I have right now are 

* how do you migrate the data warehouse schema? is that done within the ETL process or is that done somewhere separately?
* Does anybody use the alembic python library to do this?",1546530919,14,0,1,dataengineering
Would you consider this infographic a good learning path for a college student aspiring to become a data engineer?,,1546513959,17,0,1,dataengineering
1.1 Billion Taxi Rides: Spark 2.4.0 versus Presto 0.214,,1546504895,1,0,1,dataengineering
"SQL developer background, how to data engineer?",Get MS certification? Go DBA? ,1546483170,5,0,1,dataengineering
Career Advice Needed: Moving from Backend Engineer to Data Engineer,"Hi Guys,

&amp;#x200B;

I have 9+ years of experience as a backend developer (with a moderate frontend experience on React which I have used for my side projects). Lately, I've been thinking of pursuing data engineering as a career and I'm looking for any advice out there that could be thrown at me by the community. In the 9 years of experience, I did work on developing an ETL system for about 2 years in a startup and that was developed in-house using tools like Scribe, Vertica, Highcharts etc. I've browsed over the internet and came across articles explaining data engineering as an entity but did not come across anything useful regarding a career shift and so the reason for this post.

&amp;#x200B;

What I'd like to know is

1. Is it really worth to move from a regular backend engineer working at application level to a data engineer at this point of my career? Are there companies out there who might be interested in candidates such as myself?
2. If it is worth moving, are there any recommended courses out there for me to get on board. Again here, I came across few courses but they only got me confused. I'm okay to chip in few bucks if you say that the course is really useful. 

&amp;#x200B;

Thanks in advance folks!

&amp;#x200B;",1546435567,9,0,1,dataengineering
"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends"," 

Global big data and data engineering services market size was valued at $29.72 billion in 2017 and is estimated to reach $110.66 billion by 2025 with the CAGR of 17.86% during 2019-2025.

Request a sample report @ [https://www.envisioninteligence.com/industry-report/global-big-data-and-data-engineering-services-market-size-outlook-trends-and-forecasts-2019-2025/?utm\_source=reddit-Bindu](https://www.envisioninteligence.com/industry-report/global-big-data-and-data-engineering-services-market-size-outlook-trends-and-forecasts-2019-2025/?utm_source=reddit-Bindu)",1546414724,0,0,1,dataengineering
Making S3A Hadoop connector workable with Apache-Druid,"Just published the article on how to make S3A connector workable with Apache-Druid. For using S3 as a deep-storage and ingestion of parquet format data,  hadoop\_indexd which in-case of S3N connector explicitly requires AWS Secret Key, however with S3A connector AWS IAM profile credentials can be used. Have a look on the article [https://medium.com/@abhioncbr/making-s3a-hadoop-connector-workable-with-druid-35e4df4bd444](https://medium.com/@abhioncbr/making-s3a-hadoop-connector-workable-with-druid-35e4df4bd444)",1546376260,2,0,1,dataengineering
Stream-Native Storage For Unbounded Data With Pravega (Interview),,1546285486,0,0,1,dataengineering
Feature Store: the missing data layer in ML pipelines?,,1546280271,0,0,1,dataengineering
"Having Trouble Installing Airflow: ""python setup.py egg_info"" failed","# Issue

I want to try and experiment with Airflow on my local machine, but I haven't been able to get past the first step. I've been following instructions on the [official documentation](https://airflow.incubator.apache.org/installation.html). I receive the following error when using pip3: 

    &gt; pip3 install apache-airflow
    
    Command ""python setup.py egg_info"" failed with error code 1 in /private/var/folders/yp/gw_kk_h15tx5b9dptzk5rfmr0000gn/T/pip-install-4xu47h_9/apache-airflow/

OS: Mac OS X 

Python Versions Tried: 3.7.2, 3.6.5

# Troubleshooting Steps

* Installing various versions of Python (from python.org). Initially I started with Python 3.7, however, I read there was an issue with [reserved keywords](https://stackoverflow.com/questions/53176846/command-python-setup-py-egg-info-failed). I then tried 3.6.5 but still received the same error. 
* Upgrading pip3 and setup tools as per this [Github thread](https://github.com/facebook/prophet/issues/418)
* Installing python via brew

Any help would be greatly appreciated!

&amp;#x200B;

&amp;#x200B;",1546135830,13,0,1,dataengineering
"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends"," Global big data and data engineering services market size was valued at $29.72 billion in 2017 and is estimated to reach $110.66 billion by 2025 with the CAGR of 17.86% during 2019-2025.  

Request  a sampler eoprt @  https://www.envisioninteligence.com/industry-report/global-big-data-and-data-engineering-services-market-size-outlook-trends-and-forecasts-2019-2025/?utm\_source=redit-anusha ",1545810478,0,0,1,dataengineering
Question about database design.,"Hi,

&amp;#x200B;

I'm working on a side project as a consultant where I'm confronted to the following database design:

\- 1 big table including all events. An event can be anything that happens in the application (login, purchase, delivery, click on product, page visited, etc.)

\- table includes numerous columns, some are straightforward (timestamp, country of customer), others are json/dict columns ({key:value} pairs that can also change over time, a good example of that is a count by product id of what has been bought by a user: product\_bought: {'clothing1': 3, 'clothing4': 2, ...} so this can evolve over time as more products are added to the catalog).

\- in parallel, I have access to a bunch of JSON files that describe merchandise items (for instance a  'sneaker' JSON including all sneaker IDs and different parameters like color, available size, etc.)

&amp;#x200B;

Now I'm just an product guy who can analyze data and not a DBA/SWE by any mean, but I was wondering if this is the best option to organize the DB this way. My newbie mind is thinking that splitting this big monolithic DB into different tables would be better and also feed it these JSON files for easier querying.

&amp;#x200B;

Advice? Thanks!",1545662545,0,0,1,dataengineering
Real-Time Analysis Of Time-Series Data In PostgreSQL With PipelineDB (Interview),,1545636588,0,0,1,dataengineering
Does anyone have a different background?,"Are data engineers exclusively from software and CS backgrounds?

I'm not interested in building ML models or performing analysis. I am very interested in building infrastructure for data pipelines and data intensive applications and projects. 

I have a MSc in mechanical engineering but experience with embedded systems. I'm confident in my ability to learn, but less so in my ability to pass screens for jobs looking for CS backgrounds or software experience

I'm in no rush, and I don't mind taking junior or intern positions so long as I'm learning big data paradigms. 

Are there any data engineers from other backgrounds?",1545356785,18,0,1,dataengineering
Looking to use airflow at work however our current infrastructure is 100% SAS based,"I'm looking to help improve our data workflow at work and from what I can see airflow is looking pretty nice.

Our ""ETL"" process is pretty much 100% based in SAS at the moment and it's unlikely to change anytime soon. My team is more analyst focused than what might be considered a more traditional data engineering background.

No one has any experience with any new technology like airflow, and the current plan is basically writing our own scheduler, meta data processes, and web interface dashboard.

I'd like to start exploring solutions like airflow and others, but due to IT security constraints like not having access to docker I'm not sure if I can even start testing simple SAS data flows on Windows machines.

Does anyone have any experience with airflow (or an alternative) and working with SAS, or windows environments with heavy security, and is able to give me advice or suggestions? 

Haven't been able to find much online regarding SAS integration, but I'm guessing I can run batch files which can run the SAS code.",1545214785,10,0,1,dataengineering
Has Anyone Created a Data Pipeline from Teradata?,"Long time lurker on here. My team and I have a project to create an ETL process that brings in over 30M rows of data from Teradata and loads them into SQL Server.  All of the manipulation has been done on the Teradata side so it's mostly a straight shot to SQL Server. I built a simple data flow in SSIS but it's taking over 12 hours to import all 30M rows. Any ideas on a faster approach? My next approach was to export via PyODBC into a flat file from Teradata but I have little hope that it will be significantly faster. 

I'm interested to know if anyone has worked with exporting large amounts of data from Teradata and what your approach was to do so.",1545111810,7,0,1,dataengineering
The Evolution Of ETL As A Function Of Business Growth (Interview),,1545051518,0,0,1,dataengineering
[Hiring] Data Engineer - Singapore,[https://stackoverflow.com/jobs/217855/data-engineer-streaming-video-hooq?so=i&amp;pg=1&amp;offset=2](https://stackoverflow.com/jobs/217855/data-engineer-streaming-video-hooq?so=i&amp;pg=1&amp;offset=2),1545038678,0,0,1,dataengineering
"If you were a server, which data center would you choose?",,1545033020,0,0,1,dataengineering
Getting into data engineering,"**TL/DR: How do I break into data engineering and what does a data engineer do?**

&amp;#x200B;

I am a 3rd year university student studying Business and Computer Science, and I recently took a relational databases courses where I studied ER models, logical database design and normalization, Relational Algebra, Datalog, SQL, and data warehouses (star vs snowflake schema, apirori algorithm,etc.). I have become very interested in working with data and building data pipelines, the ETL process and hence started to look into data engineering. However, I am having a hard time finding a clear distinction between the role of a Data engineer vs a data scientist. Here are a few questions i have:

&amp;#x200B;

1) Who performs the ETL process? the data engineering or the data scientist? (every resource I read gives me a different answer

2) Why is the ETL process so difficult? Does data cleaning really take that much time?

3) What are the main responsibilities of a data engineer besides building data pipeline and ETL? For an established company like Google for example with major data pipelines and an established infrastructure, what does the role of a data engineer look like in such a massive company?

4) Do data engineers perform any data analysis? (I am very interested in the business side of things as well, and I am hoping to put the business knowledge i gain from school into use as well, however I am not very interested in heavy mathematical/statistical models used for things like Machine Learning,  but I am willing to learn if necessary)

5) Can an employee perform both the tasks of a data engineer and data scientist?

&amp;#x200B;

Finally, any recommended resources or types of personal projects to practice data engineering and break into the field? I have some experience with web and app development, but I have no idea on how to get started on data engineering and if there are any recommended courses online I can take (on things like Hadoop which I have no idea about) to get a better look on the field.

&amp;#x200B;",1545015537,6,0,1,dataengineering
r/ETL x-post. Airflow question on setting up dependent DAGs on different refresh intervals.,,1544980492,0,0,1,dataengineering
[Question] What is the most efficient/cheap way to store huge data sets (X TBs) + streams of data,"I am building a pipeline to analyze huge datasets of text and images (&gt; 2TB). My end goal is to store the data I have, and continuously add streams of data to it. The data is unstructured data (nested json files).

Right now, I am using Kafka to handle streams of data, Spark to run data processing tasks, and Tensorflow to train my models. However, after few weeks, my data is growing very fast (&gt; 2TB), I am trying to choose another infrastructure to store my data. I am using MongoDB right now, sometimes it crashes for not being able to perform I/O operations, and writing streams of data.

What do you advise ? I am using EC2. Do you recommend using Google BigQuery in these cases ? Do you recommend any other (cheap) way to store data. What is the most efficient NoSQL DB to handle nested json files. Is HBase/Cassandra a good option in here ? 

I don't wanna use S3/ or any blob as it is slow to read/write.  
",1544873608,11,0,1,dataengineering
Tips for a spark engineer (early in career) starting a new job,"Hi all! I just accepted a position in a different state for a large company as a spark data engineer. I will be writing a ton of spark for data ETL/processing in scala (I know Python, so they know I'll need to get up to speed on scala spark). I'm so excited for this opportunity but have only a few years of experience under my belt. 

I'm wondering what I should focus on learning/doing in the next month prior to starting at the new position. I will be catching myself up on scala and scala spark, but any good resources that you recommend (ideally free) would be immensely helpful. Thanks for your help!",1544857772,8,0,1,dataengineering
What's your local stack? (and why?),"I'm an ops side data scientist getting a new workstation. With such a convenient opportunity to start over, I'd like to take DataOps to the fullest I can on a local machine. Before anyone says it, I know ""local,"" ""ops,"" and ""fullest"" don't necessarily belong in the same sentence, but humor me.  


So, what are the gurus here running locally, and if you're allowed to share, why?",1544805418,2,0,1,dataengineering
"Global Big Data and Data Engineering Services Market - Size, Outlook, Trends",[removed],1544769819,0,0,1,dataengineering
Publish Data Outside Your Data Lake With a Spark Connector,,1544625968,0,0,1,dataengineering
Apache Superset in production environment,,1544488313,3,0,1,dataengineering
Creating a Data Engineering Culture (Talk - Video),,1544465109,3,0,1,dataengineering
Tackling Apache Spark From The Data Engineer's Perspective (Interview),"Apache Spark is a popular and widely used tool for a variety of data oriented projects. With the large array of capabilities, and the complexity of the underlying system, it can be difficult to understand how to get started using it. Jean George Perrin has been so impressed by the versatility of Spark that he is writing a book for data engineers to hit the ground running. In this episode he helps to make sense of what Spark is, how it works, and the various ways that you can use it. He also discusses what you need to know to get it deployed and keep it running in a production environment and how it fits into the overall data ecosystem.

https://www.dataengineeringpodcast.com/putting-apache-spark-into-action-with-jean-georges-perrin-episode-60/",1544447095,0,0,1,dataengineering
Need tips on how to (ideally) integrate Spark on a local Hadoop cluster,"Hi.

I am an Engineering student and am currently working on a Big Data project. I am doing the project one of the Computer Labs in my College (5 PCs - 1 Master, 4 Slaves - Ubuntu 18.04 - Hadoop 2.7.7).

For the first part of the project, I successfully configured the Multi-node cluster and carried out the required tasks (Analyzing a [data set](https://archive.ics.uci.edu/ml/datasets/Poker+Hand) using a [K-Nearest Neighbor Algorithm](https://github.com/vinitS101/knn/blob/master/KnnPokerhand.java) in Java).

For this I used this [blog](http://kishorer747.blogspot.com/2014/10/setting-up-hadoop-241-multi-node.html) as a reference.  (Additionally, if you think this isn't an ideal configuration, I would love to get some tips and feedback so that my Hadoop cluster works better).

&amp;#x200B;

Now, I plan to carry out the same analysis but with Spark on the same Hadoop Cluster. I tried to follow this [guide](https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/) to set it up (Note: I used the appropriate spark version) and it kind of worked. Spark does seem to start but there are numerous errors.

I am a beginner with very basic knowledge in this. It took me about 2-3 weeks to fully understand, configure and run my Hadoop cluster.

I would really appreciate if you could:

1. Help point out the issues with the resources I linked and how can I rectify them.
2. If you have any better tutorials, blogs or guides that I can read/watch and understand this better please share the links.
3. Am I even doing this right? I have had no actual courses on these topics in College and I am just doing this because the field of Big Data, Machine Learning, etc interest me.
4. How does Cloudera,, Hortonworks etc actually work? I have full systems with Ubuntu and not VMs and I have been unable to find a guide or blog that could help me setup either of those services for my cluster of 5 Computers. If these are any good, I would really appreciate it if you could explain it a little and/or link some blogs, guides or tutorials I could refer to.

I have been stuck on this for a few weeks now. Trying to figure out how to run it. Haven't had a lot of success. Any help would be really appreciated!

&amp;#x200B;

Thanks. :)

&amp;#x200B;

P.S: I am, what they call, a n00b when it comes to most of this. I have  done Python Scripting, web dev, etc that a usual CS grad does. This is  completely new though and for the most part I remain clueless. :D",1544368590,7,0,1,dataengineering
Apache Spark troubleshooting from the trenches,,1544188560,0,0,1,dataengineering
Lars Albertsson – Top 10 data engineering mistakes (Berlin Buzzwords 2018),,1544024523,1,0,1,dataengineering
Lars Albertsson – Top 10 data engineering mistakes (Berlin Buzzwords 2018),,1544023600,0,0,1,dataengineering
Building Distributed Systems On Top Of Apache Zookeeper (Interview),,1543841097,0,0,1,dataengineering
An Honest Review of AWS Managed Apache Kafka: Amazon MSK,,1543809664,0,0,1,dataengineering
Some might want to grab this data bundle. Ends in about 24 hours,"[Source](https://twitter.com/fwpekfjwipefj/status/1067806126635331584)

&amp;#x200B;

I just got it a few days ago and reminded of reminding others to get it too.",1543765896,0,0,1,dataengineering
Turbine: an AWS Airflow stack easily deployed with CloudFormation,,1543684240,3,0,1,dataengineering
Building our data science platform with Spark and Jupyter,,1543348343,0,0,1,dataengineering
Newbie's Experience Building an Airflow Data Pipeline,"Wrote a post about building a data pipeline in Airflow. It's very basic and I believe a good intro to people wanting to learn data pipelines. Feedback is definitely welcome.

[https://josephwibowo.github.io/Meetup\_Analytics/](https://josephwibowo.github.io/Meetup_Analytics/)",1543259376,5,0,1,dataengineering
Building The Dremio Open Source Data-as-a-Service Platform (Interview),,1543230923,0,0,1,dataengineering
Looking to do some sort of schooling in Data Science/Engineering [Looking for suggestions],"I am a petroleum engineer going to work for a very data driven company and thus, it’s probably a good idea to get a good background in data science. I have been shopping around for textbooks on R and Python for data science but wanted to get some input. Have y’all taken any edX/coursera courses that you thought were really good? Do y’all recommend one of the online masters programs at an engineering school (I will be in Midland for most of a 2 year rotation and will have TONS of time)?

Hit me with some suggestions that might be outside of those questions as well, I’ve just started dabbling in working in data science as a side hobby and am super interested in it!",1543076079,0,0,1,dataengineering
Fully Managed Services (GCP) vs Open Sourced,"I attended DataEngConfNYC recently and was surprised to not hear GCP discussed once. 

A lot of the talks were technical and I loved them but I don't come into problems with let's say back pressure in Kafka. Pub/Sub manages it for me. Dataflow manages Apache Beam, Composer Apache Airflow, Dataproc Hadoop clusters as a few examples. And authentication and security is baked in. 

My hunch is there are two main issues: 

1. cost of migrating from an older system to fully managed 
2. ability to hyper tune / optimize servers.

I'm curious if I attend the conference in 2 years, will everyone still be talking about Kafka and Spark, or will  people be using fully managed services?

Is that the case or am I missing something? Would like to get your thoughts.",1542832321,5,0,1,dataengineering
Azure HDInsight Operators For Apache Airflow,,1542829112,0,0,1,dataengineering
Client-side instrumentation for under $1 per month. No servers necessary.,,1542819861,0,0,1,dataengineering
Airflow at Drivy,,1542806123,0,0,1,dataengineering
Automatic type inference from delimited files for creating schemas...,"Hi all,

My data science team has asked me to research the feasibility to automatically infer types and generate create table scripts for Snowflake.

They’re familiar with Pandas and use it extensively in their work and would like to create schemata from a dataframe but my impression is that it isn’t very sophisticated in how it infers data types—seeming to frequently turn columns containing only integers/blanks into floats or objects instead of integers—and chop off leading zeroes in columns like ZIP, FIPS or IDs.

In the past I’ve manually determined types when a data dictionary with type/length isn’t available, curious if there a reliable or accepted way to approach this and accurately infer types for all columns in a CSV versus specifying them or manually defining them when a data dictionary isn’t available.",1542755211,1,0,1,dataengineering
Using Terraform and Packer to autoscale data pipelines,,1542651173,0,0,1,dataengineering
A simple practical example of data governance in action,"I'm reading Ladley's  *Data Governance: How to Design, Deploy and Sustain an Effective Data Governance Program,* and I think ""get"" the essence of it.

&amp;#x200B;

But I'd really appreciate it if someone could give a simple practical example of data governance in action (in your work place or even your daily life!).  ",1542633753,1,0,1,dataengineering
Scalable and Stateful Streaming Data With Apache Flink (Interview),,1542627554,2,0,1,dataengineering
Synthesizing big data frameworks and deep learning,,1542560796,1,0,1,dataengineering
"I’m looking into advancing my skills as a data engineer. Can you recommend material (books, courses) that can do that?",Most material I find talks about the tools. I’m looking for more of an architecture prospective. Looking into different use cases and the pro/cons of those to solve data and scalability problems. ,1542540043,9,0,1,dataengineering
Data Eng/Architect Masters in Canada?,,1542473879,0,0,1,dataengineering
Incomes of experienced data engineers,"I'm in LA and I see jobs with advertised rates for experienced roles in the 120k - 190k range. This is cash compensation I am assuming (base and maybe a cash bonus). The higher end end of the range tends to be more ""modern"" data engineering roles using python/scala/java/kafka/spark etc while the lower end tends to be ""traditional"" tech  - SSIS/Informatica/Oracle etc.

The company I work is more traditional tech heavy and the range for cash+bonus comp appears to be 120 k to 160k.

Is this generally the range? What do you think the average would be? Of course I know this is heavily dependent on location but share your experience please.",1542359898,26,0,1,dataengineering
Meeting of backend dev with data engineering.,"Can anyone clear up the differences in back end operations and data engineering. Log files, user actions, etc will be dealt with by the back end team, no? When does the DE team come in?

I ask because I've picked up enough python, SQL, stats to do my job and help out with other departments in the creation of small pipelines. Making a move to a more robust role means dedicating time to serious study. Should I hang out with the back end team and pick up some javascript and node? 

Last, briefly, how are distributed computing systems implemented. I understand the concept but don't have a grasp on the actual connection between something like amazon S3 and the script that logs data points. What language/tool is used here?",1542248563,5,0,1,dataengineering
Built to Scale: Running Highly-Concurrent ETL with Apache Airflow (part 1),,1542163163,1,0,1,dataengineering
[Hiring] 2 Positions: Business Intelligence Data Analyst And Data Engineer (No Visa Sponsorship),"https://recruiting.paylocity.com/recruiting/jobs/Details/68787/Viral-Launch-Inc/Data-Analyst Master's in Business Analytics preferred

https://recruiting.paylocity.com/recruiting/jobs/Details/67014/Viral-Launch-Inc/Data-Engineer MIS with some job experience preferred",1542143717,0,0,1,dataengineering
Building A Data Lake Platform In The Cloud At Upsolver (Interview),,1542114651,0,0,1,dataengineering
"Is there a recommended learning path to make the move from software development to data engineering? Or just general required skills that one could focus on learning, one by one?","I'm currently a software developer and will make the switch to data engineering. Was wondering if there is a recommended learning path, or a list of skills you need or something.   


Thanks!

&amp;#x200B;",1542114241,6,0,1,dataengineering
Postgres vs Redshift,"The data team is finally getting access to data! 

So while we wait for our replica, we're trying to decide how the data is going to be store on our end. We have a MySQL and an Elasticsearch replica that are going to be dumped in a separate AWS account. From there we have to decide if we're going to go MySQL, Postgres, or Redshift. 

For the most part, we're torn between Postgres (because MySQL would have to be 5.6 and I don't want to deal with that) and Redshift. We're being pressured to turn out results immediately so I'm leaning toward Postgres because everyone on the team knows it and none of us have extensive experience with using Redshift, let alone setting it up.

Is there any reason we shouldn't set up Postgres now and incorporate Redshift later? Would it be dumb to have a Postgres database and a Redshift database? Currently, our business data is at half a terabyte, but this doesn't include event data. The elasticsearch database is about 2 terabytes. To me, it doesn't feel imperative to go Redshift now, but I'm having a hard time convincing others.",1542038471,4,0,1,dataengineering
Google Cloud Platform Data Engineering Exam,"I'm preparing to take the [GCP Data Engineering Exam](https://cloud.google.com/certification/data-engineer) in January and wondering the best approach to studying from people that have passed. I've been working in GCP for the last year and passed the [Coursera GCP Data Engineering Specialization](https://www.coursera.org/specializations/gcp-data-machine-learning). I took the practice exam and got a 60% -- not great so I started studying.

&amp;#x200B;

The [exam case studies](https://cloud.google.com/certification/guides/data-engineer/#sample-case-study) seem important and I've been told to read the documentation for Google's various products. Initially, I was reading through the core concepts of Storage, and created a massive pile of notes. Not sure that's sustainable for each product. 

&amp;#x200B;

I'm trying a different approach: going through the [case studies](https://cloud.google.com/certification/guides/data-engineer/#sample-case-study), understanding each of them, *then* going through each product and to figure out how, as a GCP Data Engineer, would help in each scenario.

&amp;#x200B;

Does that align other people's study habits? What did you do to pass? Detail would be very helpful. 

&amp;#x200B;

Thank you!

&amp;#x200B;",1542032502,4,0,1,dataengineering
Data Engineering Pun,"Hi there,

I am a Data Scientist/Data Engineer currently setting up a ""knowledge group"" in our company together with some other colleagues.

However we've got a tremendous problem....the name...preferably something punny.

Any suggestions? 

&amp;#x200B;

I'm looking for a short name that contains some sort of pun but still contains either 'data', 'engineer' or both. Maybe you can help :D

&amp;#x200B;

&amp;#x200B;",1541953567,8,0,1,dataengineering
Dimensions in a data lake?,"Hi All,

As a background, this is going to be my first time implementing a data lake, and we're going to be using S3. However, I've been a part of implementing a typical data warehouse project, so only the data lake portion is new. I've read up a lot on typical architecture of organizing partitions, file formats, tooling, etc, but the one thing missing from pretty much everything I've read is how dimensions (particularly slowly changing) are handled within this architecture. Since pretty much everything gets stored in parquet now-a-days for data lake consumption, do you just attach all dimensional information to each incoming fact record? Do you store versioned copies of the entire source tables and take care to join only the correct version during analysis?

Any insight or relevant reading would be much appreciated!

Thank you!",1541888614,1,0,1,dataengineering
Contacted for data engineer role amazon. Advice?,"I just had a baby so have a lot going on right now so I don't know if I'll even pursue it but if I do I want to know what to expect in terms of:

1. What is the interview process like?
2. Will they give me prep material for it - I have heard they do
3. How long will they give me to prep?
4. What will be the main areas they will test me?

**My current situation**
I am data architect / technical lead working for a financial institution. Have ~6 yrs exp.

Our Data Stack is what's considered traditional - RDBMS is Oracle/Postgres/SQL server, ETL is Informatica, linux server and reporting/visualizations is Business Objects and Tableau.

My SQL and database knowledge is pretty high. I have a good understanding and experience with ETL patterns/data warehousing as well. However, when it comes to ""modern"" data skills I've taught myself enough to know whats out there and know how to use it on a superficial level - spark, nosql, Hadoop, emu etc.

Since we use informatica we don't use a general purpose programming language for our ETL. But I do know python and the popular libraries for data ""stuff"" - pandas, numpy etc. 

My ability to solve algorithms leetcode style is poor. I'm working on it.

my current job is comfortable - good boss, decent pay, always get my 20% bonus, work from home, full benefits, 6 weeks pto, 6 weeks paternity. I don't work more than 40 hrs a week. Only complaint is I feel like im not growing/learning at work. 

Right now with the newborn this setup is awesome + I still need to take my paternity. So, I feel like the opportunities from ""tech firms"" although normally enticing aren't coming at a good time.

If you have perspective to share regarding going for it anyway or just staying put please share as well.




",1541742442,13,0,1,dataengineering
Creating a Data Engineering Culture,,1541711210,0,0,1,dataengineering
"Why did you want to become a data engineer? Also, what are surprising aspects of the job that you like but that you weren't expecting? And what are some things you like most about the job?",,1541698892,9,0,1,dataengineering
Any good resources for learning the anatomy of a Data Pipeline?,"There's many different tools I can use in a data pipeline and I'm able to find resources on comparisons of those tools, but I'm having a hard time trying to find the proper anatomy and the decision making process of creating a data pipeline. I feel that my knowledge of data engineering is at the point in which I know use cases of several interchangeable technologies, but I don't know how to glue any of the compatible ones together to form a data pipeline.",1541517947,5,0,1,dataengineering
Six Rules for Deploying your Machine Learning Models Faster,,1541441269,0,0,1,dataengineering
Easy And Powerful Self Service Business Intelligence With Looker (Interview),,1541428674,0,0,1,dataengineering
Apache Spark and or Kafka classes,"I'm currently a BI Analyst that has transitioned into a data engineering role. I mostly build data sets, create SSIS jobs, and am currently helping build a new data warehouse. My boss would like me to learn Apache spark and or Kafka. What are some good courses to take or open source project to work on in order to leant these skills. ",1541352152,5,0,1,dataengineering
CS degree,Do you think a CS degree is beneficial to breaking into data engineering?  I have an MBA and a liberal arts bachelors. I have been a data analyst for a few years and want to grow into data engineering. I see most data engineer job posts ask for a bachelor's in CS but mostly want experience. Is it a good idea to get a Master's in CS to get that foundation of knowledge/ get my first data engineering job?,1541342546,7,0,1,dataengineering
"ETL framework for Python, need suggestion.","What is the best ETL framework for doing ETL using Python. We have been using SSIS for it, and sources of our dimensions and fact tables are strored procedure in sql server. Now as a team we are fed up of SSIS

because it is difficult to maintain it with GIT, it is slower and just difficult to work with. I was thinking since we have all the sources in sql server it should be relatively easy for us to use Python to do the ETL.

After building this, my intentions is to deploy it on azure.

With my situation what would be best ETL framework for python ? Recommendations?",1540927632,10,0,1,dataengineering
Apache Airflow best deployed on Qubole (Managed BigData Platform) or with EKS/k8s ?,,1540910519,2,0,1,dataengineering
How Netflix Is Using Jupyter Notebooks In Production (Interview),,1540820425,0,0,1,dataengineering
Big Data Engineers Job Outlook,"A few questions about outlook (maybe 3-4 years). Want to make sure I am consciously improving and staying ahead of the curve and doing stuff on the side to work on my career.

* Curious about what the outlook is for big data engineers. Does it scale 1 for 1 along with Data Scientists? 
* How likely could Data Engineering be outsourced outside of US?  
* What specific DE skills are currently highly valued and any insight on how that might change in a few years?

Background: I just started a somewhat entry-level Data Engineering job (or so I think). Been a python engineer for 2-3 years prior. Currently, I feel like I just write a lot of python scripts to move data into different places (data warehouses, aggregate/roll up data, clean data, importing code into Spark and run jobs) and do a bunch of ETLs. Learning a lot of Big Data technologies in the meantime(Spark, Pig, Hadoop).  Not sure if this even Data Engineering tbh.",1540495612,11,0,1,dataengineering
Beginner needing some advices on the DE carrer,"Hello folks. Not really sure this subreddit is a good place for posting this type of thread, but I'm going to risk it.  


I came here so I can share my experience as a Jr. Software Analyst and a data engineering enthusiast. My goal is to share my experiences and see/read somebody else's opinions about what I've been going through. The reason that I'm doing this is that I don't have any personal contact with anyone on the analytics/data engineering field, so I would like to read other people experiences.  
Well, I have one year worth of professional experience with background on BI and relational databases. Example of tasks that I would commonly work on: database maintenence, query optimizations, data modelling, development of ETL processes, etc.  


The first project that I happened to participate at my company was a BI project. It was a project with a few team members and I was the only one in the team with an IT tech background. I've got a really nice feedback from my superiors, but I had a lot of flexibility with the architectural decisions that I had to make in the project, since no one in the team had an IT background as me.   


This project's contract unfortunatelly was not renewed and then I got reallocated on another project. The new project has a team fully composed of IT professionals. As days gone by, my co-workers became aware of how inexperienced I am, and now I feel like they don't fully trust any decisions that I make. They are mostly developers and have different views compared to me, who has been working mostly on the database side. My co-workers work at a different branches (not in the same physical place) and I believe this make it even more difficult to propose any solution, since they are always backing each other up and not agreeing with me.   


Even though I'm inexperienced, I'm currently responsible for the ""data aspect"" of the project (collecting, loading, modelling optimizing access, integrating, collecting ""data requirements"", etc.). I've been doing well with it (most of the time, that is). What makes me really worried is that I need to propose a solution to a data engineering ETL problem in a Cloud environment. I could make all sorts of architectural decisions to solve the problem, but such decision, once made, could be vital for the whole project. 

&amp;#x200B;

In my professional life, I don't have any reference (or co-worker) in the data engineering field.  With a year of experience, I feel so pressured to come up with a solution that will scale well. I almost quit my job once because I was feeling I was not going anywhere. I feel like, even if I come up with a good solution, my colleagues are probably not going to accept it that easily. My inexperience make me feel so insecure, but I need to be strong to stand by what I've been searching/proposing, and this is really difficult when you're all alone.  


I believe there are few people in the world working as a data engineers. It is a new carreer after all with all sorts of tools and architectural solutions. Most of you may also know/work with few team members in the DE field. If anybody could give me some sort of advice or share your experiences. I would be grateful!  


(Sorry for my english, I'm not really a native speaker)  
",1540436486,4,0,1,dataengineering
Why You Can’t Do All of Your Data Engineering with SQL,,1540416949,0,0,1,dataengineering
DataCamp is hiring course instructors,[removed],1539959021,0,0,1,dataengineering
Finding My Way to Insight DevOps,"Career transitions are always challenging. Program Director John Taylor, who recently finished Insight's first DevOps Program, shares his journey from being an traditional infrastructure engineer to adopting DevOps culture. [https://blog.insightdatascience.com/finding-my-way-to-insight-devops-42a194ff7ab](https://blog.insightdatascience.com/finding-my-way-to-insight-devops-42a194ff7ab)",1539896617,1,0,1,dataengineering
Why physical storage of your database tables might matter,,1539857374,3,0,1,dataengineering
Airflow 101: Automating your workflow for complex data pipelines,,1539795180,0,0,1,dataengineering
Resources for Interview Preparation,"I have about 12 years experience as a Database Developer/Administrator based in the Bay Area. Looking for a new gig and wanted to know what are the best resources  out there for interview preparation?

What should I be focussing on?",1539729897,2,0,1,dataengineering
"Example ""Case Studies"" of ML model management and feature stores in the cloud","I was wondering if anyone can recommend articles from industry on managing deployment and updating of ML models and feature stores in a scaled environment. 

For example,

1. What distributed databases they used for real time features needed for the ML model predictions.

2. What workflow was used to identify when to retrain, how it's triggered, and how to gracefully redeploy.

3. Logging and version control management for ML model versions, particularly in A/B testing or for when failover conditions execute.

Etc.

Thank you!",1539637741,3,0,1,dataengineering
Pipeline Pitfalls,,1539615136,0,0,1,dataengineering
Iceberg: Improving The Utility Of Cloud-Native Big Data At Netflix (Interview),,1539603945,0,0,1,dataengineering
Looking for recommendations of Kafka architectures for Machine Learning Practitioners,"I'm trying to get ramped on Kafka and particularly how to deploy ML models connected to it. I'm a ML practitioner with experience coding, building ML models, stats. However, I'm new to most distributed architectures and Kafka as a whole.

Do you all have any recommended slides, documents, or videos that walk through how ML models can be deployed in Kafka? To simplify discussion, assume the ML model is built from scikit-learn (i.e. no Spark, Tensorflow, etc.) Some key topics I'm interested include:

* How to architect a distributed feature store that a ML model can query?
* How to architect a consumer that wraps/calls a ML model, particularly if the input data comes from outside the topic or consumer?
* Other information related to passing data in the distributed Kafka manner and getting it to a single ML model to consume and act on.",1539266639,6,0,1,dataengineering
Best of Linux Academy,"Recently I asked a question about learning Linux command line and the reddit community gave me some amazing suggestions **&lt;3**. One of those suggestions was Linux Academy and turns out this website has 2 months of free trial period and a lot more content which actually I was looking for other than Linux command line for data science ( data engineering to be specific ) 

Anyone here who has used or using Linux Academy at the moment? 

If yes, 

**What was the best course you came across on Linux Academy ( purely your own personal experience and opinion )**

I am asking this because I am starting the 2 month free trial period , and really want to make the optimal use of it !! ",1539265281,6,0,1,dataengineering
Flink Vs Spark | Apache Flink is successor to Hadoop and Spark,,1539175272,0,0,1,dataengineering
Insight Data Engineering Fellowship coming to Seattle,"Insight Data Engineering Fellowship program [is coming to Seattle](https://blog.insightdatascience.com/insight-data-engineering-devops-engineering-land-in-seattle-3826bc52b4de). After a year running the [Insight Data Science Fellowship out of Seattle](https://blog.insightdatascience.com/seattle-insight-data-science-one-year-anniversary-9e51153d3779), we are adding two new offerings in Seattle -- a Data Engineering Fellowship and DevOps Engineering Fellowship. (NYC [also will get](https://blog.insightdatascience.com/insight-devops-engineering-expands-to-nyc-2723ddd574c4) a DevOps program). Applications are now open for all programs.",1539107742,0,0,1,dataengineering
"Fast, Scalable, and Flexible Data For Applications And Analytics On MemSQL (Interview)",,1539087665,0,0,1,dataengineering
xkcd: Data Pipeline,,1538977157,1,0,1,dataengineering
Serverless Data Engineering in Azure,,1538963754,0,0,1,dataengineering
Building a warehouse from the ground up,"I'm new to the data engineering role and I've started at a company that is just entering into the 
""move/store"" level of the [AI hierarchy](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007).

Currently everything is stored in a MySQL database on a Heroku server or in an ElasticSearch database.
My primary focus is on the MySQL data. Before I got here, someone decided to do monthly aggregations of 
the most minimally useful data and use that for reporting. This has been fine for bare bones situations, 
however, with the hiring of a data scientist and myself, the reporting requests are getting more complicated 
and we don't have access to the data we need so we've been pushing for the chance to build a data warehouse for
analytics and eventual machine learning projects.

I've been doing a lot of reading and I've read the Kimball Data Warehouse book and I know about OLTP vs OLAP
and SQL vs NoSQL and all that jazz, but what I'd like to know: if you were asked to build a data warehouse 
right now and start turning out basic metrics quickly, what would be your first couple of steps?

My gut reaction is to create a replica of our MySQL app database in a Postgres one without PII data and as we
start building out our reports, I can figure out the best way to normalize the data for better performance
and better insight, but that doesn't seem to sit well with the person who would give me access to the MySQL
database because it won't have ""business logic"" applied to it. I personally feel like the data _shouldn't_ have business logic applied to, but I don't know enough to argue.

I'd love to know about other peoples experiences and what steps they took to get the ball rolling.",1538765172,9,0,1,dataengineering
Dr. Martin Loetzsch - ETL Patterns with Postgres,,1538712780,0,0,1,dataengineering
What are your thoughts on recruiters?,"Just curious about your perception on recruiters working at companies like Google, Facebook or Netflix.

Do they reach out to you? Do you hate them? Do you enjoy seeing what opportunities are out there?

Yes I'm a recruiter, not trying to recruit anyone. Just trying to learn more about the market and how people feel.",1538687919,6,0,1,dataengineering
DWH Team name,"Looking for some inspiration, we are looking for a code name for our data warehouse team.

Any ideas? ",1538671421,4,0,1,dataengineering
Schedule web scrapers with Apache Airflow,,1538632078,0,0,1,dataengineering
I’m at a fork in the road in choosing between two completely different career paths post graduation. Can someone help me understand a little more about my opportunities in data engineering before I make a decision?,"I’ll graduate with a Bachelor of Science in Accounting and Finance. I got really lucky in this data engineering offer, but I’m afraid I’m going to limit myself with my lack of knowledge on the industry and just get used as a “pawn” for a big corporation with no room for upward mobility.

First off, I have two job offers/potential career paths.

**Offer 1:** auditor for a public accounting firm (smaller mid size): $57,000. There are 4 promotion levels before reaching partner. After 5 years I’m expecting to make 65-75k After 10 years I’ll expect to make 80-$120k. Partners make (estimated) between $170-$400k  and that will be after 15-20 years. And after a while with the firm I can make equity partner and earn much more. 

- Pro’s: I’ve worked as an intern for 11 months total, they love me. I feel like my work is valued and I contribute a lot. I’ll be able to get my CPA to differentiate myself and allow for further job opportunities down the road.

- Con’s: work life balance is not good (50-70 hours a week, and it increases slightly with each promotion, until you reach partner) neither are benefits. 

**option 2:** Data engineer: $70K starting salary. No idea where it goes from there, all I know is that I’ll be getting paid more to start, I’ll have a better work life balance, and better benefits. I know the next promotion level is senior and that’s about it.

**Questions:**

- is my idea that I might just get used as a pawn with no room upward mobility a possibility? I find it kind of odd that I was able to get this position without any knowledge of engineering or computer science. It is in their business sector, so it makes sense, but I’m worried about what my career path looks like.

- what is my salary potential? What are he different career paths I can take? What will be my expectations in regards to further education? 

- am I hurting myself by not getting my CPA? What are other certifications I can get in the field to differentiate myself in the industry?

- is there anything else I should know?


My position as a data engineer will be strictly related to the business sector. I know big data is changing the world, in fact it’s changing my current profession entirely. I do have a strong interest in data and analyzing data, and making decisions based on data presented (I know that wouldn’t be my first year role, but it’s something I’m interested in for the future). The career itself is more appealing to me than an auditor because I feel like I am actually bringing value to the business. 

I have decent knowledge of what I’ll be doing as an employee, and I’m not worried about learning the software, methodology, etc. because I know they have a rigorous training process. 
",1538575836,2,0,1,dataengineering
Setting up Redshift with PostgreSQL in a VPC,"I found this process to be a real pain, so I wrote an explicit step by step tutorial of how to set up a Redshift database using EC2 instances (and a VPC (Virtual Private Cloud)).

[https://medium.com/@williampnowak/amazon-redshift-setup-tutorial-for-use-with-aws-vpc-ec2-and-postgresql-30281622e3b9](https://medium.com/@williampnowak/amazon-redshift-setup-tutorial-for-use-with-aws-vpc-ec2-and-postgresql-30281622e3b9)

Hope this can save someone the effort I spent to get this basic setup running. Share freely!",1538514954,0,0,1,dataengineering
The Data Engineering Behind A Real-World Knowledge Graph (Interview),,1538445589,0,0,1,dataengineering
Which tool to use in today's day and age for Star Schema ETL?,"Hello,

I was wondering if anyone had experience with my situation:

* I have a Redshift cluster which stores Stitch data from a couple of our CRM/Sales systems
* We will continue to store other system databases in this redshift warehouse in the future

My goal is to start creating star schemas for reporting given all these transactional schemas living in Redshift. However, this brings up a couple of questions:

1. How expensive is it to both store and do transformations all within Redshift? Is my suggestion an efficient way to do things or is there a better way?
2. What tool(s) should be used to even create star schema transformations on top of Redshift? Hive/spark come to mind as buzzwords but I really have no idea which one is optimal for my use case.
3. Are there any guides/tools/articles to guide me that you know of? 

Thanks a ton. I appreciate any/all help.",1538440858,8,0,1,dataengineering
Positive and Negative Data Engineering,,1538427141,0,0,1,dataengineering
Want to learn commands of linux command line for data science/data engineering jobs,"I graduated my bachelors in Computer Science this year and want to become a data engineer in the future, will be applying for basic data analyst/ETL/ data engineer jobs soon enough.

It has come to my knowledge that being good at Linux command line can help me go a long way

Any help regarding how to learn these commands, by tutorials, or online videos, or any book, anything will be appreciated

Thank you :)",1538321566,4,0,3,dataengineering
Where to learn more in-demand DE technologies?,"Where can I learn and practice with more in-demand DE technologies? (Hadoop, Hive, AWS, Snowflake, Scala, Spark, Sqoop, Airflow, Kinesis, Kafka, Spark Streaming, Flink, Nifi, DynamoDB, Jenkins, Circle CI, etc.)

I know this is a long list, but I see these technologies pop up a lot. Google searching seems like a good place to start, but I wonder if it is necessarily the best option as most of these technologies appear to be suited to the enterprise. Would I be able to easily set up my own AWS or Hadoop environment on my laptop, and learn from there?

I ask because, after four years working in a DE+Data Science role at a large tech company, I have found that I am familiar with many of the common tasks and workflows of data engineering, but my company has favored proprietary or in-house tools over more standard and/or open-source tools. I'm just now discovering how un-marketable this makes me when searching for other DE oriented jobs. However, it seems like I would need to work at a large enterprise if I were to learn some of these tools, since these are more suited for large enterprises.

Any good resources that could help me out?",1538021157,14,0,11,dataengineering
Getting started with Apache Airflow,,1537877378,0,0,23,dataengineering
The Case of the Broken Lambda,,1537801930,0,0,8,dataengineering
Big Data Curation Strategies (Interview),,1537756935,0,0,2,dataengineering
"If you have a year and a half left at college to graduate, what should you do to be prepared to apply for a data engineer position?",I have a fair background in AI. I completed the Deep Learning nanodegree program. I've been doing competitive programming for almost 3 years now. But I'm more interested in working with data specifically. Building the architecture and pipelines for it. But I have no idea where to start. Any help?,1537739132,5,0,3,dataengineering
Seeking advice on data analysis workflow,"Hello, I'm working on a data analysis platform. My data source is compressed log files in an AWS S3, they are a proprietary binary format (which adds its pains, I have a custom Python app for decoding them) each around 30mins worth of data, the data contains around 6000 channels logged every 0.5s.

I have been getting by on local processing on my machine by downloading the files. However as the data has grown I've found this approach is lacking. The bottlenecks are (unsurprisingly) downloading the files and decompressing them.

My idea is to scale this out to a multi node cluster, using docker to contain the analysis apps. I'm getting stuck on how to split up the work. The workflow is as follows:

Download file -&gt; decompress -&gt; memory map -&gt; load relevant data -&gt; run several aggregating functions -&gt; store results in DB

Theres obviously lots of ways I could do this, here are some ideas:

1. A single docker app running the full process, spinup lots to get the work done
2. Smaller micro service apps covering each part of the workflow, sending relevant parts between (is it silly to try to send the file between containers)
3. A larger data platform, like Spark, that can access the files in the S3 (would this require plugins to open the custom files?)

Thats as far as I've got. I'm not the most experienced in this, but I'd like to set off on the right path to get the work done properly. Thanks in advance to anyone willing to offer thier advice.

&amp;#x200B;",1537726182,3,0,1,dataengineering
Striking out on your own?,"I am a data analyst thinking about trying to make the leap into data engineering. As data engineers, how easy or hard is it to take those skill sets and strike out on your own, as a consultant or some sort of startup?",1537632404,5,0,5,dataengineering
Do you need Java or scala for next generation data engineering?,,1537588404,8,0,2,dataengineering
Working as a Data Engineer and Reporting Analyst?,"Who here has a role that requires them to do both data engineering and business intelligence (data analysis, report building, web analytics, etc) ? Are you enjoying doing both/all the roles? What does your tech stack look like and what would you change if you could?  Are you doing multiple roles because you want to or because your org is short-staffed?",1537559528,4,0,2,dataengineering
What do data engineers do? I’m having trouble knowing if what I do falls into that realm.,"Hi all,

I apologize if this doesn’t belong here but I have a question that has been bugging me for quite some time. I found this sub and I’m hoping it may shed some light for me.

So I work at a large F100 company. My job title is “business analyst” but we are always referred to as “hybrids” and we work in data process management.

Basically our team (about 50 people) transform data and load up Oracle SQL tables that produce other feeds that out actuaries (or other business partners) use. 

Essentially, we work with the BPs and gather the requirements. We work with the developer to set up our Oracle SQL tables and make table updates. I use SQL to query our databases to validate the correct changes were made. And we configure and run all of our processes in our very complex tool kit.

One project I have been working on is taking this feed (3 million+ records) and transforming and parsing this file and loading into several SQL tables I created with the help of a developer. I created the whole process and designed the whole architecture. I am going to be doing this again for many other large feeds. My team also uses Oracle BI a lot but I haven’t been exposed much yet. 

Is this considered data engineering? Or is the simple data analysis? I see jobs for “data engineers” but I’m hesitant to ever apply bc I’m unsure if it’s above my playing field. They also usually look for CS degrees and my degree is in Marketing but have 3 years experience doing what I do.

Any insight is greatly appreciated! Thanks everyone! ",1537543882,14,0,5,dataengineering
AWS vs Azure culture and career path,"Junior (contacted) Data Engineer here,

[NOTE: This is a request for 100% subjective opinions and experiences]

So far I've only worked with Azure as a Data Engineer.

However, I've always been an open source-y, cross-platform type of fellow and....I feel torn.

I'm a Pythonista and I love Linux, PostgreSQL, Docker, and other open source tools. So naturally I'm drawn towards that spectrum. Also, I find myself in the company of AWS fanboys alot at local Meetups etc.. 

Microsoft has done a wonderful job winning back my affections recently with its Python support and it's open sourcing of .NET and PowerShell and with its SQL Server Linux support, but I still feel torn....

In my job, although I get to use Python, I'm constantly on Windows VMs, Docker is nowhere to be seen, and we are hard-pressed to be able to use Linux VMs at all which bums me out. MS might have embraced Linux, but MS-shop developers don't use or understand Linux and don't necessarily like non-MS open source stuff. There's even some (slight) resistance to Python.

Additionally... While I like SQL Server, I don't think T-SQL is as sexy as Postgres' PL/pgSQL or even Oracle's PL/SQL. 

I'd like to work with open source technologies and databases on Linux.

Here are my questions to more experienced Data Engineers... 

Is the grass really greener in AWS-land? 

Do you find that you get to use the technologies that you love? -- If so, which technologies and tech stacks?

Do you feel less restricted by your company's development culture than I do?

P.S. I'd like to hear from folks on both sides, and especially those who have ""played ball"" for both teams. 


",1537493523,5,0,5,dataengineering
How much would you say the skills you learned was from your job?,"I’m have a year left of grad school (Masters in CS) and I’m getting so stressed and lost on which career path I should be taking. I’ve been splitting up my courses to see if I can get X skills by the time I’m out and working, but I’m scared because of requirements and whether I’m taking the right courses or not and everything. Do you normally learn Data engineering skills while on the job? Should I be taking data science classes rather than database classes, since I don’t want to be more of a Database Admin role? Right now I’ve been taking both software engineering classes and data science classes, and with planning my schedule I’m just very muddled. Does anyone have any insight on their journey through everything? Thanks for reading.",1537305593,3,0,3,dataengineering
Taking Ownership Of Your Web Analytics With Snowplow (Interview),,1537185037,0,0,1,dataengineering
Which of the following (must have) features would compel you to buy a Machine Learning platform?,"Hey everyone! I am a student and am doing a research project for Stanford GSB on machine learning platforms (like Cloudera, Databricks, etc) and wanted to get some feedback from the community. I am performing an analysis to understand the most important features required for these machine learning platforms. Let me know if you want me to add any other options. Will be sharing the results from this analysis at the end.

[https://goo.gl/forms/4sMwflF4L58lUh9j2](https://goo.gl/forms/4sMwflF4L58lUh9j2)",1537146541,0,0,3,dataengineering
Resources on Data management best practices?,"Hi everyone, I am sorry if this is the wrong subreddit but I wanted to know if anyone could point me in the direction of data management best practices? Specifically I am looking at information for the ETL process. What are some resources or concepts that you can point me to regarding data management when integrating, normalizing, and storing data into a data warehouse or similar?",1536886264,0,0,8,dataengineering
Guided Online Projects/ Labs for introduction to Data Engineering?,"Hello! Forgive me if this is not the right place to post this, but I've been exploring some additonal degree options beyond my current occupation in IT. One of these options is Data Engineering and it seems rather interesting. Is there any good resources/ projects one might be able to do to get a taste of what a data engineer does on a day-to-day basis? ",1536799904,4,0,11,dataengineering
"Building a Real-Time Bike-Share Data Pipeline with StreamSets, Kafka and MapD",,1536594514,0,0,8,dataengineering
Using Chaos Search To Make Long Term Log Storage Affordable And Useful (Interview),,1536547372,0,0,3,dataengineering
In which case use sparksql vs hive ?,"I know both can perform sql queries and be used in ETL processes.
In which case should i use one instead of the other assuming we are in hadoop environment :
structured vs unstructured data? performance ? 

",1536390834,5,0,3,dataengineering
"Any advice would be helpful, please!","Hi, I recently graduated from college in 2017 with a B.S in comp sci. Have spent the last year working professionally as a backend java developer. I have been learning spark and hadoop for the past 6 months, also been learning aws big data stack. How can i switch from backend to a big data developer? how do i show companies I know this stuff and that I know it well? Also i'm a little confused with the all the positions, are data analysts like step 1 before becoming a data engineer?",1536345847,3,0,4,dataengineering
Mirroring an FTP Using lftp and cron,,1536251351,0,0,2,dataengineering
SLAs on failed ETL jobs and backfilling?,"Our pipelines are relatively robust and fault tolerant, but issues can still come up. We're facing an issue where, when a job fails, the data scientists both need to know when it happened, and when a backfill will occur. Does anyone have any recommendations on methods to accomplish? I immediately mentally jump to introducing SLAs but I'm not sure those actually make sense for ETL jobs.",1536209613,6,0,6,dataengineering
Building A Master Data Catalog Using Machine Learning (Interview),,1536005915,0,0,1,dataengineering
Data engineering position in Madrid Spain |On-site|EU citizenship required|,"Hey!

&amp;#x200B;

If you are a data engineer who makes his ETL with Python, and have some experience send me your CV at [Christopher@akuaro.com](mailto:Christopher@akuaro.com) and we can chat there :)",1535628201,3,0,1,dataengineering
Data system opens its doors to all Liners : LINE Engineering Blog,,1535592175,0,0,0,dataengineering
Anyone ever worked with Nexla?,"These people: https://www.nexla.com/

If you've worked with them, I'd be interested to hear what you think of them, and how comprehensive they are. I'm afraid that they do the easy 80% of the work.",1535572729,0,0,2,dataengineering
Cross-post: Creating Work Queues with Apache Kafka and Apache Pulsar,,1535571236,0,0,1,dataengineering
Best Hadoop blogs/tutorial portals,What are the best Hadoop blogs/tutorial portals for learning the internals of the system?,1535544113,1,0,8,dataengineering
Using Homomorphic Encryption In Production With Enveil (Interview),,1535398249,0,0,0,dataengineering
apache-airflow 1.9.0 docker based container.,,1535240166,0,0,9,dataengineering
apache-airflow 1.9.0 docker based ready to use container.,,1535239362,0,0,1,dataengineering
Fresher computer science graduate looking to build a strong base for a long future in data engineering field,"I completed my computer science graduate this year and am looking to build a skill set, along with a portfolio that I can showcase for interviews. 

I am practicing SQL right now, and getting basics of object oriented programming, as I learnt these are the most basic things for a long career in data engineering. 

What should be my next step after this?
What technologies should I learn after these so that I can start applying for jobs? 

The options that I have come across are 

1) learn some NoSQL technologies like Cassandra, MongoDB
2) focus on cloud based technologies, cloud is the future
3) focus on learning Hadoop, Hadoop is the future
4) learn how to use ETL based tools

I am willing to know what my next step should be, is it something from the above mentioned points, or something else. 
",1535173815,12,0,3,dataengineering
Practical Machine Learning basics with The Simpsons,,1535010080,1,0,4,dataengineering
"DGraph: A Fast, Distributed, Transactional Graph Database Built For Scale (Interview)",,1534761429,1,0,2,dataengineering
ETL from individual JSONs to RDBS: language/framework/tool choice?,"Hey everyone. At my company we have an inventory of API repsonse data, each response sitting in a separate JSON file. I'm responsible for the data pipeline that extracts useful analytics from this data, and as a first stage I extract all data from each JSON file and flatten it for insertion into an RDBS so that raw data can easily be compared.

Although the main language within our team is Python, I decided to go with Go for this stage of the pipeline. The way I go about it is basically: I define a struct that represents the JSON schema, another struct that represents the record to be inserted, and a function that maps one onto the other.

However, there are issues due to the schema not being consistent at all times. There was a moment where we switched endpoints at which point the schema changed for example. Both schemas are not separated at storage, so due to the rigidity of Go I have to do some convoluted things to figure out which is which and use the appropriate struct. Another case was that a long time ago, one value ended up being stored as a float (100.0) as opposed to the int (100) it's been in more recent times. When Go unmarshals the JSON, it throws an error saying it can't map 100.0 to an int, and sets it to zero. Something a language like Python would happily let through. I have found a way using JSON specific types in Go to avoid this issue, however, then it deviates from the elegant bliss I hoped it would be again.

So I do like the idea of clearly defining a struct in Go and having it throw errors as soon as things deviate. On the other hand, I would like things to be a little more flexible, especially in terms of cleanly defining a schema and functionality that decides which one is supposed to be applied. Now I'm a bit overwhelmed and thinking whether it's best to stick with Go implementing solutions for all edge cases is the solution, or perhaps it's best to have another look at using Python and one of the many schema frameworks. Alternatively, maybe I'm missing a really obvious solution here due to my juniorness, as I assume there have to be ready-made solutions for this kind of task?

Any help, advice, criticism, greatly appreciated :) Thanks in advance!",1534704725,6,0,8,dataengineering
Data Engineer vs. DBA salary / career prospects.,"Current Data Engineer here,

At one point I wanted to go down the DBA track for my love of databases, data modeling, and in-DB development, but my breadth of skills with general programming etc.. landed me firmly in data engineering role. 

Sometimes, I still fantasize about being semi vendor-specific DBA but in a polyglot persistence environment (Oracle, PostgreSQL, MSSQL, MySQL, etc..).

My questions are as follows --

- Has anyone here converted from Data Engineer to DBA or vice versa? 
- If so, what are the salary differences? 
- Any other relevant experiences?

(Also, for salary it looks I'm already making more than the average DBA, but I'm not sure how accurate Payscale.com and other websites are).

P.S. I already do a lot of DBA tasks, but I envy the autonomy and solo-ish responsibilities of the DBAs I've met. In my experience, Data Engineers typically work in very cohesive teams, but I dislike pair programming and prefer solo work.
",1534630220,6,0,2,dataengineering
Data validation for new data source,"How does everyone here do data validation for new data sources? What are things that you look out for? I'm very new to the field, so I have some really basic things that I look for like amount of data returned on average and when that goes down I say something, but there have been a few instances where I missed something that was really bad. In the last case I broke a lot of stuff because there was a field that was completely null except for when the data scientists pulled them, and it's still unclear to me where they got the fields from. ",1534525915,2,0,2,dataengineering
Why I’m sick of ETL tools and what I use instead (hint: it’s a different ETL tool),,1534453213,0,0,4,dataengineering
Big Data Weekly Newsletter – Big Data News Weekly,,1534426281,0,0,0,dataengineering
Anyone did data engineering course in dataquest.io ?,"Anyone did data engineering course in [https://www.dataquest.io](https://www.dataquest.io) ?

It seems great, but 50$ a month is a lot.

I'm a CS student without any knowledge in databases, and  I want to learn some stuff in this subject throughout my summer.",1534420490,3,0,1,dataengineering
Why data engineering jobs require backend experience?,"I'm a 3rd year CS student. Long story short I love data and programming, and from what I understood data engineering is the way to combine both.

Pretty much all of the entry level data engineer job posts I see, require 2+ years in backend development. 

I know that backend is a part of web development, that focuses on the servers, logic, and many of the big stuff. I can see the resemblance due to the communication with databases, but I was wondering if there is any connection to web development in general.

If someone could put in order all the bullshit I just wrote, it would be great :)",1534419588,4,0,4,dataengineering
Creating Readable Spark Jobs,,1534286791,1,0,8,dataengineering
Lessons Learned While Building A Data Science Platform With Airflow (Interview),,1534178415,0,0,10,dataengineering
Anybody using Record Format conversion in AWS Kinesis Data Firehose?,"Anybody using this feature of Kinesis Firehose and care to share their experience? It looks like a convenient way to write event data directly in Orc/Parquet format into S3 for downstream processing.

[https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html](https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html)",1533950202,0,0,3,dataengineering
Anybody using AWS Kinesis Data Firehose Data Transformation?,Anybody using this product from AWS and care to share their experience? It looks like a convenient way to write event data directly in Orc/Parquet format into S3 for downstream processing.,1533949575,0,0,1,dataengineering
15 Proven Steps to Become a Data Engineer | Cracking Hadoop,,1533857136,0,0,1,dataengineering
Monitoring Airflow with Prometheus,,1533720957,0,0,6,dataengineering
A Whirlwind Tour Of The PostgreSQL Database (Interview),,1533552790,0,0,1,dataengineering
Assignment on graph processing using GraphX in Apache Spark,"Graph processing using GraphX in Apache Spark. While working on a simple issue of combining two data-sets by finding and using the common neighbors to generate a new data-set with these values and the similarity measure I realized that we need alot more articles on Spark GraphX problems and solutions. Sure it was fun researching, pair-coding as well as reaching-out to seek advise and clarification, which in turn made me realize that such a great tool should have more published and easily accessible examples online.",1533385818,0,0,8,dataengineering
What can I do to make my data engineering processing run more efficiently?,"So I'm doing a data engineering challenge and it's very straightforward. I have written a Python script (using only built-in modules) that successfully processes small datasets, but I don't yet know how to properly process large datasets and I need to learn this. I'm using Linux, with an i7 processor and 4 GB of RAM.

Basically, here's the project: 

- Read in a .txt file with two columns of data. 

- One column is the name of a product, a second column is the price of the product from some source. The product appears potentially many times, always with a different price. Like this:

    data_input = [['product_name', 'price'], ['bed', '600'], ['chair', '100'], ['bed', '800'], 'chair', '140'], ['fridge', '1000'], ['fridge', '2000'], ['knives', '180']]


- The output data should be 3 columns, where each product appears on one row, with the product name in one column, the total price in a second column, and the total count of products in a third column.

    data_output = [['product_name', 'total_cost', 'total_count'], ['bed', '1400', 2], ['chair', '240', 2], ['fridge', '3000', 2], [['product_name', 'price', 'count'], ['bed', '600', 1], ['chair', '100', 1], ['bed', '800', 1], 'chair', '140', 1], ['fridge', '1000', 1], ['knives', '180', 1]]

So, like I said, I've already written a Python script that does this for a small dataset like above. But when I expand the input data file so it's in excess of 1 GB and my computer lags with millions of rows, the computer just stalls and won't process the dataset.

What software or other language or Python technique or other method do I need to use to make this work so my code will successfully process massive datasets? And where can I look to learn how to do this?",1533240206,8,0,1,dataengineering
What is a Data Pipeline?,,1533136079,1,0,9,dataengineering
How adopting a distributed rate limiting helps scale your platform,,1533132252,0,0,2,dataengineering
Interview resources for data engineer interviews,"Hey 

1. I have a data engineer interview coming up with Snapchat, I have done all the free question related to sql (under database category) on leetcode. Will this be enough for the interview?
2. I have done some 50 question on leetcode related to arrays and strings, and can handle easy level of leetcode. 
3. I still struggle with DP related question, even medium and easy at leetcode? Should I expect them to be in data engineer interview?

Does someone has any suggestion for the interview?",1533058316,6,0,10,dataengineering
Collecting And Analysing Data At Human Scale With Ona And Canopy (Interview),,1533008478,0,0,0,dataengineering
Need help in finding best Hadoop hands on tutorial,I am beginner in the field of data engineering. I know some concepts in hadoop and big data. But I dont have much of hands on experience. So can you please suggest some free online resources where I can get some hands on experience.,1532852123,0,0,12,dataengineering
Earn money to promote this link,,1532809040,0,0,1,dataengineering
Oak Ridge NL is hiring a Data Engineer - Come help us build cool stuff!,,1532702417,1,0,0,dataengineering
What Are Good (Media) Data Engineering + Machine Learning Project Ideas?,"I have been given some freedom to create some useful big data sets and publish them for business insights / downstream use. 

I will primarily work on 'video' data (e.g. movies, TV shows) but there is some music and other 'media' data available also. 

There is a tonne of customer data (subscriptions, purchases, etc) and a reasonably robust taxonomy of data on videos (e.g. what category a video is labelled under) already; I want to push the team forward and therefore explore something fresh but still useful.

I would like to derive some project that mixes machine learning in the data engineering space and/or requires map reduce in Python and/or both to also learn something new and upskill. As I am quite junior, I am struggling to find an 'end goal' to work towards. I have asked the business but there isn't much insight provided given this area is a black hole. 

Can someone give me a few ideas, then I can figure out how to make it work technically? 

I would also be interested in resources (books, talks, etc) that cover this space and what cool stuff has been done in the space! It is an exciting area so I feel there is a tonne of untapped potential, just need ideas! ",1532698459,2,0,4,dataengineering
How to architect the perfect Data Warehouse,,1532639958,5,0,11,dataengineering
Data Analysis in the Age of Cambridge Analytica,,1532526314,0,0,2,dataengineering
Ingest tweets using Kafka and parse in Real Time with Spark Streaming,,1532419711,0,0,4,dataengineering
The future of Data Engineering,"I am looking to hold an event in London next month focusing on the future of data engineering, and would be really keen to get everyone’s input. What are your thoughts? What should be spoken about? What is everyone’s views on what the future holds for data engineering? 

Thanks! ",1532344395,1,0,4,dataengineering
Architecture and operations of data engineering project in Azure,"Hey do you guys know if theres any company blog explainging how they have used microsoft azure (and microsoft stack), to perform ETL. I am trying to learn technologies like azure data factory, azure sql db etc. and I was 

wondering how does the operations works. Few question are below:

1. How does source control works in azure data factory.

2. How does releases happen in azure data factory?",1532279561,0,0,2,dataengineering
Most requested technologies for Data Engineering Jobs,,1532269971,2,0,4,dataengineering
Developing and Testing Airflow,,1532038722,0,0,9,dataengineering
Small data engineer?,"Just saw this post by Jesse Anderson: [http://www.jesse-anderson.com/2018/07/saying-you-have-small-data-isnt-belittling-your-use-case/](http://www.jesse-anderson.com/2018/07/saying-you-have-small-data-isnt-belittling-your-use-case/). I think it's really good, and I agree with the idea.

However, it makes me wonder what the validity behind calling yourself a data engineer, if you don't have a big data use case? ",1531946625,7,0,8,dataengineering
A typical data engineering project - Netflix perspective,"Hi all, I just started sharing learninga from DE meetups in the Bay Area and conversations with experienced data engineers in the Bay area. Sharing the first post here: “Personalized Learning Platform For All Technical…” https://medium.com/hasbrain
",1531764442,0,1,27,dataengineering
Free Lectures playlist for ADVANCE HADOOP Concepts,,1531748430,0,0,2,dataengineering
"Using Ceph For Highly Available, Scalable, And Flexible File Storage (Interview)",,1531744700,0,0,1,dataengineering
What Are The Best Books / Resources On Data Engineering?,"I am becoming quite interested in the data engineering world, and there are some roles in my company for this type of work. I want to improve my depth of knowledge on a few things, and there is enough data/project here to get this type of work. Can someone give examples or / clarify actual projects which could cover this criteria (hypothetical projects are fine, it is just to get some idea)? 

1. **Code complex algorithms that traverse multiple data structure types in highly optimized manner to be performant in a big data environment.** What would be an example algorithm that would traverse data structures? 
2. **Data modeling skills for both upstream operational systems and data warehouse fact / dimensional data modeling.** Data modeling = database design? e.g. distribution keys, sort keys etc. 
3. **Distributed systems processing in the area of data ingestion and data distribution.** Using technologies like EC2/EMR? Or how your databases are distributed / how often they update? 
4. **Working with either a Map Reduce or a MPP system on any size/scale.** Again, using technologies like EMR? I use Zeppelin on EMR, but mainly for running Spark and doing ""data science"" work (but have developed scripts, then run them on spark-shell). 

I want to be the one to suggest improvements to the current data structures / pipelines etc, I have strong skills in Python and SQL mainly, so want to ramp up on the theory enough to feel confident and suggest improvements. Therefore, any book / resource recommendations that cover the above and/or more would be great. 

Thanks in advance!",1531437536,8,0,9,dataengineering
What should I add to my profile to begin a data engineering carreer (as a software engineer) ?,"Hi everyone,

I'm a software engineer, working mainly on web backend systems that recently developped a new passion about data engineering without knowing that it was an already existing and ""standardized"" field.  


I worked on multiple personal projects where I built data pipelines and analytics systems using technologies such as AWS Kinesis Stream and Data Analytics, Azure Stream Analytics and EventHub and many other related techs such as S3, Azure Blob Storage and MongoDB.

The programming language I use to extract ""transform"" and send the data is Node.js (using Typescript). Many people think that Node.js is not fit at all for this but when you look at it closely, it's really a perfect fit for those use cases.

Node.js is perfect for handling I/O in a fast and efficient way especially when you use streams (it's the best feature of Node.js but also the less known and used).

I also program in Java but I mainly use Node.js nowadays, way easier and faster to create I/O handling programs/scripts.

My main concern is that my knowledge of Node.js will not be recognized in this new field and that people/companies would favor Python. I feel that Python is one of the missing pieces in my profile but I don't feel I really need it, especially if I just focus on data engineering and not data science.

There's also all the hadoop ecosystem + apache spark that I think are really important to know. For now I learned (and still learning) to use managed services such as Kinesis Stream and Kinesis Data Analytics instead of Spark + Storm/Flink for example.

Do you think this is enough to start or is there an important missing piece that I should add before diving in this field ?

Thanks !",1531407837,0,0,8,dataengineering
Self Service Data Flows With Apache NiFi (Interview),,1531218793,0,0,4,dataengineering
How to learn Data Engineering knowledge and get an internship? • r/cscareerquestions,,1531188742,0,0,6,dataengineering
New Skills for data engineer [MICROSOFT],"Hey I am a software engineer with expertise in building ETL pipelines using SQL and SSIS. I also code in C#

to write webservices off the datawarehouse which are used by analytics reporting in different parts of our product.

Since I have expertise in SQL, data modelling and programming in general, I wanted to make a shift in data 

engineer role. Can someone suggest what new skills I should pick up and after that how can I model my resume 

to get interview calls? 

FYI: We are primarily a Microsoft technology stack. So please keep the suggestion related to Microsoft (if that's possible)",1531150827,25,0,4,dataengineering
"""Building Data Flows In Apache NiFi With Kevin Doran and Andy Lopresto - Episode 39"" (https://www.dataengineeringpodcast.com/nifi-with-kevin-doran-and-andy-lopresto-episode-39/)",,1531109400,0,0,3,dataengineering
Integrating Crowd Scale Human Intelligence In AI Projects (Interview),,1530534294,0,0,2,dataengineering
Airflow ETL for Google Sheets and PostgreSQL,,1530488199,0,0,8,dataengineering
The Two Types of Data Engineering,,1530127553,11,0,7,dataengineering
A Beginner's Guide to Data Engineering - The Series Finale,"Hi all,

Data engineering is a very important adjacent discipline to data science, but it is new, often under-appreciated, and rarely discussed in details relative to its close cousin Data Science. I still remembered the first time I was trying to learn Luigi, an open-sourced project from Spotify for ETL, and it was really difficult to find well presented materials out there.

Having worked at Airbnb for a few years, I was really fortunate to learn a few things about data engineering from some of the best people in the industry. As I am acutely aware of this knowledge gap, I hope to contribute to sharing and disseminating knowledge with all of you who are still learning.

You can find my post on Medium: https://medium.com/@rchang/a-beginners-guide-to-data-engineering-the-series-finale-2cc92ff14b0, focusing on data engineering frameworks. If you are completely new to DE, you might consider reading Part I &amp; Part II of the series.

Your feedback, comments, and suggestions are always helpful and welcome!",1530051884,3,0,21,dataengineering
How to partition by event processing time in Airflow?,"In this article by the creator of Airflow (https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a) it is mentioned that data should be partitioned by event processing time to land immutable blocks of data. How is this implemented?

For an example, if I have a system with events entering some stream (e.g. Kafka or Kinesis) and then periodically the data is written to storage (e.g. S3 or other) which are then batch processed on some schedule (e.g. airflow), then there are multiple 'time' values to consider.

t1 -&gt; time of the event occurring t2 -&gt; time of the event entering the stream t3 -&gt; time of persisting batch of events to storage t4 -&gt; time of batch run (airflow) for further processing

What is considered the ""event processing"" time in this case? How is a partition generated so that at immutable block of data can be landed predictably? Presumably there must be some deterministic pattern for generating batch runs so that time partitions are immutable and so that backfill tasks can be generated.",1529964858,7,0,3,dataengineering
Quilt: The Package Manager And Repository For Your Data (Interview),,1529922542,0,0,2,dataengineering
Deploying machine learning models to production an overview,,1529545107,0,0,3,dataengineering
"Moving from DevOps into Big Data, too much or achievable?","I am considering 2 job offers. 1 as a traditional DevOps Linux infrastructure, which is my background.

big data stack: big data pipelines with Spark, Scala on AWS EMR including Redshift, Snowplow and Airflow.

kinesis kafka cluster Kubernetes cluster python data bridge data-iq scala packer s3 kickstream redshift ec2 elasticsearch aws terraform docker aws-eks (nieuwe eks)

I like learning open source technologies, but I haven't done any kinesis, kubernetes, kafka, data bridge, data-iq, kickstream, redshift, emr, snowplow, airflow, spark before. They know I don't know any scala/java, just normal scripting and that was fine.

Could this be fun or does it seem too much to learn? ",1529484715,7,5,6,dataengineering
How to implement consistent hashing efficiently,,1529403784,0,0,4,dataengineering
Heap's Data Infrastructure In Depth (Interview),,1529321559,0,0,2,dataengineering
How to master the basics of Data Engineering,,1529307091,0,0,11,dataengineering
Suggestions for a data pipeline that sometimes has to correct old outputs?,"I am working on a project that works basically like so:

* New files of protobuf messages are dropped into an S3 bucket, and correspond to 1 hour of event data
* Each of these files must be read, translated to apache parquet and written to a different S3 bucket, and separated into directories according to day (so, all data for 2018-06-17 goes together, all the data for 2018-06-18 goes together, etc)  
  
* Sometimes, a new proto file will appear in the source bucket, which corresponds to an hour which has already been processed. This is because upstream producer of the data may detect that it needs to reprocess a time interval, and write a new file with the more correct data. In the case that one of these files occurs, it's important to delete the old data for that hour, and write the new data in its place.  

Other info: 
  * max acceptable delay in writing output is 4 hours  
  * these ""corrections"" in point 3 can happen to any data that is up to 7 days old  
  

The first two points are very easy to me, I've set up a nice apache flink application which reads proto in, and writes parquet to a bucket, bucketing output by day. That's fine. But these streaming frameworks like Spark and Flink don't seem to cover the case of going in and deleting old data that has already been written. Are streaming frameworks the wrong tool for this job? What would you use?  
  
Thanks!",1529251254,1,0,4,dataengineering
"I am completing a data engineering assignment for a class, and it was my plan to use Pandas for the assignment. But the instructions specifically say do not use Pandas because it's apparently not scalable for the kind of problem I'm working on. What's a better alternative for Python?","I will acknowledge that I know next to nothing about scalability, but I honestly thought that Pandas was better for scalability than just using a regular Python script with built-in functions.

Am I actually better off just using regular Python with built-ins rather than Pandas for scalability, or is there some other trick within Python that I'm not aware of?",1528839353,18,0,2,dataengineering
Suggestions for a small personal project,"Hey guys, so I'm making a small project which is basically as follows:  I have a python script running through crontab in a VM in AWS pulling data from an API every minute, changing a bit of the data and then sending it to a PSQL instance also in AWS. 

It's been working five for days (planning on leaving it for months to compile data), but i've been thinking about redundancy.

Now I'm really new at this and mostly deal with the business side of things but have wanted to learn more about the technical side of things. 

My original idea was to use docker to create 2 containers running the data aquisition service, and then have a 3rd container doing a reconciliation service where it will grab the data from both containers and kind of create 1 dataframe taking only 1 of the data so if at 00:00 both containers send data, it will pick up only one copy, but if 1 container fails to collec and the other one does , the data goes in.  And this reconciliation (if you can even call it that) service then pushes the dataframe to PSQL that's a different instance. maybe push it to 2 different databases in different services like 1 PSQL and 1 MongoDB. 

The problem is I have no idea how to do that 3rd service that grabs the data from both and reconciles the data between both of them. I guess my main problem is dealing with the pipeline as I don't know the best way to send the data between containers until the end. My original Idea was to spin up a service and a psql instance on the reconciliation container and have both services push into those psql instances and then have a python script collect the data from the database, remove duplicates and then send it to the final database. But that sounds needlessly complicated and those databases will grow big quick and i don't know enough to do some kind of ""streaming service"" where only the newest data is appended. 

Either way to stop ranting about it I wanted to ask if you guys had any suggestions of what I could do or how bad my line of thinking about this was. 

Thank you for your time.",1528795681,3,0,2,dataengineering
Electrical Engineer Entering the World or Data Engineering,"Hi, I am an 28 years old EE graduate who is currently working  as a controls/automation engineer in a automotive testing company. I have always had a strong passion to get into the world or big data. 

I have this plan to take online courses and build projects using exisiting data of our automotive test measurements. But my concern is that without a mentor at work (All my co workers are mech engineers) can i atually make something useful to put on my resume so i can break into data engineering/data sciense as an intermediate level developer? Or should i just apply for a junior position now as a data analyst. 

Which way would be the best to get into big data. Any answer or advice will be really appreciated 

My skill set now: 2 years mssql
5 years control systems and UI/HMI programming using plc ladder logic, C# and web app development with JavaScript and react 
Intermediate level python and spark ",1528745731,2,0,2,dataengineering
CockroachDB In Depth with Peter Mattis - Episode 35,,1528717309,0,0,1,dataengineering
Bistro: a radically new approach to data processing (alternative to MapReduce),,1528536584,2,0,4,dataengineering
New site that ranks the most popular data science tools,,1528300224,0,0,3,dataengineering
The Math behind Face Recognition,,1528261168,0,0,1,dataengineering
Data Engineering Weekly Digest,,1528215471,0,0,2,dataengineering
"Fast, Scalable, and Flexible Data Storage with ArangoDB (Interview)",,1528117566,0,0,3,dataengineering
Big Break in Big Data: Sapient Talent Hunt for Data Engineers,,1528091436,0,0,1,dataengineering
Anyone like Nepusz Tamas in the world? he made this,,1528013864,2,0,2,dataengineering
Choosing between jobs,"I am interviewing for these two roles:

Junior IT Developer \- [https://capsconnections.ualberta.ca/caplet/Job/Detail/17678?fromsearch=True](https://capsconnections.ualberta.ca/caplet/Job/Detail/17678?fromsearch=True)

Data Scientist \- [https://group.bnpparibas/en/careers/offers\-world/standard\-permanent/data\-scientist\-9/amp?utm\_campaign=google\_jobs\_apply&amp;utm\_source=google\_jobs\_apply&amp;utm\_medium=organic](https://group.bnpparibas/en/careers/offers-world/standard-permanent/data-scientist-9/amp?utm_campaign=google_jobs_apply&amp;utm_source=google_jobs_apply&amp;utm_medium=organic)

Which one do you think would give me more salary and more opportunity to grow?",1528010218,2,0,0,dataengineering
Some help in terms of which classes to take to go towards data engineering?,"Hello! I am a Master’s student in CS (bachelors was not in CS) and I want to get towards a career in Data Engineering. The program is around 2 years and I am currently midway through the 1st year. 

My major allows me to take 4 classes from a specific discipline (SE, Data Science, Database Systems, AI, Theory, or Software and Systems Development) and then 4 more classes from any of those listed disciplines. I’m not sure which classes would be most beneficial in terms of the data engineering field and obviously you guys won’t know what classes these are exactly but the names might help. Here are classes in the three main disciplines that I’m assuming I should be concerned with: 

Database Systems:

- Database Programming
- Database Administration and Management
- Spatial Databases and Geographic IS
- Database System Implementation
- Distributed Database Systems
- Advance Database Concepts
- Advance Database Management 
- Mining Big Data
- Intelligent Information Retrieval 

Data Science:

- Data Analysis and Regression
- Intro to Image Processing 
- Applied Image Analysis
- Computer Logic Design 
- Neural Networks and Deep Learning
(Left some classes out here)

Software Engineering (general SE courses there’s a whole lot to type)

Thanks so much in advance!",1527955844,12,0,1,dataengineering
Resources for Data UAT Best Practices,"My company is building a new data platform. We're consolidating data that had lived in different databases and other repositories, which includes completely re-working many of our base tables. We've moved into the user acceptance testing stage for tables in the new platform, but our process is pretty haphazard. So I volunteered to craft UAT guidelines that both the techy people and business users can refer to. 

The problem is, I'm only finding suggestions for software UAT. Can anyone point me towards best practices for data-ralated UAT? I'd want to ensure that both our base tables are accurate and eventually that the reports in our BI tool are also correct.",1527780869,0,0,4,dataengineering
The Alooma Cloud Data Pipeline Deep Dive (Interview),,1527500740,0,0,3,dataengineering
Data Engineering Project Ideas,"I'm looking for suggestions for a data engineering related project idea.. something that I could dig my heels into and then push on github to help with my job search. I think my lack of github projects has hurt my employability and no one wants to interview me even though I feel like I have 75% of the experience. I'm hoping to spend the next month to solely just work on this project. Hopefully that'll help!

Thanks in advance!",1527409521,16,0,5,dataengineering
Convert scraping script into live data stream?,"When I want to test an hypothesis, I know how the steps to take to scrape for the data, clean it. However, I find it challanging to then turn those steps into a program that adds new rows to the dataset. I can manage it, but it feels like I'm missing some basic tool that I don't know about.

I feel that way because, after some time has past, and I did my initial analysis, the data is already stale, and I need to scrape it from the date of the scrape to the present, then continue to stream it from that date without data loss.

the resulting code requires changes to the structure of the code that is quite different from the structure of code in the original script. Which is tedious and unproductive to do for every hypothesis raised. What is a better workflow/tool that suits these situations than coding these rules by hand in python?",1527340594,3,0,0,dataengineering
Data Warehouse Concept Question,"Hello,

Newb question here. So if I use an ETL tool like [Stitch](https://www.stitchdata.com/) and load all these data tables into my Data Warehouse, is the best practice to create a data mart from this warehouse for my specific reporting needs? If so, how would I refresh this data mart given the data from the original data warehouse is being refreshed daily? Would I have to write a bunch of SQL scripts to do this? Obviously that sounds like not a good idea, so there must be some technologies I am unaware of.",1527025520,10,0,2,dataengineering
Analyzing Your Data Lake With PrestoDB (Interview),,1526954663,0,0,2,dataengineering
"One ETL, one partition rule","Maxime has talked about a ""one ETL, one partition"" guideline in this post: https://medium.com/@maximebeauchemin/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a. Can anyone shed some light on this? At first, I thought this meant one ETL should output one HDFS partition. However, HDFS has a normal block size (default is 128mb). Does that mean all ETLs should return results less than the HDFS partition size? ",1526869712,2,0,2,dataengineering
Java in Data Engineering,"Hi everyone! 

I've done some searches for Data Engineering on various sites and it seems that at least some of those jobs require 3\+ years of experience with enterprise Java. In your experience, is it hard to get a big data dev job without a lot of experience of Java? And what is it used for\(why not Scala/Python, in other words\)?",1526740088,8,0,3,dataengineering
Testing Data Pipelines With Great Expectations (Interview),,1526390117,2,0,3,dataengineering
A Brief Look At Geospatial Data And Graph Databases (Interview),,1526390100,0,0,2,dataengineering
Getting Data to Data Lake from Microservices — Part 2: The Logs or Clickstream,,1526332968,0,0,4,dataengineering
Curious to know about real world cases where an always-on cluster setup has proven more useful than cloud storage like S3 for storing all your data,"Also hybrid setup?

For the uninitiated - https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html",1525851892,2,0,4,dataengineering
What do you do in a situation where your internal customer wants to own the pipeline you build?,"I should first ask: Do you guys own the pipelines after you build them? Or do you typically hand them off?

I have a pipeline (with not many dependencies) I need to create for a different department and they want their BI dev to own the whole thing after I'm done. But I don't want them having access to my Airflow instances which are running other processes, and having them needing to learn Python etc to manage it. I'm using Apache Nifi for some other things and was thinking about throwing it on there. Thoughts? How do you guys handle situations like this? Is my approach here sensible?",1525808227,3,0,4,dataengineering
Brief Conversations On Data Engineering From The Open Data Science Conference,,1525771311,0,0,3,dataengineering
"As someone who's still learning data engineering, I recently attempted to get into a data engineering program by performing a data engineering challenge. I was able to write all of the data cleaning/preparation code correctly, but didn't know how to do the rest. What else do I need to learn?","I started out learning to write code for data science last year but I'm also trying to learn data engineering. Thus far, the classes I've taken have all focused on just writing code to mine and clean data (lots of Pandas), visualize it (Matplotlib and Seaborn), perform statistical analysis (Numpy/Scikit-learn). And I generally have used either Jupyter Notebooks or Spyder IDEs as my coding environments with no other software that data engineers use (for example, I'm aware of things like Apache Spark and Hadoop but I don't yet know how to use them or why I would use them other than that I know there's necessary for managing scalable projects).

So with this data engineering challenge, the data cleaning/wrangling was doable as I've learned Python for data science at at least an intermediate level. But the challenge asked me to do things I just haven't learned how to do, such as:

1) Arranging my Python file within the proper directory structure to allow scaling for large amounts of data, 

2) Loading my directory structure for the file to Github, 

3) Have the directory corrected arranged to allow the use of unit tests, etc...

Basically, I know there's a set of software that data engineers would use to manage their work and prepare it for scaling and testing but I don't know where to go to learn this stuff.

1) Is there like one piece of IDE software or something else that I should use for preparing my code? Or should I be using something like Django or Spark?

2) Are there good tutorials I can check out to learn what I need to learn?",1525477525,4,0,5,dataengineering
Advice on the HOW,"I have been tasked with managing our data which will be used for various machine learning work by our data scientist. Most of my experiences have been around development so I am little lost as the best path to take. We have about 300K word documents that are parsed and text extracted. It is growing at a steady rate. To all those with a million times more experiences than me, how would you approach this problem? Any advices is greatly appreciated! ",1525455852,1,0,2,dataengineering
"The best article I’ve read about the modern data engineer, by the creator of Airflow",,1525387685,11,0,18,dataengineering
"Cloud Composer is now in beta: build and run practical workflows with minimal effort. Google cloud composer is a managed service, combine the strengths of Google Cloud Platform with Airflow.",,1525286109,0,0,1,dataengineering
"Google cloud composer is a managed service, combine the strengths of Google Cloud Platform with Airflow.",,1525285478,0,0,1,dataengineering
Astronomer Airflow v0.2.0 – A fully open source alternative to Google Composer,,1525274991,0,0,3,dataengineering
Crosspost: Data Engineers Vs. Data Scientists And Why These Differences Are So Important,,1525214928,2,0,7,dataengineering
Has anyone taken Jesse Anderson's online database engineer courses?,I'm curious because i'm considering purchasing it and i'm wondering if anyone here has gone through with it.,1525110502,14,0,8,dataengineering
Self Service Business Intelligence For Everyone With Metabase (Interview),,1525050744,0,0,3,dataengineering
"Moved from software engineering into data engineering, getting ready to interview elsewhere...","I have about ten years professional experience, with plenty of Python application development and web application development.  I started doing data analytics and about two years ago started filling a more data engineering oriented role, picking up Scala and Spark as well as Hadoop and related scheduling and graphdb tools.  Most of my work was still very application oriented and I didn't do a lot of ""building pipelines"" as the architecture was already there.

Anyway, long story short, now I'm starting to interview for data engineer roles elsewhere and I'm not sure what I might be expected to know in an on\-site interview.  I'm used to programming interviews where you whiteboard some stuff about data structures or whatever, show that you know how to use sets and iterate recursively and so on.  What will the interview structure be like?  What kinds of technical questions might I get?  When it comes to my day to day, I do much more functional development than pipeline management or whatever so I'm used to having to look up how to do a lot of stuff there and I don't know if I could whiteboard a data pipeline solution much past

\-\-\-\-\&lt;\-\- hdfs? aws? \&lt;\-\-\-\-\-\-\-\-\-ingestion \(csv? raw?\) 

|                  \^

|                  |

|                  |

\-\-\-\-\-\-\-\-\-\-\-\- scala,spark,maybe oozie?\-\-\-\-\&gt; logs \(yarn?\)

So yeah... I feel that I can speak to a general idea of how a data engineer should approach a problem, work with their data science team, and so on.  I can certainly roughly code a map reduce or a transform using rdds or whatever.

I'm not really even sure what questions I should be asking here because I have so little idea what to expect.  Any advice is very welcome.  I am interviewing next week with a large financial company as well as with a small pharmaceutical testing company.  Thanks in advance!",1525029761,2,0,2,dataengineering
Mesos or YARN for Spark Jobs?,"Hi,
We want to use a resource scheduler for our Spark jobs. I came across Mesos and Yarn but am unable to decide which one to use.
The primary goal is ease of setup, parallelization of jobs and better resource utilization. I read a lot on the differences but can't find any opinion on what to use.
For now the use case is Spark but we would like to extend the resource pooling to other services too, though that will be quite later in the future.
Please help me out on this. Thanks ",1524728670,3,0,5,dataengineering
Are any of you doing AI / ML?,"I'm a brand new DE and I'm helping establish / define what DE is at my org, but the problem is, I'm also being lumped into doing AI projects which I don't necessarily think is the right thing to do. I'm a software / systems engineer historically, not a Data Scientist or someone with a degree in Mathematics. So I feel like I don't have the right knack to do the job properly and it's pretty much keeping me up at night. I can learn it no problem, but I don't think it's the right structure to my future self / team. Thoughts?",1524694505,4,0,3,dataengineering
Octopai Managed Metadata Service For Better Business Intelligence (Interview),,1524445749,5,0,0,dataengineering
Data Warehouse or DataLake?,"Hey guys,

This is my first post here and I am not sure if this is the right sub for it. So I recently joined an organization and one of their goals for next few years is to create a central repository for data from different databases.

Currently, we have 15 different databases and a badly designed datawarehouse where there are no conformed dimensions and hence, not able to cross query and users being able to get the data they way they would like.

One step would be to recreate the EDW and gather the requirements from users again and start with the grain and move on from there.

Second option would be to create a datalake and get all the data in there and transform and use it the way we want it.

regarding this technique, I have just started reading about it and not sure about the resources it would take to accomplish this task.

The data is less than 100 TB and growth is minimal.

I would really appreciate if there is a path we could take besides from these two or if we chose the second option, what resources it would take and how much time frame should we look at to get this taken care of?

Note: I am new to this and would like to come with ideas before my next meeting with the team.

We currently have SQL 2012 And We got 3 SQL BI developers and 2 DBA's.

Thanks guys and happy weekend!",1524230576,10,0,1,dataengineering
Best Home Based Data Entry Jobs Online,,1524218346,2,0,0,dataengineering
Who are the most influential and knowledgeable people working as Data engineers?,"Hi,
I was just interested in knowing who all are considered as people worth following in this field. Anytime I search, I find Data scientists and Analysts. Now this field is a little more closely tied to software engineering and its a little hard to differentiate between the two, but I wanted to know about people who have worked a lot on Big Data and Distributes systems. Eg- Martin Kleppman, Jay Kreps, Neha Narkhede etc.
Names or links to blogs would do.
Thanks",1523953285,8,0,5,dataengineering
Keeping Up With The Data Engineering industry (Interview),,1523771249,0,0,2,dataengineering
"How much understanding of the business side of the data should the data engineers and scientists have, in your opinion?","Hi everybody. I started working as a data engineer at a large telecom company about 2 months ago. One thing I've noticed is my manager expects me to have a deep understanding of what kind of story the data we're working with can tell actually us.

For example, let's say we have a ""business process""(I think that's what we would call it?..) of somebody buying a sim card and activating it using a particular device. She thinks I should have enough knowledge to translate these words into an SQL query that would give us stuff like where the sim card was bought, when the activation occurred, who bought it, where it was bought, etc etc. The data warehouse we're working with has numerous problems, from the absence of strong naming conventions, documentation and so on and so forth...so constructing a query such as this can take me a really, really long time. But I also think that I shouldn't even be the one who cares about this stuff. I'm supposed to work on data infrastructure and pipelines, not actually understanding the whole business side. Am I wrong?",1523703660,12,1,3,dataengineering
Transition to blockchain?,"Hi
Do you think it is a good career move for a data engineer to develop skills in blockchain file system and databases  (ipfs,bigchaindb...)?",1523521468,3,0,1,dataengineering
[Question] Current best practices around running multiple queries to multiple sources that run for hours at a time?,"Hey Everyone

I have a project that I need to get out the door soon and I am immediately looking at Airflow right now for this, but I was just curious, is there anything else I need to be taking into consideration? I plan on taking Airflow and running it on a local k8's cluster, basically an Airflow container per business area. The queries that I have are already built, I just need to orchestrate them and make sure data is flowing efficiently and in order because right now it's a monolithic manual process that my Data Scientist has to run.

Also - what's the best way to do something like this on AWS or GCP? Just build an AMI for Airflow and throw it up on an instance where you need it?

I'm a fresh new DE coming from a Python / Big Data / DevOps background so this might be a dumb question. sorry :x

Thanks!",1523487705,0,0,2,dataengineering
What language do you wish your Data Science colleagues knew?,"1. Java
2. Scala
3. C++
4. PHP
5. Ruby
6. Python",1523324769,6,0,3,dataengineering
