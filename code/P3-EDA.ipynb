{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Reddit Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages. \n",
    "- We are going to need the big three: pandas, numpy, matplotlib. \n",
    "- We also will use nltk.corpus to examine stop words.\n",
    "- From Sklearn, we will pull in\n",
    "    - train_test_split, cross_val_score\n",
    "    - Models:\n",
    "        - LogisticRegression\n",
    "        - NaiveBayes\n",
    "        - SVC\n",
    "        - RandomForestClassifier\n",
    "        - BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 591 ms, total: 2.46 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Here we are bringing in our cleaned dataset coming from P3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the clean data\n",
    "df = pd.read_csv('../data/clean.csv')\n",
    "\n",
    "# View a preview\n",
    "clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean.clean_title.isna().sum() > 0:\n",
    "    clean.drop(labels = clean[clean.clean_title.isna()].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39813 entries, 0 to 39812\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   title           39813 non-null  object\n",
      " 1   selftext        23811 non-null  object\n",
      " 2   clean_title     39783 non-null  object\n",
      " 3   selftext_urls   8152 non-null   object\n",
      " 4   title_urls      93 non-null     object\n",
      " 5   clean_selftext  23806 non-null  object\n",
      " 6   created_utc     39813 non-null  int64 \n",
      " 7   num_comments    39813 non-null  int64 \n",
      " 8   num_crossposts  39813 non-null  int64 \n",
      " 9   score           39813 non-null  int64 \n",
      " 10  subreddit       39813 non-null  object\n",
      " 11  Subreddit_name  39813 non-null  int64 \n",
      " 12  merged          39793 non-null  object\n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn's stopwords, extracted\n",
    "sklearn_stopwords = list(CountVectorizer(stop_words = 'english').get_stop_words())\n",
    "\n",
    "#Custom created list\n",
    "custom_stopwords = ['good','time','python','tool','source','best','learn','science']\n",
    "\n",
    "# Personalized stopwords\n",
    "personal_stopwords = sklearn_stopwords + custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRYING SOME EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by = 'subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = [len(element) for element in df['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a count vectorizer to conduct some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words = 'english', min_df=4, max_df = 1.0) #stop_words = 'english',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = cvec.fit_transform(df['clean_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with our term_matrix outputted from Count_vec\n",
    "term_df = pd.DataFrame(term_matrix.toarray(), columns = cvec.get_feature_names())\n",
    "# Lets insert our target as \"Subreddit\"\n",
    "term_df.insert(0, 'Subreddit_name', df['Subreddit_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df.groupby('Subreddit_name').mean().T.sort_values(1, ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequency(word, term_df):\n",
    "''' \n",
    "    word (string): Any word that exists in the dataframe of term frequency\n",
    "    term_df (dataframe): A dataframe that lists the word frequency of each word in two different corpuses.\n",
    "    This is how you add document strings to your functions\n",
    "'''\n",
    "    term_df[term_df['Subreddit_name']==0]['word'].value_counts().to_dict() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_word_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning frequency of \"data\"\n",
    "term_df[term_df['Subreddit']==0]['data'].value_counts().to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science frequency of \"data\"\n",
    "term_df[term_df['Subreddit']==1]['data'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "term_df[term_df['Subreddit']==0]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df[term_df['Subreddit']==1]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==0]['data'].value_counts().to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==1]['data'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "term_df[term_df['Subreddit_name']==0]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df[term_df['Subreddit_name']==1]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==0]['help'].value_counts().to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==1]['help'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "term_df[term_df['Subreddit_name']==0]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df[term_df['Subreddit_name']==1]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_ML = list(term_df.groupby('Subreddit_name').\n",
    "     mean().T.sort_values(0, ascending=False).head(250).index)\n",
    "\n",
    "top_words_DS = list(term_df.groupby('Subreddit_name').\n",
    "     mean().T.sort_values(1, ascending=False).head(250).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_overlap = [element for element in top_words_DS if element in top_words_ML]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_words_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could conduct a hypothesis test on this.\n",
    "\n",
    "$H_0$: The subreddits for DS and ML have the same mean frequency for word $x$.\n",
    "\n",
    "$H_A$: The subreddits for DS and ML have a different mean frequency for word $x$.\n",
    "\n",
    "We'll set our alpha at .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_dict = {}\n",
    "\n",
    "# Cycle through each word in overlap list\n",
    "for word in top_words_overlap:\n",
    "    \n",
    "    # Conduct a t-test, and append the result statistic\n",
    "    ttest_dict[word] = ttest_ind(term_df[term_df['Subreddit_name']==1][word], # word count in DS\n",
    "         term_df[term_df['Subreddit_name']==0][word]) # word count dist in ML\n",
    "    \n",
    "ttest_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to examine common top words and their p-values\n",
    "\n",
    "ttest_df = pd.DataFrame([ttest_dict]).T.sort_values(0)\n",
    "ttest_df['pvalue'] = [element.pvalue for element in ttest_dict.values()]\n",
    "ttest_df['statistic'] = [element.statistic for element in ttest_dict.values()]\n",
    "ttest_df.drop(columns = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df.sort_values(by='pvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cvec_dist(words, dataframe, target = 'Subreddit_name', classes = [0,1]):\n",
    "    nrows = len(words)//2 if not len(words)%2 else len(words)//2 + 1\n",
    "    class_0 = dataframe[dataframe[target]==classes[0]]\n",
    "    class_1 = dataframe[dataframe[target]==classes[1]]\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=nrows, figsize=(20, 7*nrows))\n",
    "    ax = ax.ravel()\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        counts_0 = class_0[word].value_counts()[1:].to_dict()\n",
    "        counts_1 = class_1[word].value_counts()[1:].to_dict()\n",
    "        mean_0 = class_0[word].mean()\n",
    "        mean_1 = class_1[word].mean()\n",
    "        ax[i].bar(counts_0.keys(), counts_0.values(), color='goldenrod', alpha=.3)\n",
    "        for keys, values in counts_0.items():\n",
    "            ax[i].text(keys-.1, values, s=values, fontsize=14, color='goldenrod')\n",
    "        for keys, values in counts_1.items():\n",
    "            ax[i].text(keys+.1, values, s=values, fontsize=14, color='grey')\n",
    "        ax[i].bar(counts_1.keys(), counts_1.values(), color='grey', alpha=.3)\n",
    "        ymin, ymax = ax[i].get_ylim()\n",
    "        ax[i].plot([mean_0]*2, [ymin, ymax], ':', color='goldenrod')\n",
    "        ax[i].plot([mean_1]*2, [ymin, ymax], ':', color='grey')\n",
    "        ax[i].set_title(f'{word} frequency counts\\nmeans: {mean_0:0.02f} vs {mean_1:0.02f}')\n",
    "        ax[i].legend(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cvec_dist(top_words_overlap, term_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
