{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import relevant packages:\n",
    "- Here we will include the big three: pandas, numpy, and matplotlib\n",
    "- BeautifulSoup for cleaning html artifacts from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Machine learning and datascience data\n",
    "mc = pd.read_csv('./data/machinelearning_1.csv')\n",
    "ds = pd.read_csv('./data/datascience_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data\n",
    "df = pd.concat([mc,ds])\n",
    "df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[D] Hinton responds to Schmidhuber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1587609168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hinton responds to Schmidhuber</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1587609111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[D] Other than vectorization, what other aspec...</td>\n",
       "      <td>I'm helping a friend design a course with dual...</td>\n",
       "      <td>1587606108</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Survey for IT Employees working from home! Hel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1587604741</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[R] Chip Placement with Deep Reinforcement Lea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1587604558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MachineLearning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                 [D] Hinton responds to Schmidhuber   \n",
       "1                     Hinton responds to Schmidhuber   \n",
       "2  [D] Other than vectorization, what other aspec...   \n",
       "3  Survey for IT Employees working from home! Hel...   \n",
       "4  [R] Chip Placement with Deep Reinforcement Lea...   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0                                                NaN   1587609168   \n",
       "1                                                NaN   1587609111   \n",
       "2  I'm helping a friend design a course with dual...   1587606108   \n",
       "3                                                NaN   1587604741   \n",
       "4                                                NaN   1587604558   \n",
       "\n",
       "   num_comments  num_crossposts  score        subreddit  \n",
       "0             0               0      1  MachineLearning  \n",
       "1             1               0      1  MachineLearning  \n",
       "2             2               0      1  MachineLearning  \n",
       "3             2               0      1  MachineLearning  \n",
       "4             1               0      1  MachineLearning  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning [deleted] and [removed] rows from title and selftext\n",
    "\n",
    "There are still some remaining rows in title and selftext that have some deleted and removed rows.\n",
    "\n",
    "Lets create a mask that looks for them and then drops them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for titles that are removed and deleted\n",
    "title_removed = (df['title']=='[removed]')\n",
    "title_deleted = (df['title']=='[deleted]')\n",
    "\n",
    "# Create a boolean mask for subtexts that are removed and deleted\n",
    "selftext_removed = (df['selftext']=='[removed]')\n",
    "selftext_deleted = (df['selftext']=='[deleted]')\n",
    "\n",
    "# Check if there are any removed or deleted values. \n",
    "#  Remove them\n",
    "\n",
    "if (len(df[title_removed]) + len(df[title_deleted]) + \n",
    "     len(df[selftext_removed]) + len(df[selftext_deleted])) > 0:\n",
    "    \n",
    "    # Remove rows with '[deleted]' as the title or selftext\n",
    "    df.drop(labels = df[selftext_deleted].index, axis = 0, inplace=True)\n",
    "    df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean out each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that runs over reviews \n",
    "def review_to_words(raw_review):\n",
    "    # Remove \n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    #text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', letters_only, flags=re.MULTILINE)\n",
    "    text = re.sub(r'(\\(https:\\/\\/[^\\s]+)|https:\\/\\/[^\\s]+', '', letters_only, flags=re.MULTILINE)\n",
    "    words = text.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Running through review_to_words to clean each row\n",
    "df.insert(2,column = 'clean_title',values = [review_to_words(element) for element in df['title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lzone.de docs.python.org\n"
     ]
    }
   ],
   "source": [
    "raw_review = 'here is a sample URL https://lzone.de/examples/Python%20re.sub or how about this https://docs.python.org/3/library/re.html'\n",
    "def review_to_url(raw_review):\n",
    "    domain_string = ' '\n",
    "    domains = []\n",
    "    urls = re.findall('https:\\/\\/[^\\s]+', raw_review)\n",
    "    for url in urls:\n",
    "        base_url = re.findall('^((http[s]?|ftp):\\/)?\\/?([^:\\/\\s]+)((\\/\\w+)*\\/)([\\w\\-\\.]+[^#?\\s]+)(.*)?(#[\\w\\-]+)?$', url)\n",
    "        a, b, domain, d, e, f, g, h = base_url[0]\n",
    "        domains.append(domain)\n",
    "    \n",
    "    return domain_string.join(domains)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we work on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a subreddit column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a target variable of 0/1 in dataframe\n",
    "df['Subreddit_name'] = [1 if element == 'datascience' else 0 for element in df['subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X variable\n",
    "\n",
    "X = df['clean_title']\n",
    "y = df['Subreddit_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words = 'english', max_df = 0.8, max_features = 173)\n",
    "\n",
    "cvec.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train = cvec.transform(X_train)\n",
    "transformed_train_df = pd.DataFrame(C_train.toarray(), \n",
    "                             columns = cvec.get_feature_names())\n",
    "\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(transformed_train_df, y_train)\n",
    "SS_train = ss.transform(transformed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train_df = pd.DataFrame(C_train.toarray(), \n",
    "                             columns = cvec.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr = LogisticRegression()\n",
    "\n",
    "tr.fit(transformed_train_df, y_train)\n",
    "len(tr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC model\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(SS_train, y_train)\n",
    "svc.score(SS_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cross_val_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn's stopwords, extracted\n",
    "sklearn_stopwords = list(CountVectorizer(stop_words = 'english').get_stop_words())\n",
    "\n",
    "#Custom created list\n",
    "custom_stopwords = ['good','time','python','tool','source','best','learn','science']\n",
    "\n",
    "# Personalized stopwords\n",
    "personal_stopwords = sklearn_stopwords + custom_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(personal_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Pipeline\n",
    "\n",
    "C = np.logspace(-2,1,15)\n",
    "pipe = Pipeline([\n",
    "    ('vec',TfidfVectorizer()),\n",
    "    ('lr',LogisticRegression())\n",
    "])\n",
    "# Define pipe parameters\n",
    "pipe_params = {\n",
    "    'vec__max_features':[173],\n",
    "    'vec__max_df':[0.8],\n",
    "    'vec__max_df':[0.8],\n",
    "    'vec__stop_words': ['english'],\n",
    "    'vec__ngram_range':[(1,1)],\n",
    "    'lr__C': C\n",
    "    #'lr__penalty':['l2']\n",
    "}\n",
    "\n",
    "# Instantiate Grid Search\n",
    "gs = GridSearchCV(pipe, pipe_params, cv= 5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.score(X_train, y_train))\n",
    "print(gs.score(X_test, y_test))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.estimator.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRYING SOME EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by = 'subreddit').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = [len(element) for element in df['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a count vectorizer to conduct some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words = 'english', min_df=4, max_df = 1.0) #stop_words = 'english',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = cvec.fit_transform(df['clean_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with our term_matrix outputted from Count_vec\n",
    "term_df = pd.DataFrame(term_matrix.toarray(), columns = cvec.get_feature_names())\n",
    "# Lets insert our target as \"Subreddit\"\n",
    "term_df.insert(0, 'Subreddit_name', df['Subreddit_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df.groupby('Subreddit_name').mean().T.sort_values(1, ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequency(word, term_df):\n",
    "''' \n",
    "    word (string): Any word that exists in the dataframe of term frequency\n",
    "    term_df (dataframe): A dataframe that lists the word frequency of each word in two different corpuses.\n",
    "    This is how you add document strings to your functions\n",
    "'''\n",
    "    term_df[term_df['Subreddit_name']==0]['word'].value_counts().to_dict() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning frequency of \"data\"\n",
    "term_df[term_df['Subreddit']==0]['data'].value_counts().to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science frequency of \"data\"\n",
    "term_df[term_df['Subreddit']==1]['data'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "term_df[term_df['Subreddit']==0]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df[term_df['Subreddit']==1]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==0]['data'].value_counts().to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==1]['data'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "term_df[term_df['Subreddit_name']==0]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df[term_df['Subreddit_name']==1]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==0]['help'].value_counts().to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science frequency of \"data\"\n",
    "term_df[term_df['Subreddit_name']==1]['help'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "term_df[term_df['Subreddit_name']==0]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df[term_df['Subreddit_name']==1]['learning'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_ML = list(term_df.groupby('Subreddit_name').\n",
    "     mean().T.sort_values(0, ascending=False).head(250).index)\n",
    "\n",
    "top_words_DS = list(term_df.groupby('Subreddit_name').\n",
    "     mean().T.sort_values(1, ascending=False).head(250).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_overlap = [element for element in top_words_DS if element in top_words_ML]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_words_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could conduct a hypothesis test on this.\n",
    "\n",
    "$H_0$: The subreddits for DS and ML have the same mean frequency for word $x$.\n",
    "\n",
    "$H_A$: The subreddits for DS and ML have a different mean frequency for word $x$.\n",
    "\n",
    "We'll set our alpha at .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_dict = {}\n",
    "\n",
    "# Cycle through each word in overlap list\n",
    "for word in top_words_overlap:\n",
    "    \n",
    "    # Conduct a t-test, and append the result statistic\n",
    "    ttest_dict[word] = ttest_ind(term_df[term_df['Subreddit_name']==1][word], # word count in DS\n",
    "         term_df[term_df['Subreddit_name']==0][word]) # word count dist in ML\n",
    "    \n",
    "ttest_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to examine common top words and their p-values\n",
    "\n",
    "ttest_df = pd.DataFrame([ttest_dict]).T.sort_values(0)\n",
    "ttest_df['pvalue'] = [element.pvalue for element in ttest_dict.values()]\n",
    "ttest_df['statistic'] = [element.statistic for element in ttest_dict.values()]\n",
    "ttest_df.drop(columns = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_df.sort_values(by='pvalue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cvec_dist(words, dataframe, target = 'Subreddit_name', classes = [0,1]):\n",
    "    nrows = len(words)//2 if not len(words)%2 else len(words)//2 + 1\n",
    "    class_0 = dataframe[dataframe[target]==classes[0]]\n",
    "    class_1 = dataframe[dataframe[target]==classes[1]]\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=nrows, figsize=(20, 7*nrows))\n",
    "    ax = ax.ravel()\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        counts_0 = class_0[word].value_counts()[1:].to_dict()\n",
    "        counts_1 = class_1[word].value_counts()[1:].to_dict()\n",
    "        mean_0 = class_0[word].mean()\n",
    "        mean_1 = class_1[word].mean()\n",
    "        ax[i].bar(counts_0.keys(), counts_0.values(), color='goldenrod', alpha=.3)\n",
    "        for keys, values in counts_0.items():\n",
    "            ax[i].text(keys-.1, values, s=values, fontsize=14, color='goldenrod')\n",
    "        for keys, values in counts_1.items():\n",
    "            ax[i].text(keys+.1, values, s=values, fontsize=14, color='grey')\n",
    "        ax[i].bar(counts_1.keys(), counts_1.values(), color='grey', alpha=.3)\n",
    "        ymin, ymax = ax[i].get_ylim()\n",
    "        ax[i].plot([mean_0]*2, [ymin, ymax], ':', color='goldenrod')\n",
    "        ax[i].plot([mean_1]*2, [ymin, ymax], ':', color='grey')\n",
    "        ax[i].set_title(f'{word} frequency counts\\nmeans: {mean_0:0.02f} vs {mean_1:0.02f}')\n",
    "        ax[i].legend(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cvec_dist(top_words_overlap, term_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
